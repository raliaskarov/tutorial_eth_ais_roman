{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebfd835-dc90-4be8-8724-fdbb7c39a5b2",
   "metadata": {},
   "source": [
    "# Creating a Coding AI Agent using MCP\n",
    "\n",
    "In this notebook we will build an AI agent that does automatic coding and correct itself.\n",
    "The idea behind this is that we can give our agent tasks such as \"evaluate data xy\" and it will automatically execute and analyze the code.\n",
    "In later steps we will even extend this and more functionalities.\n",
    "\n",
    "> **Note:** This notebook has a more complicated setup and uses some advanced python programming functionalities.\n",
    "You don't need to understand every detail here, it is just important to understand the overall steps.\n",
    "\n",
    "We are using the **Model Context Protocol (MCP) Framework** - introduced in late 2024 by Anthropic - for agentic AI. It consists of a server-client architecture. They client (here in the notebook) sends requests to the servers (running in the background).\n",
    "Key components of this framework are:\n",
    "- **Resources** → external files and data the agent can use (e.g., CSV files in `resources/data/`).  \n",
    "- **Prompts** → standardized prompt/text templates that guide the AI when planning, executing tasks, or summarizing results (in `prompts/` folder).  \n",
    "- **Tools** → the actual functions exposed by servers (e.g., `code.run`, `csv.inspect`, `fridge.add`). These are what the agent calls to get work done.\n",
    "\n",
    "Feel free to take a look at the folders to get a feeling of the content of the files!\n",
    "\n",
    "\n",
    "**Folder Structure:**\n",
    "\n",
    "```text\n",
    "├─ agent.ipynb                  # ← You are here\n",
    "├─ servers/\n",
    "│  ├─ code_server.py            # Tools the agent can call (e.g., run code, inspect CSVs)\n",
    "│  └─ ...                       # (optional) more servers, e.g., search/fridge\n",
    "├─ resources/\n",
    "│  ├─ data/\n",
    "│  │  ├─ supermarket_sales.csv  # Example CSVs to analyze\n",
    "│  │  └─ ...\n",
    "│  └─ runs/                     # Agent outputs: code, logs, plots, result tables\n",
    "│     └─ ...\n",
    "├─ prompts/                     # Lightweight prompt templates (planning, repair, etc.)\n",
    "│  ├─ error_repair.md\n",
    "│  └─ ...\n",
    "└─ utils/\n",
    "   ├─ agent_support.py          # Small helpers to keep the notebook clean (mostly technical details - not important for general understanding)\n",
    "   └─ multiprocess_utils.py     # Further helpers to manage multiprocessing  (mostly technical details - not important for general understanding)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb1026",
   "metadata": {},
   "source": [
    "## Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90a04b-c5d4-4a14-bab0-be4ec0f1cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"mcp>=1.13.1\" \"openai>=1.0.0\" nest-asyncio ipywidgets matplotlib pandas ddgs numpy\n",
    "\n",
    "import atexit\n",
    "import asyncio\n",
    "import contextlib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from utils.multiprocess_utils import sweep_stale_servers_in_dir\n",
    "import pandas as pd\n",
    "\n",
    "# Setup for OpenAI API\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\", \"986IfxLKwN3Paiq4yx1Kn2iTG7FyG2GxFg17qQSyr1KZqGLaizAGJQQJ99BCACI8hq2XJ3w3AAABACOGQfvw\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\", \"https://cas-dml-llm.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\") # Available models: \"gpt-35-turbo\" or \"gpt-4o\"\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "\n",
    "\n",
    "from utils.agent_support import * # all helper functions from agent_support.py file\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "SERVERS_DIR = ROOT / \"servers\"\n",
    "os.environ[\"ROOT\"] = str(ROOT)\n",
    "\n",
    "# (IGNORE) manages processes in the background\n",
    "atexit.register(lambda: asyncio.get_event_loop().run_until_complete(teardown()))\n",
    "sweep_stale_servers_in_dir(SERVERS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1aa8fc-6603-48ef-91e7-28e04ea83a17",
   "metadata": {},
   "source": [
    "## Overview and Core Function `run_task`\n",
    "Before we dive into implementation details let's look at the overview of the agent we want to implement shown in this graphic.\n",
    "\n",
    "\n",
    "![agent_overview](resources/agent_overview.png)\n",
    "\n",
    "The process of answering a request is managed through the `run_task` function, where we first make a plan and then execute this plan step by step.\n",
    "This is implemented here. Ignore for now the details of the implementation of the execution step, those are clarified in the following steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f3716-a423-4e5d-bda4-ecdd50ccaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_task(task_text: str, max_attempts: int = 5):\n",
    "\n",
    "    # (0) Received task\n",
    "    print_step(\"START Task\")\n",
    "    print(f\"User task: '{task_text}'\")\n",
    "\n",
    "    # (1) Make plan how to solve task\n",
    "    plan = await plan_task(task_text)\n",
    "    print_plan(plan)\n",
    "\n",
    "    # Is the task doable with our available functionalities?\n",
    "    if plan[0][\"intent\"] == \"out_of_scope\":\n",
    "        print(\"Requested task is considered as 'out-of-scope' for the agent.\")\n",
    "        return\n",
    "\n",
    "    # (2) Execute the plan step by step\n",
    "    ctx = {\"user_task\": task_text, \"plan\": plan} # context in which all steps can write intermediate results\n",
    "    for i, step in enumerate(plan, 1):\n",
    "        intent = step[\"intent\"]\n",
    "        print_step(f\"STEP {i}: {intent}\")\n",
    "\n",
    "        # Check if the collected ctx contains everything needed for next step\n",
    "        needed = step.get(\"use\") or []\n",
    "        missing = [k for k in needed if k not in ctx]\n",
    "        if missing:\n",
    "            print(f\"Missing inputs {missing}. Available: {list(ctx.keys())}\")\n",
    "            return\n",
    "\n",
    "        # Get handler for current step and execute it\n",
    "        handler = INTENT_REGISTRY[intent]['fn']\n",
    "        out = await handler(step=step, ctx=ctx, max_attempts=max_attempts)\n",
    "        out = out or {}\n",
    "\n",
    "        # Save intermediated outputs under requested names if provided\n",
    "        save_keys = step.get(\"save\") or list(out.keys())\n",
    "        if save_keys and out:\n",
    "            for new_name, (k, v) in zip(save_keys, out.items()):\n",
    "                ctx[new_name] = v\n",
    "        ctx.update(out) # also adding raw out keys\n",
    "\n",
    "        compact_preview(out)\n",
    "\n",
    "    # (3) Print final output/answer\n",
    "    print_step(\"DONE with Task ✅\")\n",
    "    show_final_outputs({k: ctx[k] for k in step[\"output\"] if k in ctx})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36ae16-35f2-4c0f-8a3c-a0b3a004f736",
   "metadata": {},
   "source": [
    "## Task Planner\n",
    "As seen, a main component and needed for the first step is the **planner**. The planner takes an initial request and then -knowing which tools (intents) are available- using an LLM plans what steps need to be done after each other to complete the task. For example a standard request asking for a CSV analysis could look like:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"intent\": \"csv_analytics\", \"params\": {\"wants_plot\": true}, \"save\": [\"result_csv\", \"plot_png\"]},\n",
    "  {\"intent\": \"summarize\", \"use\": [\"result_csv\", \"plot_png\"], \"save\": [\"answer_text\"], \"output\": [\"answer_text\", \"plot_png\"]}\n",
    "]\n",
    "```\n",
    "\n",
    "The following function implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f45dc-7364-4e4f-8db4-672d5e3e9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_task(task_text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creating a plan via asking the LLM to propose a 1–5 step plan using registered intents.\n",
    "    Returns a LIST of 1-5 steps. Each step:\n",
    "      {\n",
    "        \"intent\": \"code_general\" | \"csv_analytics\" | ... ,\n",
    "        \"params\": {\"...\": \"...\" },      # step-specific params\n",
    "        \"use\": [\"...\"],                 # optional: ctx keys this step expects\n",
    "        \"save\": [\"...\"],                # optional: keys to store outputs under\n",
    "        \"output\" : [\"...\"]              # optional: for the last step, specify the overall final output\n",
    "      }\n",
    "    \"\"\"\n",
    "    print_step(\"PLANNING Task\")\n",
    "\n",
    "    # Collect additional infos we need for the prompt\n",
    "    allowed_intents = list(INTENT_REGISTRY.keys())\n",
    "    csvs = await available_csvs()\n",
    "\n",
    "    # Creating the prompt using the user's request, the available intents, and csvs\n",
    "    planning_prompt = f\"\"\"\n",
    "You are a concise planner for an agentic notebook.\n",
    "Create a SHORT plan (1–5 steps) to solve the USER_TASK.\n",
    "Keep it simple, many tasks are solvable with 1-3 step.\n",
    "Use ONLY these intents for the planning steps (including potential use and save values they have):\n",
    "{intent_specs_text(INTENT_REGISTRY)}\n",
    "If totally unrelated to these, return a SINGLE STEP with intent \"out_of_scope\".\n",
    "\n",
    "Step schema:\n",
    "[\n",
    "  {{\n",
    "    \"intent\": \"...\",                 // one of: {allowed_intents} or 'out_of_scope'\n",
    "    \"params\": {{ \"...\": \"...\" }},    // OPTIONAL step-specific params\n",
    "    \"use\": [\"...\"],                  // OPTIONAL list of ctx keys this step expects\n",
    "    \"save\": [\"...\"]                  // OPTIONAL list of keys to save outputs under\n",
    "    \"output\" : [\"...\"]               // ONLY for the last step, specify the overall final output\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "For \"params\", \"use\", and \"save\" you can ONLY use the ones that are specified before for each intent !!!\n",
    "All other parameters (like csv names) are managed in the functions! Don't introduce new params/use/save!\n",
    "\n",
    "{{To help with your csv_analytics decision, these CSV files are available: {csvs}}}\n",
    "\n",
    "Conventions:\n",
    "- IN the LAST  add a key \"output\" with all displayable results (only from the \"save\" values from the steps before).\n",
    "- Use \"summarize\" to create a friendly final explanation from \"stdout\", \"answer_text\",  \"result_csv\", or \"plot_png\" to check if there are plots.\n",
    "- Make sure if steps require ctx keys, that previous steps save those\n",
    "- DO NOT add IDs; order is the execution order.\n",
    "- Keep each step small and understandable.\n",
    "\n",
    "USER_TASK: {task_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "    #  Querying the LLM witht the prompt\n",
    "    content = await llm_chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": load_prompts()['system_return_json']},\n",
    "            {\"role\": \"user\",   \"content\": planning_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Parse LLM output\n",
    "        plan = json.loads(content)\n",
    "        if isinstance(plan, dict):\n",
    "            plan = [plan]\n",
    "    \n",
    "        # Check if all steps (intents) are valid\n",
    "        final_plan = []\n",
    "        for step in plan:\n",
    "            intent = step.get(\"intent\")\n",
    "            if not isinstance(step, dict) or intent not in (allowed_intents + ['out_of_scope']):\n",
    "                raise ValueError\n",
    "            final_plan.append({\n",
    "                \"intent\": intent,\n",
    "                \"params\": step.get(\"params\") or {},\n",
    "                \"use\": step.get(\"use\") or [],\n",
    "                \"save\": step.get(\"save\") or [],\n",
    "                \"output\": step.get(\"output\") or []\n",
    "            })\n",
    "    except Exception:\n",
    "        raise ValueError(f\"The LLM returned an invalid planning output, got: {content}\")\n",
    "            \n",
    "    return final_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e0f21-19ec-4078-9464-798072849977",
   "metadata": {},
   "source": [
    "## MCP Client\n",
    "\n",
    "We now start with implementing the functions and objects needed for the execution step. First of all we need a **client** that communicates with the servers (remember the graphic).\n",
    "We’ll use a `MCPClient` wrapper that connects to “servers” (tools).\n",
    "We can then add servers to this client.\n",
    "\n",
    "Each server exposes a few **tools** (e.g., `code.run`, `code.inspect_csv`) the agent can call.\n",
    "\n",
    "\n",
    "> **Quick note on asynchronous functions**\n",
    ">\n",
    "> Some functions in this notebook are **asynchronous** (they run tasks in the background).\n",
    "They are defined with the keyword `async`. Aside from that they work exactly as normal functions.\n",
    ">\n",
    "> In Jupyter you call them with `await`, see in the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0741ed8-744f-4c5b-b80a-09c14cb01f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPClient:\n",
    "    def __init__(self, default_workspace_dir=ROOT, default_env=None):\n",
    "        self.default_workspace_dir = default_workspace_dir\n",
    "        self.default_env = default_env or {}\n",
    "        self._cfg = {}\n",
    "        self._state = {}\n",
    "        self._stack = None\n",
    "\n",
    "    # (IGNORE)\n",
    "    async def __aenter__(self):\n",
    "        if self._stack is None:\n",
    "            self._stack = contextlib.AsyncExitStack()\n",
    "            await self._stack.__aenter__()\n",
    "        return self\n",
    "    # (IGNORE)\n",
    "    async def __aexit__(self, *exc):\n",
    "        await self.aclose()\n",
    "    # (IGNORE)\n",
    "    async def aclose(self):\n",
    "        if self._stack:\n",
    "            await self._stack.aclose()\n",
    "            self._stack = None\n",
    "            self._state.clear()\n",
    "\n",
    "    async def add_server(self, name, server_py, *, workspace_dir=None, env=None, print_tools=True):\n",
    "        \"\"\"Add a new MCP server to the agent with optional environment and workspace.\"\"\"\n",
    "        if self._stack is None:\n",
    "            self._stack = contextlib.AsyncExitStack()\n",
    "            await self._stack.__aenter__()\n",
    "\n",
    "        if name in self._state:\n",
    "            print(f\"Server '{name}' already exists, no new server added.\")\n",
    "            return\n",
    "\n",
    "        ws = str(workspace_dir or self.default_workspace_dir or \"\")\n",
    "        self._cfg[name] = {\"server_py\": str(server_py), \"workspace_dir\": ws, \"env\": env or {}}\n",
    "\n",
    "        env_all = {\n",
    "            **os.environ,\n",
    "            **self.default_env,\n",
    "            \"WORKSPACE_DIR\": ws,\n",
    "            **(env or {}),\n",
    "        }\n",
    "        params = StdioServerParameters(\n",
    "            command=sys.executable,\n",
    "            args=[\"-u\", str(server_py)],\n",
    "            env=env_all\n",
    "        )\n",
    "\n",
    "        stdio_r, write = await self._stack.enter_async_context(stdio_client(params))\n",
    "        session = await self._stack.enter_async_context(ClientSession(stdio_r, write))\n",
    "        await session.initialize()\n",
    "        tools = [t.name for t in (await session.list_tools()).tools]\n",
    "\n",
    "        self._state[name] = {\"session\": session, \"stdio\": stdio_r, \"write\": write, \"tools\": tools}\n",
    "\n",
    "        if print_tools:\n",
    "            print(f\"Connected to server [{name}]. Available tools: {tools}\")\n",
    "\n",
    "    async def tools(self, name: str) -> list[dict]:\n",
    "        \"\"\"Return available tools for a given server.\"\"\"\n",
    "        return list(self._state[name][\"tools\"])\n",
    "\n",
    "    def servers(self) -> dict[str, list[str]]:\n",
    "        \"\"\"Return all available servers with its tools.\"\"\"\n",
    "        return {name: list(st.get(\"tools\") or []) for name, st in self._state.items()}\n",
    "\n",
    "    async def call(self, selector: str, args: dict):\n",
    "        \"\"\"Call a specific tool with given arguments. The selector format is 'server:tool' (e.g. 'code:code.run')\"\"\"\n",
    "        try:\n",
    "            server, tool = selector.split(\":\", 1)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Use 'server:tool', e.g. 'code:run_code'.\")\n",
    "\n",
    "        session = self._state[server][\"session\"]\n",
    "        res = await session.call_tool(tool, args)\n",
    "\n",
    "        for item in res.content:\n",
    "            typ = getattr(item, \"type\", None)\n",
    "            if typ == \"json\":\n",
    "                return item.data\n",
    "            if typ == \"text\":\n",
    "                try:\n",
    "                    return json.loads(item.text)\n",
    "                except Exception:\n",
    "                    return {\"text\": item.text}\n",
    "        return {}\n",
    "\n",
    "# Instantiating our client\n",
    "client = MCPClient()\n",
    "\n",
    "# Function to conveniently call the servers of our client\n",
    "async def mcp(selector: str, **kwargs):\n",
    "    return await client.call(selector, kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd6ce5",
   "metadata": {},
   "source": [
    "Having instantiated our client, we can now add our first server. This is how we add a server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7e829-5383-431e-98e4-fe34de6b6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we add a new server to our client. Here we add the code_server:\n",
    "await client.add_server(\"code\", SERVERS_DIR / \"code_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f48b9",
   "metadata": {},
   "source": [
    "With the following call, we can see which servers we are connected to and which tools are available. So far there should be only one server called 'code':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb10fdd-3d0d-4f27-8867-9c6ed2f6630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting all our available servers and the tools per server\n",
    "client.servers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39a53a",
   "metadata": {},
   "source": [
    "Now we can call the servers with an input (this is not our agent yet, just a simple server call to demonstrate the functionality).\n",
    "In this case a code sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf154b-385e-44de-906a-bf752535f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the server's coding functionality\n",
    "test_code = \"\"\"\n",
    "a = 21\n",
    "b = 12\n",
    "print(a+b)\n",
    "\"\"\"\n",
    "# we call it in the format server:server.tool e.g.:\n",
    "out = await mcp(\"code:code.run\", code=test_code)\n",
    "print(\"Raw 'code:code.run' output:\", out)\n",
    "print(\"\\nCode output:\", out['stdout_tail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225fece-028f-45a2-8585-090597fef71c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Available resources (CSV files)\n",
    "\n",
    "Let's next explore the available data (in our case CSV files). The files are located in `resources/data/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c6dda-57cb-4f63-a9dd-9d0d1cd20c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display our available data sources (csv files only in our case)\n",
    "async def available_csvs():\n",
    "    \"\"\"List available CSV files in the workspace.\"\"\"\n",
    "    res = await mcp(\"code:code.list_csvs\")\n",
    "    csvs = res.get(\"csvs\", [])\n",
    "    return [csv['filename'] for csv in csvs]\n",
    "    \n",
    "\n",
    "# Function to peek into all available CSV files\n",
    "async def peek_data(n_head: int = 5):\n",
    "    \"\"\"Preview all available CSV files\"\"\"\n",
    "    print_step(\"Listing CSVs\")\n",
    "    res = await mcp(\"code:code.list_csvs\")\n",
    "    csvs = res.get(\"csvs\", [])\n",
    "    for i, it in enumerate(csvs, 1):\n",
    "        print(f\"{i}. {it['filename']}  ({it['size_kb']} KB)  @ {it['path']}\")\n",
    "    print_step(\"Inspecting CSVs\")\n",
    "    for csv in csvs:\n",
    "        meta = await mcp(\"code:code.inspect_csv\", path=csv[\"path\"], n_head=n_head)\n",
    "        print(f\"path: {meta['path']}\")\n",
    "        print(f\"rows: {meta['n_rows']}\")\n",
    "        print(\"columns:\", meta[\"columns\"])\n",
    "        compact_df_preview(meta.get(\"head\", []))\n",
    "    return\n",
    "\n",
    "await peek_data()\n",
    "await available_csvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0edd4-c08c-47b6-b781-db104faceeea",
   "metadata": {},
   "source": [
    "## Adding the coding functionality to our agent\n",
    "\n",
    "A last component missing are the **handlers**. \n",
    "Here we implement the coding handlers.\n",
    "Before doing so, we define some helpers for the coding functionality.\n",
    "\n",
    "These helpers let the agent:\n",
    "- choose the best fitting CSV (with the help of an LLM),\n",
    "- generate Python code for a given task task (with the help of an LLM),\n",
    "- run it, and if it fails, repair and retry a few times,\n",
    "- save any plot (e.g., `plot.png`) and table results (e.g., `result.csv`),\n",
    "- optionally validate that certain result columns exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae082c7-0d42-4c81-989b-ed952e788d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _choose_csv(task_text, csvs, max_cols=20, n_head=2):\n",
    "    \"\"\"Helper function to pick the best fitting CSV for a prompt\"\"\"\n",
    "\n",
    "    # Collecting some data for each availabe CSV file\n",
    "    inspected, candidates = [], []\n",
    "    for i, it in enumerate(csvs, 1):\n",
    "        m = await mcp(\"code:code.inspect_csv\", path=it[\"path\"], n_head=n_head)\n",
    "        inspected.append((it, m))\n",
    "        cols = m.get(\"columns\", []) or []\n",
    "        candidates.append({\n",
    "            \"index\": i,\n",
    "            \"filename\": it.get(\"filename\"),\n",
    "            \"n_rows\": m.get(\"n_rows\", 0),\n",
    "            \"n_cols\": len(cols),\n",
    "            \"columns\": cols[:max_cols],\n",
    "            \"sample\": (m.get(\"head\", []) or [])[:1],\n",
    "        })\n",
    "\n",
    "    # Formulate prompt\n",
    "    prompt = f\"\"\"\n",
    "You are selecting the best CSV for this task.\n",
    "\n",
    "TASK: {task_text}\n",
    "\n",
    "Choose EXACTLY ONE candidate that best supports this task based on filename and basic metadata.\n",
    "Return STRICT JSON ONLY (no prose, no wrapping in code etc., just plain json):\n",
    "{{\n",
    "  \"index\": <int from the list below>,\n",
    "  \"reason\": \"<one-line reason>\"\n",
    "}}\n",
    "\n",
    "Candidates:\n",
    "{json.dumps(candidates, indent=2)}\n",
    "\"\"\".strip()\n",
    "    # Asl LLM for best fitting CSV\n",
    "    resp = await llm_chat(\n",
    "        [{\"role\":\"system\",\"content\":load_prompts()['system_return_json']},\n",
    "         {\"role\":\"user\",\"content\":prompt}]\n",
    "    )\n",
    "    # Extract and return best match\n",
    "    data = json.loads(resp)\n",
    "    i = data[\"index\"]\n",
    "    chosen, meta = inspected[i-1]\n",
    "    print_step(\"Selecting CSV\", level=2)\n",
    "    print(f\"LLM chose: {chosen['filename']} — {data.get('reason','')}\")\n",
    "    return chosen, meta\n",
    "\n",
    "def _compose_user_prompt(task_text: str, plan: Dict[str,Any], csv_meta: Dict[str,Any] | None) -> str:\n",
    "    \"\"\"Helper function to compose a coding prompt\"\"\"\n",
    "    if plan[\"intent\"] == \"csv_analytics\" and csv_meta:\n",
    "        return f\"\"\"\n",
    "USER TASK:\n",
    "{task_text}\n",
    "\n",
    "INTENT: {plan['intent']}\n",
    "OUTPUT PREFERENCE: {plan.get('result_preference','text')}\n",
    "WANTS_PLOT: {plan.get('wants_plot', False)}\n",
    "\n",
    "CSV CONTEXT:\n",
    "path: {csv_meta.get('path')}\n",
    "columns: {json.dumps(csv_meta.get('columns', []), indent=2)}\n",
    "dtypes: {json.dumps(csv_meta.get('dtypes', {}), indent=2)}\n",
    "head (sample rows):\n",
    "{json.dumps(csv_meta.get('head', [])[:5], indent=2)}\n",
    "\n",
    "Produce ONLY the Python code (no explanations, no backticks).\n",
    "Follow the intent's rules. If table-like, save 'result.csv'. If a chart, save 'plot.png'.\n",
    "\"\"\".strip()\n",
    "    else:\n",
    "        return f\"\"\"\n",
    "USER TASK:\n",
    "{task_text}\n",
    "\n",
    "INTENT: {plan['intent']}\n",
    "OUTPUT PREFERENCE: {plan.get('result_preference','text')}\n",
    "WANTS_PLOT: {plan.get('wants_plot', False)}\n",
    "\n",
    "Produce ONLY the Python code (no explanations, no backticks).\n",
    "- If scalar or short text, print as: print(\"ANSWER:\", value_or_text)\n",
    "- If table-like, save 'result.csv'\n",
    "- If a chart is appropriate, save 'plot.png'\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "async def _generate_code(task_text: str, plan: Dict[str,Any], csv_meta: Dict[str,Any] | None) -> str:\n",
    "    \"\"\"Helper function to generate code giving the task and meta data\"\"\"\n",
    "    print_step(\"Generating code\", level=2)\n",
    "\n",
    "    resp = await llm_chat(\n",
    "        [{\"role\":\"system\",\"content\": load_prompts().get(f\"system_{plan['intent']}\",\"\")},\n",
    "         {\"role\":\"user\",\"content\": _compose_user_prompt(task_text, plan, csv_meta)}]\n",
    "    )\n",
    "    code = extract_code(resp)\n",
    "    return code\n",
    "\n",
    "async def _repair_code(prev_code: str, stdout_tail: str, stderr_tail: str, task_text: str, plan: Dict[str,Any], csv_meta: Dict[str,Any]) -> str:\n",
    "    \"\"\"Helper function to regenerate code giving the task, meta data, and previous code, outputs, and errors\"\"\"\n",
    "    print_step(\"Repairing code\", level=2)\n",
    "    original_prompt = _compose_user_prompt(task_text, plan, csv_meta)\n",
    "    prompt = f\"\"\"\n",
    "Prior attempt failed. Error tail:\n",
    "{stderr_tail}\n",
    "\n",
    "Prior Output tail:\n",
    "{stdout_tail}\n",
    "\n",
    "This was the original prompt (reminder):\n",
    "{original_prompt}\n",
    "\n",
    "CSV columns/dtypes:\n",
    "{json.dumps(csv_meta.get('columns', []))} / {json.dumps(csv_meta.get('dtypes', {}))}\n",
    "\n",
    "PREVIOUS CODE:\n",
    "{prev_code}\n",
    "\n",
    "\"\"\".strip()\n",
    "    resp = await llm_chat(\n",
    "        [{\"role\":\"system\",\"content\": load_prompts()[\"system_code_csv_analytics\"] + load_prompts()[\"error_repair\"]},\n",
    "         {\"role\":\"user\",\"content\": prompt}]\n",
    "    )\n",
    "    return extract_code(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83a3c1-a6a7-45c2-b1ca-dcc46484d3c3",
   "metadata": {},
   "source": [
    "### Coding handler functions\n",
    "\n",
    "With these helpers we can now implement the **handlers**. The handlers wrap all the helper functions and server calls into one central function that fully executes one step (e.g. creating and verifying code).\n",
    "Each handler receives the current **step** + shared **context**, runs the right tool(s),\n",
    "and returns compact outputs (stdout, file paths, etc.). The agent stitches those together.\n",
    "\n",
    "For now we define two coding handlers:\n",
    "\n",
    "1. `code_general`, which is suited for any general coding task\n",
    "2. `csv_analytics`, which is tailored for analyzing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a5bc8-622c-4007-a963-7454070f0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handler_code_general(step, ctx, max_attempts=5) -> Dict[str, Any]:\n",
    "    \"\"\"Iteratively generate/repair user-task code, run it in a persistent work dir, collect artifacts, and optionally validate outputs.\"\"\"\n",
    "    # Reading in relevant context\n",
    "    params = step.get(\"params\") or {}\n",
    "    required_cols = params.get(\"result_columns\") or []\n",
    "    if isinstance(required_cols, str):\n",
    "        required_cols = [c.strip() for c in required_cols.split(\",\") if c.strip()]\n",
    "    task_text = ctx.get(\"user_task\").strip()\n",
    "\n",
    "    # Loop until success\n",
    "    plan = {\"intent\":\"code_general\", **params}\n",
    "    persistent_run_dir = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        # Generating new code\n",
    "        code = await _generate_code(task_text, plan, csv_meta=None) if attempt == 1 else \\\n",
    "               await _repair_code(last_code, last_out, last_errors, task_text, plan, csv_meta=None)\n",
    "        last_code = code\n",
    "\n",
    "        # Running the new code\n",
    "        print_step(f\"Attempt {attempt}: run code\", level=2)\n",
    "        last_run_out = await mcp(\"code:code.run\", code=code, attempt=attempt, run_dir=persistent_run_dir)\n",
    "        last_out = last_run_out.get(\"stdout_tail\")\n",
    "\n",
    "        # Directory where all files (python, png, outputs) of the code are stored\n",
    "        if persistent_run_dir is None:\n",
    "            persistent_run_dir = last_run_out.get(\"run_dir\")\n",
    "        print(f\"Created and ran code for attempt {attempt} in folder {persistent_run_dir}\")\n",
    "\n",
    "        # Writing some relevant infos into the results\n",
    "        result = {\n",
    "            \"stdout\": last_out,\n",
    "            \"run_dir\": persistent_run_dir\n",
    "        }\n",
    "        result.update(discover_run_artifacts(persistent_run_dir))\n",
    "\n",
    "        # Check if code output is valid\n",
    "        if last_run_out.get(\"success\"):\n",
    "            if required_cols and result.get(\"result_csv\"):\n",
    "                val = await mcp(\"code:code.validate\", run_dir=persistent_run_dir, required_columns=required_cols)\n",
    "                if val.get(\"passed\"):\n",
    "                    print(\"Validation OK.\")\n",
    "                    return result\n",
    "                else:\n",
    "                    last_errors = val.get(\"messages\", \"\")\n",
    "            else:\n",
    "                return result\n",
    "        else:\n",
    "            last_errors = last_run_out.get(\"stderr_tail\")\n",
    "        print(\"Got error: \", last_errors[-200:])\n",
    "\n",
    "    # If didn't succeed after all attempts, add error\n",
    "    print_step(\"FAILED (code_general) ❌\", level=2)\n",
    "    result[\"error\"] = \"Could not sucesfully complete the coding step\"\n",
    "    return result\n",
    "\n",
    "async def handler_csv_analytics(step, ctx, max_attempts=5) -> Dict[str, Any]:\n",
    "    \"\"\"Same as handler_code_general, but optimized for tasks that involve CSV analytics\"\"\"\n",
    "    # Reading in relevant context\n",
    "    params = step.get(\"params\") or {}\n",
    "    required_cols = params.get(\"result_columns\") or []\n",
    "    if isinstance(required_cols, str):\n",
    "        required_cols = [c.strip() for c in required_cols.split(\",\") if c.strip()]\n",
    "    task_text = ctx.get(\"user_task\",\"\").strip()\n",
    "\n",
    "    # Choosing the best fitting CSV\n",
    "    res = await mcp(\"code:code.list_csvs\")\n",
    "    csvs = res.get(\"csvs\", [])\n",
    "    chosen, csv_meta = await _choose_csv(task_text, csvs)\n",
    "    os.environ['CSV_PATH'] = csv_meta[\"path\"] # backup for more robust results\n",
    "\n",
    "    # Loop until success\n",
    "    plan = {\"intent\":\"csv_analytics\", **params}\n",
    "    persistent_run_dir = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        # Generating new code\n",
    "        code = await _generate_code(task_text, plan, csv_meta) if attempt == 1 else \\\n",
    "               await _repair_code(last_code, last_out, last_errors, task_text, plan, csv_meta)\n",
    "        last_code = code\n",
    "\n",
    "        # Running the new code\n",
    "        print_step(f\"Attempt {attempt}: run CSV analytics\", level=2)\n",
    "        last_run_out = await mcp(\"code:code.run\", code=code, attempt=attempt, run_dir=persistent_run_dir)\n",
    "        last_out = last_run_out.get(\"stdout_tail\")\n",
    "\n",
    "        # Directory where all files (python, png, outputs) of the code are stored\n",
    "        if persistent_run_dir is None:\n",
    "            persistent_run_dir = last_run_out.get(\"run_dir\")\n",
    "        print(f\"Created and ran code for attempt {attempt} in folder {persistent_run_dir}\")\n",
    "\n",
    "        # Writing some relevant infos into the results\n",
    "        result = {\n",
    "            \"stdout\": last_out,\n",
    "            \"run_dir\": persistent_run_dir\n",
    "        }\n",
    "        result.update(discover_run_artifacts(persistent_run_dir))\n",
    "\n",
    "        # Check if code output is valid\n",
    "        if last_run_out.get(\"success\"):\n",
    "            if required_cols and result.get(\"result_csv\"):\n",
    "                val = await mcp(\"code:code.validate\", run_dir=persistent_run_dir, required_columns=required_cols)\n",
    "                if val.get(\"passed\"):\n",
    "                    print(\"Validation OK.\")\n",
    "                    return result\n",
    "                else:\n",
    "                    last_errors = val.get(\"messages\", \"\")\n",
    "            else:\n",
    "                return result\n",
    "        else:\n",
    "            last_errors = last_run_out.get(\"stderr_tail\")\n",
    "        print(\"Got error: \", last_errors[-200:])\n",
    "\n",
    "    # If didn't succeed after all attempts, add error\n",
    "    print_step(\"FAILED (csv_analytics) ❌\", level=2)\n",
    "    result[\"error\"] = \"Could not sucesfully complete the coding csv analytics step\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446d34c-c2ec-4565-8da8-82a309a5b1fc",
   "metadata": {},
   "source": [
    "We add one more handler that is useful to have:\n",
    "\n",
    "`summarize`, which takes any kind of outputs and can summarize it into a text. This is also well suited for final outputs and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796f832-41b2-4732-92c3-29c15a03daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handler_summarize(step, ctx, **kw) -> Dict[str, Any]:\n",
    "    \"\"\"Summarize prior outputs (e.g., answer_text / stdout / result_csv head)\"\"\"\n",
    "\n",
    "    # Check what we want to consider and collect it in 'parts'\n",
    "    wants = step.get(\"use\") or []\n",
    "    parts = []\n",
    "    if \"answer_text\" in wants and ctx.get(\"answer_text\"):\n",
    "        parts.append(f\"[answer_text]\\n{ctx['answer_text']}\")\n",
    "    if \"stdout\" in wants and ctx.get(\"stdout\"):\n",
    "        parts.append(f\"[stdout]\\n{ctx['stdout']}\")\n",
    "    if \"result_csv\" in wants and ctx.get(\"result_csv\"):\n",
    "        df = pd.read_csv(ctx[\"result_csv\"]).head(10)\n",
    "        parts.append(f\"[result_csv head]\\n{df.to_csv(index=False)}\")\n",
    "    if \"plot_png\" in wants:\n",
    "        parts.append(f\"[plot_png] exists:\\n{'plot_png' in ctx}\")\n",
    "    if \"error\" in ctx:\n",
    "         parts.append(f\"[error before]\\n{ctx['error']}\")\n",
    "\n",
    "    # Formulate prompt and integrating the original user's request and the collected outputs\n",
    "    blob = \"\\n\\n\".join(parts) if parts else \"(no inputs to summarize)\"\n",
    "    prompt = f\"\"\"\n",
    "    You are part of an agentic task chain. \n",
    "    The original user prompt was:\\n{ctx.get(\"user_task\")}\\n\n",
    "    This was the full derived plan to solve this (in order of the list): \\n{ctx.get(\"plan\")}\\n\n",
    "    Summarize these results:\\n{blob}\\n\n",
    "    Give a short explanation and possibly some key takeaway if the output needs to be interpreted.\n",
    "    If the input is trivial and/or unambigous, just return it in a compact way and skip the takeaways.\n",
    "    If there is mainly a plot as result, simply refer to it.\n",
    "    It's possible that there have been errors before, keep that in mind and consider this in your response.\n",
    "    Your output will be prinited in the console, so keep that in mind for the formatting.\n",
    "    \"\"\" \n",
    "    # Let LLM answer this prompt and return the result\n",
    "    msg = await llm_chat(\n",
    "        [{\"role\":\"system\",\"content\":\"Explain clearly and briefly. Avoid jargon.\"},\n",
    "         {\"role\":\"user\",\"content\": prompt}]\n",
    "    )\n",
    "    return {\"answer_text\": msg.strip()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dd05b",
   "metadata": {},
   "source": [
    "### Intent registry\n",
    "\n",
    "To manage all the functionalities and handlers, we introduce a global register called `INTENT_REGISTRY`.\n",
    "Here we specify what inputs (\"uses\"), outputs (\"saves\") each of these functionality can expect and write to.\n",
    "This is very useful to easily select the right handlers and get all relevant information about them we can then provide the planner in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf706642-0ff7-4718-9a5a-383c16e70543",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_REGISTRY = {}\n",
    "INTENT_REGISTRY.update({\n",
    "    \"code_general\": {\n",
    "        \"params\": {\n",
    "            \"wants_plot\": \"Optional bool. If True, prefer producing a chart (save as plot.png).\",\n",
    "            \"result_columns\": \"list[str]. (e.g. ['abc', 'def']) If provided, validate result_csv has these columns. Do a PROPER LIST here as indicated or don'T provide this param at all!\"\n",
    "        },\n",
    "        \"uses\": [],\n",
    "        \"saves\": [\"stdout\", \"run_dir\", \"result_csv\", \"plot_png\", \"error\"],\n",
    "        \"fn\": handler_code_general,\n",
    "        \"description\": \"write & run small Python code (no specific CSV expected and can't read from the provided CSVs)\",\n",
    "    },\n",
    "    \"csv_analytics\": {\n",
    "        \"params\": {\n",
    "            \"wants_plot\": \"Optional bool. If True, prefer producing a chart (save as plot.png).\",\n",
    "            \"result_columns\": \"Optional list[str]. If provided, validate result_csv has these columns.\"\n",
    "        },\n",
    "        \"uses\": [],\n",
    "        \"saves\": [\"stdout\", \"run_dir\", \"result_csv\", \"plot_png\", \"error\"],\n",
    "        \"fn\": handler_csv_analytics,\n",
    "        \"description\": \"analyze a CSV and optionally produce a table (result_csv) and/or a plot (plot_png). If any CSV reading is required, use this.\",\n",
    "    },\n",
    "    \"summarize\": {\n",
    "        \"params\": {},\n",
    "        \"uses\": [\"answer_text\", \"stdout\", \"result_csv\", \"error\"],\n",
    "        \"saves\": [\"answer_text\"],\n",
    "        \"fn\": handler_summarize,\n",
    "        \"description\": \"summarize/clarify results from earlier steps\",\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e2bdb-9dfe-483d-875c-b794c78f3a60",
   "metadata": {},
   "source": [
    "This completes now all the functionalities we need for our agent. At this point, you might now want to revisit the `run_task` function and check the execution step in detail. Now you should be able to follow some of the details of this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aad11d-6bca-47a2-9ab4-20da9d7b93bb",
   "metadata": {},
   "source": [
    "## Running the agent\n",
    "\n",
    "Let's now try the agent with some requests!\n",
    "\n",
    "When the agent creates and runs code, you can inspect the generated files under `resources/runs/`.\n",
    "\n",
    "> **Note:** We are at several points parsing LLM outputs to desired formats. As you know LLM outputs can potentially be arbitrary, this is not guaranteed to suceed always. Moreover, this notebook is not desined to catch every possibly occuring *error*.\n",
    "> Hence, it is likely that on certain requests the pipeline will crash.\n",
    ">\n",
    "> Usually **retrying or changing the prompt** solves this.\n",
    "> \n",
    "> However, it is also interesting to analyze these crashes to understand why it fails! You can even try to **make it crash** with your prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943e4ff-8f71-4356-a9c1-ea1258a2aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1) A simple coding task\n",
    "user_task = \"calculate 17 times 23\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae2466-1b56-4714-bb88-9f4846c07456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2) A more advanced analytics task\n",
    "user_task = \"use the supermarkt csv data and figure out what's the most bought product. No plots needed, just the results\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed265f-c076-41ec-83aa-16f350b94f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3) Another analytics task with visualization\n",
    "user_task = \"check given the data, which swiss train station is the most busiest one on average per year and find a proper plot to visualize this\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f2fed-2c29-489e-9619-531524a38d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4) A challenging ML task. We didn't even design the agent for this, but it might be able to do this. Howver, this request also might fail.\n",
    "user_task = \"Use the bahnhof data and build a simple machine learning model that predicts the number of passengers in the future. Give me your prediction for Zurich HB 2025 and visualize the outcome. Do this in max two steps.\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258d063-4a11-456d-9931-5f0c5b87bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5) Ideas for further alternative tasks the agent might be able to solve\n",
    "user_task = \"plot the mandelbrot set\"\n",
    "user_task = \"visualize all siwss train stations on a map. make their size according to their average passenger number per year\"\n",
    "user_task = \"draw an image with a house, a tree, and a sun\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d1020",
   "metadata": {},
   "source": [
    "**(TO-DO)** Try out your own task requests that might be solvable via coding! Be creative and explore the limits of this agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3c02e-751b-425a-ba16-f56a170b4609",
   "metadata": {},
   "source": [
    "## Adding more functionality: Fridge management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e0260",
   "metadata": {},
   "source": [
    "Having now the coding functionality, we can also add new functionalities to our agent!\n",
    "In the next cells we will add (roughly) the same logic of the fridge management as we had it in the notebook before.\n",
    "\n",
    "As before, we define handler functions, register them in the `INTENT_REGISTRY`, add the corresponding server and try out new prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb41a5-5f4f-4c4a-a6ce-7b44d7330a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handler_fridge_add(step: Dict[str, Any], ctx: Dict[str, Any], **kw) -> Dict[str, Any]:\n",
    "    \"\"\"Adding items to the fridge (the fridge CSV file)\"\"\"\n",
    "    # Reading in relevant context\n",
    "    print_step(\"Fridge: add item\", level=2)\n",
    "    p = step.get(\"params\") or {}\n",
    "    item = (p.get(\"item\") or \"\").strip()\n",
    "    qty  = int(p.get(\"quantity\") or 1) # default 1\n",
    "    exp = str(p.get(\"expiry_date\") or (datetime.today() + timedelta(days=14)).strftime(\"%Y-%m-%d\")) # default 2 weeks from now\n",
    "    if not item:\n",
    "        msg = \"Missing 'item' in params.\"\n",
    "    else:\n",
    "        # Execute addition functionality on the server\n",
    "        out = await mcp(\"fridge:fridge.add\", item=item, quantity=qty, expiry_date=exp)\n",
    "        # Check if operation was successfull and formulate output message accordingly\n",
    "        if out[\"ok\"]:\n",
    "            msg = f\"Added {qty} × {item}\" + (f\" (exp {exp})\" if exp else \"\")\n",
    "        else:\n",
    "            msg = f\"error: {out['error']}, message: {out['message']}\"\n",
    "    print(msg)\n",
    "    return {\"answer_text\": msg, \"stdout\": msg}\n",
    "\n",
    "async def handler_fridge_remove(step: Dict[str, Any], ctx: Dict[str, Any], **kw) -> Dict[str, Any]:\n",
    "    \"\"\"Removing items to the fridge (the fridge CSV file)\"\"\"\n",
    "    # Reading in relevant context\n",
    "    print_step(\"Fridge: remove item\", level=2)\n",
    "    p = step.get(\"params\") or {}\n",
    "    item = (p.get(\"item\") or \"\").strip()\n",
    "    qty  = int(p.get(\"quantity\") or 1) # default 1\n",
    "    if not item:\n",
    "        msg = \"Missing 'item' in params.\"\n",
    "    else:\n",
    "        # Execute addition functionality on the server\n",
    "        out = await mcp(\"fridge:fridge.remove\", item=item, quantity=qty)\n",
    "        # Check if operation was successfull and formulate output message accordingly\n",
    "        if out[\"ok\"]:\n",
    "            msg = f\"Removed {qty} '{item}'\"\n",
    "        else:\n",
    "            msg = f\"error: {out['error']}, message: {out['message']}\"\n",
    "            \n",
    "    print(msg)\n",
    "    return {\"answer_text\": msg, \"stdout\": msg}\n",
    "\n",
    "async def handler_fridge_cost(step: Dict[str, Any], ctx: Dict[str, Any], **kw) -> Dict[str, Any]:\n",
    "    \"\"\"Estimating the total costs of the current fridge content using an LLM\"\"\"\n",
    "    # Reading in relevant context\n",
    "    print_step(\"Fridge: estimate total cost\", level=2)\n",
    "    inv = await mcp(\"fridge:fridge.list\")\n",
    "    rows = (inv or {}).get(\"rows\", [])\n",
    "    names = sorted({r.get(\"Item\",\"\") for r in rows if r.get(\"Item\")})\n",
    "\n",
    "    price_map_raw = {}\n",
    "    if names:\n",
    "        # Formulating prompt\n",
    "        pricing_prompt = (\n",
    "            \"You estimate typical Swiss supermarket prices in CHF for everyday groceries.\\n\"\n",
    "            \"Return ONLY valid JSON: an object that maps each item name to a positive float (unit price in CHF).\\n\"\n",
    "            \"Use the EXACT keys I provide (same casing) and assume standard package sizes when relevant.\\n\\n\"\n",
    "            \"Items to price:\\n\" + \"\\n\".join(f\"- {it}\" for it in names)\n",
    "        )\n",
    "        # Getting price estimations from LLM\n",
    "        resp = await llm_chat(\n",
    "            [{\"role\":\"system\",\"content\": load_prompts()[\"system_return_json\"]},\n",
    "             {\"role\":\"user\",\"content\": pricing_prompt}]\n",
    "        )\n",
    "        print(resp)\n",
    "        try:\n",
    "            price_map_raw = json.loads(resp)\n",
    "        except Exception:\n",
    "            price_map_raw = {}\n",
    "\n",
    "    # Create a proper price map\n",
    "    price_map: Dict[str, float] = {}\n",
    "    for k, v in (price_map_raw.items() if isinstance(price_map_raw, dict) else []):\n",
    "        try:\n",
    "            price_map[str(k)] = float(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Execute cost calculation functionality on the server\n",
    "    cost = await mcp(\"fridge:fridge.cost\", prices=price_map)\n",
    "\n",
    "    # Returning result\n",
    "    items = cost.get(\"items\", [])\n",
    "    total = cost.get(\"total_cost\")\n",
    "    cur = cost.get(\"currency\", \"CHF\")\n",
    "    missing = cost.get(\"missing_prices\", [])\n",
    "    msg = f\"Estimated total: {total} {cur} (pricing_mode={cost.get('pricing_mode','unknown')})\"\n",
    "    if missing:\n",
    "        msg += \" | Missing: \" + \", \".join(missing)\n",
    "\n",
    "    # Return compact text + in-memory table\n",
    "    return {\"answer_text\": msg, \"stdout\": msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6674b9-b029-49cc-a0db-c09c6fd7e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.add_server(\"fridge\", SERVERS_DIR / \"fridge_server.py\")\n",
    "\n",
    "INTENT_REGISTRY.update({\n",
    "    \"fridge_add\": {\n",
    "        \"params\": {\n",
    "            \"item\": \"str. Required. Name of the item to add.\",\n",
    "            \"quantity\": \"int. Optional; default 1.\",\n",
    "            \"expiry_date\": \"str. Optional ISO date (YYYY-MM-DD); default two weeks from today.\"\n",
    "        },\n",
    "        \"uses\": [],\n",
    "        \"saves\": [\"answer_text\", \"stdout\"],\n",
    "        \"fn\": handler_fridge_add,\n",
    "        \"description\": \"Add an item to the fridge with optional quantity and expiry date.\"\n",
    "    },\n",
    "    \"fridge_remove\": {\n",
    "        \"params\": {\n",
    "            \"item\": \"str. Required. Name of the item to remove.\",\n",
    "            \"quantity\": \"int. Optional; default 1.\",\n",
    "        },\n",
    "        \"uses\": [],\n",
    "        \"saves\": [\"answer_text\", \"stdout\"],\n",
    "        \"fn\": handler_fridge_remove,\n",
    "        \"description\": \"Remove an item from the fridge by name.\"\n",
    "    },\n",
    "    \"fridge_cost\": {\n",
    "        \"params\": {},\n",
    "        # If your handler reads a prices table from a previous step, leave 'result_csv' here.\n",
    "        # If not, you can set [].\n",
    "        \"uses\": [\"result_csv\"],\n",
    "        \"saves\": [\"answer_text\", \"stdout\"],\n",
    "        \"fn\": handler_fridge_cost,\n",
    "        \"description\": \"Estimate total fridge cost using built-in or provided price guesses.\"\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f59ffe-2296-4601-aa23-25e98a0b9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1) Simple fridge task\n",
    "user_task = \"Show me what's in the fridge!\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe0468-1fdd-41bd-a9e5-6488b86d2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2) More complex fridge task\n",
    "user_task = \"add two Tomatos to the fridge, then look again into the fridge data and show me what's actually in the fridge\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821ec0f-74f7-444a-81fe-0e08201d1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3) (Advanced) Analyze the output here, why does the summary here potentially fail?\n",
    "user_task = \"calculate the fridgs costs, remove then one Tomato from the fridge and then calculate the costs again\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152970e",
   "metadata": {},
   "source": [
    "**(TO-DO)** Try out additional prompts. Specifically, search for tasks that might require combinations of different functionalities, or, tasks, where it's not that trivial to come up with a good plan for the agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4810b05-b90e-40ad-9926-1eaa3af53b1c",
   "metadata": {},
   "source": [
    "## (TO-DO) Adding more functionality: Web Search\n",
    "\n",
    "Now we want to add a web search functionality. it enables our agent to do similar things as for example ChatGPT Web Search.\n",
    "\n",
    "We already provide the helper function below. Your task is to \"plug it in\" into our agent and test it with some requests!\n",
    "\n",
    "> **Note:** We are using a very simple open web browser version. Depending on the search request, the results can be potentially let's say creative, some might say very poor. Keep that in mind for your evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874327f0-d6a4-4222-9040-9a078fafff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handler_search_web(step: Dict[str, Any], ctx: Dict[str, Any], **kw) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM-in-the-loop search:\n",
    "      1) LLM proposes 1-3 concise queries (starting from the user question).\n",
    "      2) For each query, run search.query and (optionally) search.fetch_text.\n",
    "      3) Ask LLM if the gathered evidence is sufficient; if not, refine and loop (max ~2-3 rounds).\n",
    "      4) Print a compact final answer + top sources.\n",
    "\n",
    "    Returns:\n",
    "      {\"answer_text\": str, \"sources\": List[str]}\n",
    "    \"\"\"\n",
    "    print_step(\"Running web search (LLM-assisted)\")\n",
    "\n",
    "    # keep 'question' behavior identical to original, but read from step/ctx\n",
    "    question = (step.get(\"params\", {}).get(\"query\") or ctx.get(\"user_task\")).strip()\n",
    "\n",
    "    # 1. Ask LLM for a few smart queries\n",
    "    search_planner_prompt = f\"\"\"\n",
    "You are a precise search strategist. Given the user question below,\n",
    "return STRICT JSON with up to 3 short queries that would likely find the answer quickly.\n",
    "\n",
    "Return JSON ONLY:\n",
    "{{\n",
    "  \"queries\": [\"<short query 1>\", \"<short query 2>\", \"<short query 3>\"],\n",
    "  \"answer_style\": \"one-liner\" | \"bullets\" | \"short-paragraph\"\n",
    "}}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\".strip()\n",
    "\n",
    "    plan_json = json.loads(await llm_chat(\n",
    "        [{\"role\":\"system\",\"content\": load_prompts()[\"system_return_json\"]},\n",
    "         {\"role\":\"user\",\"content\": search_planner_prompt}]\n",
    "    ))\n",
    "\n",
    "    queries = [q for q in (plan_json.get(\"queries\") or []) if isinstance(q, str) and q.strip()] or [question]\n",
    "    answer_style = plan_json.get(\"answer_style\") or \"bullets\"\n",
    "\n",
    "    # 2. Loop: search → (optional) fetch → judge sufficiency\n",
    "    evidence = []  # list of {url,title,snippet,text?}\n",
    "    seen_urls = set()\n",
    "    max_rounds = min(3, len(queries) + 1)  # allow a refined query to be added\n",
    "\n",
    "    for round_idx in range(max_rounds):\n",
    "        q = queries[round_idx] if round_idx < len(queries) else queries[-1]\n",
    "        out = await mcp(\"search:search.query\", q=q, top_k=5)\n",
    "        results = (out or {}).get(\"results\", []) or []\n",
    "\n",
    "        print(f\"Search {round_idx+1}: “{(out or {}).get('query', q)}” → {len(results)} results\")\n",
    "\n",
    "        # Pull top 3 as evidence\n",
    "        top = results[:3]\n",
    "        for r in top:\n",
    "            url = r.get(\"url\", \"\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            item = {\n",
    "                \"url\": url,\n",
    "                \"title\": (r.get(\"title\") or \"\")[:200],\n",
    "                \"snippet\": (r.get(\"snippet\") or \"\")[:400],\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "            evidence.append(item)\n",
    "            seen_urls.add(url)\n",
    "\n",
    "        # 3. Ask LLM: do we have enough? If yes, produce the final compact answer\n",
    "        judge_prompt = f\"\"\"\n",
    "You are a careful answerer.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Evidence (array of objects with title, snippet, optional extracted text, url):\n",
    "{json.dumps(evidence)[:8000]}\n",
    "\n",
    "Decide if there is enough evidence to answer confidently. Then output STRICT JSON:\n",
    "\n",
    "{{\n",
    "  \"sufficient\": true | false,\n",
    "  \"refined_query\": \"<few-keyword refined query if not sufficient, else empty>\",\n",
    "  \"answer\": \"<a {answer_style} answer, extremely concise; if bullets, max 5 bullets; if one-liner, keep to one sentence>\",\n",
    "  \"sources\": [\"<up to 3 most relevant urls>\"]\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "        judge = json.loads(await llm_chat(\n",
    "            [{\"role\": \"system\", \"content\": load_prompts()[\"system_return_json\"]},\n",
    "             {\"role\": \"user\", \"content\": judge_prompt}]\n",
    "        ))\n",
    "\n",
    "        if judge.get(\"sufficient\"):\n",
    "            # 4. Final compact answer + 2–3 sources \n",
    "            ans = (judge.get(\"answer\") or \"\").strip()\n",
    "            if not ans:\n",
    "                ans = \"No concise answer could be generated.\"\n",
    "            print(\"\\n\" + ans)\n",
    "            srcs = [u for u in (judge.get(\"sources\") or []) if isinstance(u, str) and u.strip()][:3]\n",
    "            if srcs:\n",
    "                print(\"\\nSources:\")\n",
    "                for u in srcs:\n",
    "                    print(\"-\", u)\n",
    "            return {\"answer_text\": ans, \"sources\": srcs}\n",
    "\n",
    "        # Not sufficient → add refined query once if present and new\n",
    "        rq = (judge.get(\"refined_query\") or \"\").strip()\n",
    "        if rq and rq not in queries:\n",
    "            queries.append(rq)\n",
    "\n",
    "    # Fallback if loop ends without sufficient answer\n",
    "    print(\"Couldn’t confidently answer from snippets. Consider rephrasing or enabling `search.fetch_text` for deeper context.\")\n",
    "    return {\"answer_text\": \"\", \"sources\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84187424-985c-438d-86cd-aedd47ea4e64",
   "metadata": {},
   "source": [
    "Before actually adding the we search functionality, let's first try the following request. Try this request again after having added the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8044e66-c2b5-4b61-b733-fc59d381a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_task = \"What were the main events in 2025?\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b4b47-95cc-4858-bcc9-c28fa65c0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TO-DO) Add the web search server to client \n",
    "# ...\n",
    "# (TO-DO) Update the registry\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06b15e-0b3c-4e96-8699-57c105d27b52",
   "metadata": {},
   "source": [
    "Let's try the request from before again now with the functionality added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a621f-4000-4bdd-9a85-7c69634377be",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_task = \"What were the main events in 2025?\"\n",
    "await run_task(user_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1fa2e-88aa-4c4d-84d4-bae30cc318da",
   "metadata": {},
   "source": [
    "(TO-DO) Feel free to try out new requests, that might require the web search functionality combined with some of the functionalities from before and check if the agent can solve this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
