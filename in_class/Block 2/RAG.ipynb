{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnHjddYy4uud"
   },
   "source": [
    "<center>\n",
    "  <img src=\"https://python.langchain.com/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png\"\n",
    "       width=\"640\" alt=\"RAG concepts\">\n",
    "  <div><small><a href=\"https://python.langchain.com/docs/concepts/rag\">Source</a></small></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDSDxzFkScQz"
   },
   "source": [
    "# Building a minimal Retrieval-Augmented Generation pipeline\n",
    "\n",
    "In this tutorial, you will build a simple Retrieval-Augmented Generation pipeline using the [ETH Zurich Degree Programmes PDF](https://ethz.ch/content/dam/ethz/main/education/bachelor/studiengaenge/files/ETH-Zurich-Degree-programmes.pdf) as the corpus. We begin by showing why plain LLM queries on this document don't always work, continuing with setting up the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNbf4XlFkqCU"
   },
   "source": [
    "![](https://i.ibb.co/nsJTYh6j/LLM-azure-text-clean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqJOryuy790k"
   },
   "source": [
    "We will call the LLM via an API (Application Programming Interface) — a defined interface that allows two programs to interact (here, the notebook code and the LLM service). \n",
    "\n",
    "## Preparations\n",
    "As a first step, we need to install a few libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ipywidgets  # may need to update ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you restart the kernel so that the newly installed packages will be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching models using AzureOpenAI\n",
    "\n",
    "Here, we will use AzureOpenAI. We are hard-coding an Azure API key — note that we will disable this one after the block; so if you want to run this notebook afterwards, you have to set a different value for the  `azure_key`, otherwise you will get an error and will not be able to get responses from the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Technical set-up\n",
    "azure_key = \"986IfxLKwN3Paiq4yx1Kn2iTG7FyG2GxFg17qQSyr1KZqGLaizAGJQQJ99BCACI8hq2XJ3w3AAABACOGQfvw\"\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://cas-dml-llm.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-35-turbo\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", azure_key)\n",
    "\n",
    "# Initialize AzureOpenAI Service client with key-based authentication\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kz6A6AJ-CC-"
   },
   "source": [
    "We initialized the AzureOpenAI client and now want to send queries to the model. The system message sets the assistant’s behavior (answer in English, be helpful), and the user message contains our actual question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сore funtion to interact with the llm over the API\n",
    "def get_ai_response(query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer questions in English.\"},  # set the behavior\n",
    "        {\"role\": \"user\", \"content\": query}  # our question\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,  # deployment name in Azure\n",
    "        messages=messages,\n",
    "        temperature=0,  # deterministic answer\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content  # extract AI's reply; choices containts possible model answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZszQgubS-jkW"
   },
   "source": [
    "Now, let us check the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is RAG in AI?\"\n",
    "answer = get_ai_response(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULz6eCbycVe"
   },
   "source": [
    "## Why we need RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWEDB5L1P18w"
   },
   "source": [
    "The test answer seems nice. Now, let's try asking something more local and recent. The question we want to ask is: “How many ETH Zurich spin-off companies were founded in 2024?”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many ETH Zurich spin-off companies were founded in 2024?\"\n",
    "answer = get_ai_response(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yp-CvlglQJN1"
   },
   "source": [
    "**Why it happened?** The model doesn't have up-to-date or document-specific knowledge. Remember that the LLM’s knowledge comes from a fixed training snapshot. Therefore, if a fact is niche or very recent, the model likely didn’t see it during the training.\n",
    "\n",
    "So, let us try to enrich the prompt with relevant information extracted from the file about ETH Zurich so the model can answer based on facts, not guesswork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0T8qtIRk4oN"
   },
   "source": [
    "## What is RAG (briefly)\n",
    "\n",
    "* **R**etrieval — fetch relevant chunks from an external corpus (e.g., a PDF).\n",
    "\n",
    "* **A**ugmented **G**eneration — inject these chunks into the prompt so the LLM relies on facts.\n",
    "\n",
    "![](https://i.ibb.co/qMghzdwY/image.png)\n",
    "\n",
    "### Main steps:\n",
    "\n",
    "1. Extract text from the PDF → split it into chunks.\n",
    "\n",
    "2. Build vector representations of the chunks using a neural model.\n",
    "\n",
    "3. Create an index (a vector database).\n",
    "\n",
    "4. For each question → embed the question → search for the top-k most similar chunks → build a prompt like:\n",
    "\n",
    "```\n",
    "Context:\n",
    "<chunk1>\n",
    "<chunk2>\n",
    "\n",
    "Question: <question>\n",
    "\n",
    "Instructions: answer using only the context; if the answer is not present — say so.\n",
    "```\n",
    "\n",
    "5. Send this prompt to the model via AzureOpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfcFvPFSJIfC"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5kVm19cPK3k"
   },
   "source": [
    "### Step 1. Install dependencies and download the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM5W_6AWByns"
   },
   "source": [
    "The document we'll be using as basis for RAG is already available on RenkuLab. \n",
    "\n",
    "If you need to download a file from a public source on the internet, you can use the code below (With changed file names as necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the document by link\n",
    "\n",
    "# !wget --no-check-certificate \\\n",
    "#      --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64)\" \\\n",
    "#      -O ETH-Zurich-Degree-programmes.pdf \\\n",
    "#      \"https://ethz.ch/content/dam/ethz/main/education/bachelor/studiengaenge/files/ETH-Zurich-Degree-programmes.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMD5tfpqPQCq"
   },
   "source": [
    "### Step 2. Read the PDF and extract text\n",
    "\n",
    "Next, we open the pdf file with pypdf — a Python library for reading and manipulating PDFs (extract text, merge/split, rotate, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = Path(\"ETH-Zurich-Degree-programmes.pdf\")\n",
    "reader = PdfReader(FILE_PATH)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "entire_text = \"\"\n",
    "for page_num in range(number_of_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    entire_text += page.extract_text()\n",
    "\n",
    "# Let us have a look at the text\n",
    "entire_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21TxdoZcEImc"
   },
   "source": [
    "Hidden/template layers often get mixed into the plain text. For instance, \"Spitztitel Lorem Ipsum dolor sit amet\": German \"Spitztitel\" = running title; the Lorem Ipsum is template/placeholder text left in a master page layer. Text extractors still see it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu7BN-X1PWa0"
   },
   "source": [
    "### Step 3. Split text into chunks\n",
    "\n",
    "Now we’ll split the text into **chunks** — small, self-contained pieces that the model can search.\n",
    "\n",
    "We’ll use a ```RecursiveCharacterTextSplitter``` splitter, which cuts on natural boundaries (paragraphs → lines → words) and keeps a small overlap to preserve context.\n",
    "\n",
    "Note: depending on your data, you may wish to pick another splitter: Markdown (```MarkdownHeaderTextSplitter```), token-based (RecursiveTokenTextSplitter) for strict context budgets, or language-aware for code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain is an open-source framework for building LLM apps from modular blocks\n",
    "# We are going to only take the splitter from it\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "text_chunks = text_splitter.split_text(entire_text)\n",
    "print(f\"Total chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrzvVclD0SQZ"
   },
   "source": [
    "```chunk_size``` is the target length of each chunk (in tokens or characters, depending on the splitter), and ```chunk_overlap``` is how much content to repeat between adjacent chunks (e.g., 10–25%) so information that spans a boundary isn’t lost.\n",
    "\n",
    "**How to choose the chunk size and overlap?** A typical answer should fit in one chunk (sometimes two) without dragging along lots of extra text. Start with defaults, then test 5–10 real queries: if the relevant passage isn’t in the top-k, increase chunk size/overlap; if the context feels bloated, decrease them.\n",
    "\n",
    "**Larger** ```chunk_size``` generally increases recall — i.e., the chance that relevant content appears in the top-k — because more context stays together; but it also adds noise, slows search, and reduces diversity. **Smaller** ```chunk_size``` improves precision and speed but can split facts across chunks (mitigate with 10–20% overlap or a higher k).”\n",
    "\n",
    "Use ```chunk_overlap``` to protect **boundary info**: a moderate 10–20% overlap keeps cross-boundary details; higher (20–30%, e.g., for code/step-by-step docs) improves recall but bloats the index and yields near-duplicates; lower (0–10%) is lighter and faster but may miss boundary context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VflRNHYaT7r"
   },
   "source": [
    "Let us have a look at the first chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXIzD3ga4en9",
    "outputId": "b5a9f2c9-5ecc-42bc-aa69-7bce8ff3324e"
   },
   "outputs": [],
   "source": [
    "text_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gAniZshI9fb"
   },
   "source": [
    "## Make vector dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8TVlGoef1P6"
   },
   "source": [
    "## How to compare the strings (what is embeddings)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWhEFzibeh9n"
   },
   "source": [
    "Now we have a list of chunks (context passages).\n",
    "\n",
    "**What we want?** Given a question, we want to automatically retrieve the most relevant chunks and pass them to the model.\n",
    "\n",
    "**What do we need?** To do that, we need a way to measure the semantic similarity (closeness) between the question and each chunk.\n",
    "\n",
    "**How to compare the closeness?** Raw strings are hard to compare “by meaning.” So we turn each chunk into a vector of numbers (an **embedding**). Embeddings have a useful property: texts with similar meaning → nearby vectors (high cosine similarity). That lets us search by meaning, not exact words.\n",
    "\n",
    "**Why we have this property or what is an encoder?** Encoder is a pretrained model that maps text → vector. It’s trained so semantically similar texts land close together in vector space. Important: what counts as “similar” depends on the task, so different tasks may need different encoders.\n",
    "\n",
    "**How to pick an encoder?** There are [lots of encoders](https://huggingface.co/spaces/mteb/leaderboard)! Choose based on your task and technical needs.\n",
    "\n",
    "**What we'll do next?** In practice, we convert text to embeddings (vectors) and compare them (e.g., with cosine similarity), then send the top-k chunks to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb94E9N7s-R8"
   },
   "source": [
    "<center>\n",
    "  <img src=\"https://arize.com/wp-content/uploads/2022/06/blog-king-queen-embeddings.jpg\"\n",
    "       width=\"640\" alt=\"RAG concepts\">\n",
    "  <div><small><a href=\"https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/\">Source</a></small></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7aoecWJeX-S"
   },
   "source": [
    "### Step 1. Chunks to embeddings (vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false  # technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "acfa569a563846c984db03826a766f34",
      "1abdd43f775942b8b9b2d1ca6eb4eaf3",
      "dd44218817ba41c9924225e68a5996ea",
      "3039ad2f79504d0c945c27b45aa2eaf4",
      "480a10a63f3249478d76d60f59d2b43a",
      "b4300b8b622d4b4d8dcd60ba4167d32d",
      "a46ff1e6b02745689c62cbabb0798605",
      "6b6dc2afdbcb4ee9899e8157024d4693",
      "b19d66537801475e9a1cc287881ce487",
      "efa4d6322acc4f32b3142bdcef49701b",
      "377ec84efaad40d5b218b6dd8d8df618"
     ]
    },
    "id": "F4H9TBkGuaEH",
    "outputId": "d2783eb0-9535-4970-f860-d53fc2b23453"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download a model to create vector representations of text\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode text chunks into embeddings (one vector per chunk)\n",
    "embeddings = model.encode(text_chunks, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC8JQcBkulpO"
   },
   "source": [
    "### Step 2. Make a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOz1QNMjQ8Nn"
   },
   "source": [
    "To store embeddings and search for the nearest embeddings fastly, we use [FAISS](https://github.com/facebookresearch/faiss) (Facebook AI Similarity Search). It is a fast library for finding nearest vectors. A **FAISS index** is both a container for your embedding vectors and the search method that makes lookups fast.\n",
    "\n",
    "Let's make a FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09OZ1KTr2pTS"
   },
   "outputs": [],
   "source": [
    "# Create a FAISS index for efficient similarity search\n",
    "\n",
    "import faiss\n",
    "embeddings = embeddings.astype(\"float32\")\n",
    "\n",
    "# Cosine-similarity trick: L2-normalize so inner product ≈ cosine similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Build an exact inner-product index\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQiEqRkDusEW"
   },
   "source": [
    "## Getting the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u17GkTlEzv7N"
   },
   "source": [
    "Now, we will do a final step! For each question → embed the question → search for the top-k most similar chunks → build a prompt like:\n",
    "\n",
    "```\n",
    "Context:\n",
    "<chunk1>\n",
    "<chunk2>\n",
    "\n",
    "Question: <question>\n",
    "\n",
    "Instructions: answer using only the context; if the answer is not present — say so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA__GWgq3jms"
   },
   "source": [
    "### Step 1. Embed the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1saGfAsF3q06"
   },
   "outputs": [],
   "source": [
    "query = \"How many ETH Zurich spin-off companies were founded in 2024?\"\n",
    "\n",
    "# reminder: model is an embedding model we initialized before\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwrI5SHc3-jc"
   },
   "source": [
    "### Step 2. Search for the top-k most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mYoz74Y9KHg",
    "outputId": "f6582c74-5d2b-4ef9-b962-5829a02c4932"
   },
   "outputs": [],
   "source": [
    "# reminder: index is FAISS index (vector database) we have made before\n",
    "D, I = index.search(query_embedding, k=2)  # search for top-2 closest chunks\n",
    "\n",
    "retrieved_chunks = [text_chunks[i] for i in I[0]]  # I has a (n_queries, k) shape; we have 1 query\n",
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOW09JTm44q1"
   },
   "source": [
    "**The chunk we need**. The nearest chunk was retrieved correctly, but the content is low-quality. We need the following fragment, but it’s corrupted by text extraction:\n",
    ">Switzerland created this \\n place of innovation and \\nknowledge\\nspin-off companies \\nfounded in 2024 \\nof which CHF 1.42bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUztdmD_KLvH"
   },
   "source": [
    "**Why it happened?** The problem is that while the PDF looks fine to a human, the computer fails to capture the number 37 in the extracted text. Let us have a look at the PDF fragment we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdqlBrm86gfP"
   },
   "source": [
    "<center>\n",
    "  <img src=\"https://i.ibb.co/pv9j6FzJ/image.png\"\n",
    "       width=\"640\" alt=\"RAG concepts\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFOhOsoi7GL-"
   },
   "source": [
    "### Get the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOUSl8KRQjI8"
   },
   "source": [
    "Finally, we’ll generate an answer by injecting the retrieved chunks into the model prompt. First, let’s refactor the AI interuction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzLHf2Z1-ncS"
   },
   "outputs": [],
   "source": [
    "def get_ai_response_with_context(query, context):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant that answers questions based on the provided information.\"\n",
    "            },  # set the behavior\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {query}\\n\\nContext:\\n{context}\"\n",
    "            }\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,  # deployment name in Azure\n",
    "        messages=messages,\n",
    "        temperature=0,  # deterministic answer\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content  # extract AI's reply; choices containts possible model answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHZEZhm-_hQr"
   },
   "source": [
    "Now, we can get an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7iGZ1Hj_CJp",
    "outputId": "18a1eef5-d8ab-4b43-eb7b-fffc730e6cb6"
   },
   "outputs": [],
   "source": [
    "query = \"How many ETH Zurich spin-off companies were founded in 2024?\"\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "answer = get_ai_response_with_context(query, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvDSA0tVKkva"
   },
   "source": [
    "As we have seen, the reason is data preparation. Let's fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVpS6nMDKpIt"
   },
   "source": [
    "## Another try: data preparation using Docling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxWK5nBMRPWW"
   },
   "source": [
    "### Step 1. Data preparation using Docling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srgg1O2xRURt"
   },
   "source": [
    "Our goal is to extract text from PDF correctly. We use the [Docling](https://github.com/docling-project/docling?tab=readme-ov-file) library to convert PDF into structured Markdown. Unlike simple text extraction, Docling combines existing text with OCR (text from scanned pages) and preserves basic structure such as headings, paragraphs, and tables.\n",
    "\n",
    "This is a pretty compute-intensive task, so it will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVX5nfX8_CNI",
    "outputId": "6b540f33-4760-4001-86d2-7e4eb15217fe"
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(\"ETH-Zurich-Degree-programmes.pdf\")\n",
    "\n",
    "# the below code can be used to work with pdf files that are available on the internet:\n",
    "# source = \"https://ethz.ch/content/dam/ethz/main/education/bachelor/studiengaenge/files/ETH-Zurich-Degree-programmes.pdf\"  # document per local path or URL\n",
    "# converter = DocumentConverter()\n",
    "# result = converter.convert(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = result.document.export_to_markdown()\n",
    "print(md[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ui3qD40oKxtI"
   },
   "source": [
    "As we see, it's much better. Let us clean the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyQVYbpHRjGt"
   },
   "source": [
    "### Step 2. Clean the extracted text\n",
    "\n",
    "The text still contains noise: invisible placeholders, fake headings, and formatting artifacts. This function cleans the text by removing image markers, “lorem ipsum” placeholders, and page artifacts, while also fixing line breaks and hyphenated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "iLscGprgDPJ9",
    "outputId": "ff6727e5-9af9-4d1c-ed49-c87451fa82f3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_docling_markdown(text: str) -> str:\n",
    "    # remove comments <!-- image -->\n",
    "    text = re.sub(r\"<!--\\s*image\\s*-->\\s*\", \"\", text, flags=re.I)\n",
    "\n",
    "    # remove obvious placeholder strings\n",
    "    placeholders = [\n",
    "        r\"lorem ipsum.*\",   # lorem ipsum ...\n",
    "        r\"spitztitel\",      # layout headline\n",
    "        r\"dummy\",           # the word \"dummy\"\n",
    "        r\"platzhalter\",     # \"placeholder\" in German\n",
    "        r\"placeholder\",     # \"placeholder\" in English\n",
    "    ]\n",
    "    text = re.sub(\"(?mi)^(\" + \"|\".join(placeholders) + r\")\\s*$\", \"\", text)\n",
    "\n",
    "    # normalize line breaks and spaces\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)              # remove trailing spaces before newline\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)              # maximum two consecutive newlines\n",
    "\n",
    "    # merge word breaks across line breaks: \"Spin-\\noff\" -> \"Spinoff\"\n",
    "    text = re.sub(r\"(\\w)[\\-–]\\n(\\w)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # remove very short single-line \"placeholder headings\"\n",
    "    text = re.sub(r\"(?m)^\\s*[A-ZÄÖÜ][A-Za-zÄÖÜäöüß]{1,12}\\s*$\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "clean_md = clean_docling_markdown(md)\n",
    "clean_md[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o62i866QymFo"
   },
   "source": [
    "We don’t remove the page numbers here because the PDF’s layout makes it easy to accidentally delete other content. You might normally do this to reduce unnecessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a_UZl3eR9GM"
   },
   "source": [
    "### Step 3. Split the text into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASgAjk14DPjk"
   },
   "source": [
    "Now, we should just repeat the steps we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkWWgFRCDd3i",
    "outputId": "d600aa73-4860-46b6-faa2-fa58a2584dbc"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(clean_md)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Example chunk: \\n {chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn2-zJWoK51c"
   },
   "source": [
    "Looks nice, let us repeat the vector dataset preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvvrx3X0SAt-"
   },
   "source": [
    "### Step 4. Encode chunks and create FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c0f54874226d47f688de480ca728764d",
      "f2eab58db2ce4026afe450f137f99b06",
      "fde033324c4044e787e829a1882ad6fd",
      "afa693ddfdc1485f956d73c6966e225c",
      "56a53436231d411885ea760ce8bd12fe",
      "6a204c5fe3a94028ba6393b47c739b41",
      "a211a3b8940949e58cee2cba20e4379a",
      "faa175ae97f74c7793f535ae171e041e",
      "6c603984c4e94e17b0a8db49740d67cd",
      "e22b32b04a314b6485632184713c898e",
      "ba3cbbb4a9d24c2198af182050c39ff0"
     ]
    },
    "id": "TPvqY_MrD5fT",
    "outputId": "80613e70-e210-4bea-d7ba-4a84a0b5c5d1"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunks, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "embeddings = embeddings.astype(\"float32\")\n",
    "faiss.normalize_L2(embeddings)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdqO2YpESHPw"
   },
   "source": [
    "### Step 5. Retrieve top chunks for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfUTR323D_2j",
    "outputId": "9167a391-39b2-4872-f7ba-6553ae72941f"
   },
   "outputs": [],
   "source": [
    "query = \"How many ETH Zurich spin-off companies were founded in 2024?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "D, I = index.search(query_embedding, k=2)  # get top-2 closest chunks\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in I[0]]\n",
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d6AkQzMSKdj"
   },
   "source": [
    "### Step 6. Ask the LLM with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLq1CuGIDpCy",
    "outputId": "e5a8321a-7fb1-4d81-c3db-1081edae7e81"
   },
   "outputs": [],
   "source": [
    "query = \"How many ETH Zurich spin-off companies were founded in 2024?\"\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "answer = get_ai_response_with_context(query, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcoyl3VbE96-"
   },
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUIPry_UFO_5"
   },
   "source": [
    "### Extend a Query to a RAG Query\n",
    "\n",
    "You want to know how many patent applications were reported at ETH Zurich in 2024. You ask the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many patent applications were reported at ETH Zurich in 2024?\"\n",
    "answer = get_ai_response(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-TaToUDFZwK"
   },
   "source": [
    "**Task 1:** Fix the situation, using the same PDF about degree programs at ETH. In particular, you should:\n",
    "* get a query embedding\n",
    "* find the chunks closest to the query (in embedding space)\n",
    "* build a list of the retrieved chunks, and add this to the context\n",
    "* get an AI response for a query with the constructed context.\n",
    "\n",
    "Note that all the necessary function calls are already available above — so you just need to find the relevant code and put it together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of `chunk_size` and `chunk_overlap` parameters\n",
    "To get an intuition for the impact of the two parameters, you will experiment with the ```chunk_size``` and ```chunk_overlap``` parameters.\n",
    "\n",
    "**Task 2:**\n",
    "\n",
    "(a) Write a function that takes a cleaned Markdown string (we already have it!), chunk_size, chunk_overlap, and a query. It should return the retrieved chunks and the model’s answer. The function should do the following:\n",
    "\n",
    "1. Split the cleaned Markdown into overlapping text chunks.\n",
    "\n",
    "2. Encode the chunks with a SentenceTransformer.\n",
    "\n",
    "3. Build a FAISS inner-product index (cosine similarity on L2-normalized vectors).\n",
    "\n",
    "4. Retrieve the top-k chunks most similar to the query (use a reasonable default, e.g., k = 2).\n",
    "\n",
    "5. Call a LLM helper to produce an answer from the (query + concatenated context).\n",
    "\n",
    "Again, note that all relevant parts are already given above - you just have to select and combine.\n",
    "\n",
    "(b) Examine the function’s behavior for different chunk sizes and overlaps. Describe your findings.\n",
    "\n",
    "You may want to print or inspect the retrieved chunks to see how they affect the final answer.\n",
    "\n",
    "**For your experiments, here are sample questions with their answers (based on the document)**:\n",
    "\n",
    "1. How many ETH Zurich spin-off companies were founded in 2024? — 37\n",
    "\n",
    "2. At what semester will ETH Zurich change its academic calendar? — Autumn semester 2027\n",
    "\n",
    "3. How many patent applications were reported at ETH Zurich in 2024? — More than 100\n",
    "\n",
    "4. What percentage of Master’s graduates at ETH Zurich have a job one year after graduation? — 97%\n",
    "\n",
    "5. After Autumn Semester 2027, what will be the maximum permitted duration of studies? — 6 years"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1abdd43f775942b8b9b2d1ca6eb4eaf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4300b8b622d4b4d8dcd60ba4167d32d",
      "placeholder": "​",
      "style": "IPY_MODEL_a46ff1e6b02745689c62cbabb0798605",
      "value": "Batches: 100%"
     }
    },
    "3039ad2f79504d0c945c27b45aa2eaf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efa4d6322acc4f32b3142bdcef49701b",
      "placeholder": "​",
      "style": "IPY_MODEL_377ec84efaad40d5b218b6dd8d8df618",
      "value": " 3/3 [00:00&lt;00:00,  4.04it/s]"
     }
    },
    "377ec84efaad40d5b218b6dd8d8df618": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "480a10a63f3249478d76d60f59d2b43a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a53436231d411885ea760ce8bd12fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a204c5fe3a94028ba6393b47c739b41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b6dc2afdbcb4ee9899e8157024d4693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c603984c4e94e17b0a8db49740d67cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a211a3b8940949e58cee2cba20e4379a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a46ff1e6b02745689c62cbabb0798605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acfa569a563846c984db03826a766f34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1abdd43f775942b8b9b2d1ca6eb4eaf3",
       "IPY_MODEL_dd44218817ba41c9924225e68a5996ea",
       "IPY_MODEL_3039ad2f79504d0c945c27b45aa2eaf4"
      ],
      "layout": "IPY_MODEL_480a10a63f3249478d76d60f59d2b43a"
     }
    },
    "afa693ddfdc1485f956d73c6966e225c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e22b32b04a314b6485632184713c898e",
      "placeholder": "​",
      "style": "IPY_MODEL_ba3cbbb4a9d24c2198af182050c39ff0",
      "value": " 4/4 [00:00&lt;00:00,  5.51it/s]"
     }
    },
    "b19d66537801475e9a1cc287881ce487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b4300b8b622d4b4d8dcd60ba4167d32d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3cbbb4a9d24c2198af182050c39ff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0f54874226d47f688de480ca728764d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2eab58db2ce4026afe450f137f99b06",
       "IPY_MODEL_fde033324c4044e787e829a1882ad6fd",
       "IPY_MODEL_afa693ddfdc1485f956d73c6966e225c"
      ],
      "layout": "IPY_MODEL_56a53436231d411885ea760ce8bd12fe"
     }
    },
    "dd44218817ba41c9924225e68a5996ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b6dc2afdbcb4ee9899e8157024d4693",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b19d66537801475e9a1cc287881ce487",
      "value": 3
     }
    },
    "e22b32b04a314b6485632184713c898e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efa4d6322acc4f32b3142bdcef49701b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2eab58db2ce4026afe450f137f99b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a204c5fe3a94028ba6393b47c739b41",
      "placeholder": "​",
      "style": "IPY_MODEL_a211a3b8940949e58cee2cba20e4379a",
      "value": "Batches: 100%"
     }
    },
    "faa175ae97f74c7793f535ae171e041e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fde033324c4044e787e829a1882ad6fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faa175ae97f74c7793f535ae171e041e",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c603984c4e94e17b0a8db49740d67cd",
      "value": 4
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
