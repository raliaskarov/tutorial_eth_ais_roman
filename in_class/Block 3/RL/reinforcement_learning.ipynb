{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning notebook\n",
    "\n",
    "Reinforcement learning (RL) is a fundamental branch of machine learning where an agent learns to make optimal decisions by interacting with an environment. This notebook focuses on action selection strategies in a reinforcement learning context, specifically within the Frozen Lake environment.\n",
    "\n",
    "## Objective\n",
    "Our primary goal is to implement and understand the Q-learning algorithm. We will create an action selection function that determines the next move of an agent based on the following approach:\n",
    "\n",
    "Exploration: The agent selects a random action to gather more knowledge about the environment.\n",
    "Exploitation: The agent chooses the action that maximizes its expected future rewards based on prior experience (Q-values).\n",
    "By implementing and fine-tuning this strategy, we aim to enhance the agent’s ability to navigate efficiently through the environment.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* Understanding the balance between exploration and exploitation.\n",
    "* Implementing ε-greedy action selection.\n",
    "* Leveraging a Q-table to improve decision-making.\n",
    "* Applying concepts to a real reinforcement learning problem using OpenAI Gym’s Frozen Lake environment.\n",
    "\n",
    "Now, let’s dive in and start implementing our action selection strategy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5o2k0d6sL9r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds3wZZcZsL9t"
   },
   "outputs": [],
   "source": [
    "# Define the environment as a grid map\n",
    "# 'S' = Start, 'F' = Frozen surface, 'H' = Hole (fall in = lose), 'G' = Goal (reach = win)\n",
    "env_map = [\n",
    "    ['S', 'F', 'F', 'F'],\n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G']\n",
    "]\n",
    "\n",
    "\n",
    "# Total number of states (each cell in the grid is a state)\n",
    "# len(env_map) -> number of rows\n",
    "# len(env_map[0]) -> number of columns\n",
    "num_states = len(env_map) * len(env_map[0])\n",
    "\n",
    "# Number of possible actions (Up, Down, Left, Right)\n",
    "num_actions = 4\n",
    "\n",
    "# Whether the environment is slippery (affects movement randomness)\n",
    "is_slippery = False  # If False, movement is deterministic; if True, the agent can slip\n",
    "\n",
    "# Maximum exploration rate (initially the agent explores a lot)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Minimum exploration rate (ensures some exploration even after training)\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "# This table stores the learned Q-values for each (state, action) pair\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Start at the initial state (usually the top-left corner of the grid)\n",
    "state = 0  # This corresponds to the 'S' position in the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz8Zs9eisL9u"
   },
   "source": [
    "# Task 0: Implementing the Action Selection Strategy\n",
    "\n",
    "In this task, we will create a function `get_next_action` that determines the next action our agent should take in the **Frozen Lake** environment. The action selection is based on the **exploration vs. exploitation** strategy:\n",
    "\n",
    "1. **Generate** a random number and compare it with the `exploration_rate`.\n",
    "2. If the random number is **less than** the exploration rate, the agent will **explore** by selecting a random action.\n",
    "3. Otherwise, the agent will **exploit** the knowledge from the Q-table by choosing the action with the highest Q-value.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "In the code cell below, fill in the two lines marked with `# TODO`:\n",
    "\n",
    "- **One line** to select a random action (exploration).\n",
    "- **One line** to select the best-known action (exploitation).\n",
    "\n",
    "### Helpful Hints:\n",
    "\n",
    "- Select a random action from the available actions using `random.choice(...)`. Since the Q-table's shape is `[number_of_states, number_of_actions]`, the number of available actions is given by `q_table.shape[1]`.\n",
    "- `random.choice(range(10))` produces random numbers from 0 to 9 (including).\n",
    "- Find the best action using `np.argmax(...)` on the Q-values for the current state. Remember, the shape of `q_table` is `[number_of_states, number_of_actions]`. Access the Q-values for all actions in the current state using `q_table[state]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kp7NfzKsL91"
   },
   "outputs": [],
   "source": [
    "def get_next_action(q_table, state, exploration_rate):\n",
    "    \"\"\"\n",
    "    Choose the next action using an epsilon-greedy exploration/exploitation strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - q_table (list): Q-values table with shape (num_states, num_actions).\n",
    "    - state (int): Current state index.\n",
    "    - exploration_rate (float): Probability of choosing a random action (epsilon).\n",
    "\n",
    "    Returns:\n",
    "    - action (int): Selected action index.\n",
    "    \"\"\"\n",
    "    # Compare a random number with the exploration_rate\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        # Explore: choose a random action\n",
    "        # TODO: Fill the next line to choose a random action (hint: random.choice or random.randint)\n",
    "        action = ...\n",
    "    else:\n",
    "        # Exploit: choose the best action based on Q-values\n",
    "        # TODO: Fill the next line to choose the action with the highest Q-value (hint: np.argmax)\n",
    "        action = ...\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4jjsadfsL92"
   },
   "source": [
    "# Task 1: Implementing the Q-learning Update Rule\n",
    "\n",
    "In this task, we will update our Q-table using the **Q-learning** update formula after selecting an action and observing the next state and reward:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left(r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- **$Q(s, a)$**: Current Q-value for state $s$ and action $a$.\n",
    "- **$\\alpha$ (learning rate)**: Controls how quickly the agent learns.\n",
    "- **$\\gamma$ (discount factor)**: Determines how much future rewards are valued.\n",
    "- **$r$**: Immediate reward received after taking the action.\n",
    "- **$\\max_{a'} Q(s', a')$**: Maximum Q-value among all possible actions in the next state $s'$.\n",
    "\n",
    "\n",
    "The update process involves the following steps:\n",
    "\n",
    "1. **Select** an action using the provided `get_next_action(state, exploration_rate)` function.\n",
    "2. **Perform** the selected action and observe the new state and the immediate reward.\n",
    "3. **Update** the Q-value for the state-action pair based on the observed reward and the estimated optimal future reward.\n",
    "4. **Return** the new state, the reward and indicate if the episode has ended.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "In the code cell below, complete the implementation by filling in the line marked with `# TODO`:\n",
    "\n",
    "- **One line** to update the Q-table entry for the current state-action pair using the Q-learning formula.\n",
    "\n",
    "### Helpful Hints:\n",
    "\n",
    "- Retrieve the maximum Q-value of the next state using `np.max(q_table[new_state])`.\n",
    "- Ensure the Q-table update exactly follows the provided formula, correctly applying the learning rate (`alpha`) and discount factor (`gamma`).\n",
    "\n",
    "\n",
    "### Understanding `get_next_state`\n",
    "\n",
    "The `get_next_state` function calculates the next state of the agent based on its current state and chosen action. It returns the actual action taken due to the fact that slippery environment may change the given action, the next state, the reward obtained (1 if the goal is reached, 0 otherwise), and a boolean indicating whether the episode has ended due to reaching a goal (`'G'`) or falling into a hole (`'H'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdCUbliWsL92"
   },
   "outputs": [],
   "source": [
    "from helper_functions import get_next_state\n",
    "\n",
    "def q_learning_execute_one_step(q_table, state, exploration_rate, learning_rate, discount_factor):\n",
    "    \"\"\"\n",
    "    Execute a single step of the Q-learning algorithm:\n",
    "\n",
    "    Parameters:\n",
    "    - q_table (list): Table containing Q-values for state-action pairs.\n",
    "    - state (int): Current state of the agent.\n",
    "    - exploration_rate (float): Probability of choosing a random action (exploration vs. exploitation).\n",
    "    - learning_rate (float): Rate at which the Q-values are updated.\n",
    "    - discount_factor (float): How much future rewards are valued.\n",
    "\n",
    "    Returns:\n",
    "    - new_state (int): The next state after executing the action.\n",
    "    - reward (int): Immediate reward after taking the action at given state\n",
    "    - done (bool): Flag indicating whether the episode has ended (i.e., hitting on a hole or present).\n",
    "    \"\"\"\n",
    "    # Step 1: Choose an action\n",
    "    action = get_next_action(q_table, state, exploration_rate)\n",
    "\n",
    "    # Step 2: Execute the action in the environment\n",
    "    # Warning: Here, get_next_state(...) may return different action than the given one if the environment is slippery.\n",
    "    action, new_state, reward, done = get_next_state(state, action)\n",
    "\n",
    "    # Step 3: Update the Q-table\n",
    "    # TODO: Fill in the next line using the Q-learning formula (line 10)\n",
    "    q_table[state, action] = ...\n",
    "\n",
    "    # Step 4: Return the new state and done flag\n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Izo7An2sL92"
   },
   "source": [
    "# Task 2: Running One Episode of Q-Learning\n",
    "\n",
    "In this task, we will run a full episode of **Q-learning**, meaning the agent will interact with the environment for multiple steps until the episode ends or the maximum number of steps is reached.\n",
    "\n",
    "### Each episode follows this process:\n",
    "1. **Start** from the given state.\n",
    "2. **Repeat** for a fixed number of steps (or until the episode ends):\n",
    "   - Take one step of Q-learning.\n",
    "   - **Accumulate** the total reward, and update the current state.\n",
    "   - **Stop** if the episode has ended.\n",
    "3. **Return** the total reward earned in the episode.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Complete the missing part inside the `for` loop in the function below. You need to:\n",
    "\n",
    "- **Call** `q_learning_execute_one_step(...)` to execute one step of Q-learning.\n",
    "- **Update** `total_reward` with the received reward.\n",
    "- **Update** `state` with the new state.\n",
    "- **Check** if `done` is `True`, **stop** the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFla8SLMsL93"
   },
   "outputs": [],
   "source": [
    "def q_learning_execute_one_episode(q_table, state, max_steps, exploration_rate, learning_rate, discount_factor):\n",
    "    \"\"\"\n",
    "    Run one episode of Q-learning.\n",
    "\n",
    "    Parameters:\n",
    "    - q_table (list): Table containing Q-values for state-action pairs.\n",
    "    - state (int): Initial state of the agent.\n",
    "    - max_steps (int): Maximum number of steps per episode.\n",
    "    - exploration_rate (float): Probability of choosing a random action (exploration).\n",
    "    - learning_rate (float): Rate at which the Q-values are updated.\n",
    "    - discount_factor (float): How much future rewards are valued.\n",
    "\n",
    "    Returns:\n",
    "    - total_reward (int): The sum of all rewards received in the episode.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        # TODO: Call the function that executes one step of Q-learning and update variables\n",
    "        new_state, reward, done = ...\n",
    "        total_reward = ...\n",
    "        state = ...\n",
    "\n",
    "        # TODO: Check if the episode has ended\n",
    "        if ...:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlA76Iy9sL93"
   },
   "source": [
    "# Task 3: Running Multiple Episodes of Q-Learning with Exploration Rate Decay  \n",
    "\n",
    "In this task, we will run the **Q-learning algorithm** for multiple episodes, allowing the agent to explore and improve its decision-making over time.  \n",
    "\n",
    "### The Q-learning process follows these steps:\n",
    "1. **Loop over multiple episodes** to allow the agent to learn over time.\n",
    "2. **In each episode**:\n",
    "   - Run one full episode of Q-learning.\n",
    "   - Store the **total reward** received in the episode.\n",
    "   - Adjust the **exploration rate** using an exponential decay function.\n",
    "3. **Return** the final exploration rate after all episodes.\n",
    "\n",
    "\n",
    "## Exploration Rate Decay  \n",
    "\n",
    "In **Q-learning**, the agent must balance between:  \n",
    "- **Exploration** (trying new actions to discover better rewards).  \n",
    "- **Exploitation** (choosing the best-known action based on learned values).  \n",
    "\n",
    "To **control this balance**, we gradually **decrease the exploration rate** over time, encouraging the agent to exploit learned knowledge in later episodes. We use the following decay function to adjust the **exploration rate**:\n",
    "\n",
    "$$\n",
    "\\text{exploration\\_rate} = \\text{min\\_exploration\\_rate} + (\\text{max\\_exploration\\_rate} - \\text{min\\_exploration\\_rate}) \\times e^{-\\text{decay\\_rate} \\times \\text{episode}}\n",
    "$$\n",
    "\n",
    "### Parameters:\n",
    "- **`min_exploration_rate`**: The minimum exploration rate (ensures the agent always explores a little).\n",
    "- **`max_exploration_rate`**: The initial exploration rate (starts high for more exploration).\n",
    "- **`decay_rate`**: Controls how quickly the exploration rate decreases.\n",
    "- **`episode`**: The current episode number.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"./assets/exploration_rate_decay.png\" alt=\"Exploration Rate Decay\" width=\"600\">\n",
    "        <figcaption><b>Figure 1:</b> Exploration rate decay over episodes (decay rate = 0.001)</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Complete the missing parts in the function below. You need to:\n",
    "- **Call** `q_learning_execute_one_episode(...)` to execute one episode of Q-learning.\n",
    "- **Update** the `exploration_rate` to decrease over time using the given formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEVpcqd5sL93"
   },
   "outputs": [],
   "source": [
    "def q_learning(q_table, state, num_episodes, max_steps, learning_rate, discount_factor, exploration_rate, decay_rate):\n",
    "    \"\"\"\n",
    "    Run the Q-learning algorithm for a specified number of episodes.\n",
    "\n",
    "    Parameters:\n",
    "    - q_table (list): Table containing Q-values for state-action pairs.\n",
    "    - state (int): Initial state of the agent.\n",
    "    - num_episodes (int): Total number of episodes to run.\n",
    "    - max_steps (int): Maximum steps per episode.\n",
    "    - learning_rate (float): Rate at which Q-values are updated.\n",
    "    - discount_factor (float): How much future rewards are valued.\n",
    "    - exploration_rate (float): Initial probability of choosing a random action.\n",
    "    - decay_rate (float): Factor by which exploration_rate decreases over episodes.\n",
    "\n",
    "    Returns:\n",
    "    - exploration_rate (float): Updated exploration rate after all episodes.\n",
    "    - episode_rewards\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        # TODO: Task 3, Call the function to execute one episode and store the reward\n",
    "        total_reward = ...\n",
    "\n",
    "        # TODO: Task 4, Update the exploration rate to gradually decrease\n",
    "        # Do not forget the minus sign in np.exp(...).\n",
    "        exploration_rate = ... + (... - ...) * np.exp(...)\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return exploration_rate, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3Oj85cZTsSt"
   },
   "source": [
    "## Playing the game\n",
    "\n",
    "We now display the frozen lake game with the model that we trained. For this, run the cell below. Then click on ''run N episodes''. In the lower part of the window you can click on the button that displays the logs and then see how your program would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJqF8fmpsL93"
   },
   "outputs": [],
   "source": [
    "%run -i helper_functions.py\n",
    "run_game()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
