{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecf719b",
   "metadata": {},
   "source": [
    "# Autoencoder Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a3c2e",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using 10 epochs will make this notebook run rather quickly.\n",
    "# If you have time and are willing to wait a bit longer for better results, increase this value to e.g., 50.\n",
    "nEpochs = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4207a5eeb7dc59e7"
  },
  {
   "cell_type": "markdown",
   "id": "0e32db67",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_val_images, train_val_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Scale image data:\n",
    "train_val_images = train_val_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Split into training / validation\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels,\n",
    "                                                                      test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddaf33-3cc3-4ca0-8fc1-74d1b35f15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_label_df = pd.DataFrame(train_val_labels)\n",
    "train_val_label_df.columns = ['label']\n",
    "train_val_label_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11e168-87f4-458b-ac21-81fb7648510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_df = pd.DataFrame(val_labels)\n",
    "val_label_df.columns = ['label']\n",
    "val_label_df['label'].value_counts(sort=False, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c3db4",
   "metadata": {},
   "source": [
    "## The autoencoder\n",
    "Every autoencoder consists of two parts: an encoder and a decoder.\n",
    "\n",
    "* The **encoder** receives the original data (in our case, the black and white images) as input and generates a lower-dimensional code from it.\n",
    "* The **decoder** receives the code and decodes it into original data (e.g. the images) in the same format as the encoder's inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e6f7b",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd56fb0",
   "metadata": {},
   "source": [
    "The two parts put together form the autoencoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1a773-7283-4330-838a-c8ece79d3de1",
   "metadata": {},
   "source": [
    "**Comments on activation functions:**\n",
    "\n",
    "***Why ReLU?***\n",
    "\n",
    "* Simplicity and Efficiency: ReLU (Rectified Linear Unit) is computationally efficient because it involves simple thresholding at zero. This makes it faster to compute compared to other activation functions.\n",
    "* Sparse Activation: ReLU promotes sparsity in the network by setting negative values to zero, which can help in learning more robust features.\n",
    "* Gradient Propagation: ReLU helps mitigate the vanishing gradient problem, allowing gradients to propagate more effectively during backpropagation.\n",
    "\n",
    "***Sigmoid*** at the end to get an output between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628fd6f-3c94-4d49-8f00-3f9e946db46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")\n",
    "])\n",
    "\n",
    "CNNmnist_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (7, 7, 64)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n",
    "    tf.keras.layers.UpSampling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n",
    "    tf.keras.layers.UpSampling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n",
    "])\n",
    "\n",
    "CNNmnist_ae = tf.keras.Sequential([CNNmnist_encoder, CNNmnist_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1842b1",
   "metadata": {},
   "source": [
    "### Training\n",
    "The autoencoder uses unsupervised learning, i.e. we do not pass any predefined labels or similar. Instead, the output should be as similar as possible to the input. We use the squared error as a measure of the quality of the reconstruction.\n",
    "\n",
    "For a start, we allow a maximum of 10 epochs to train - as with the other convolutional networks (and with deep neural networks in general), you should allow significantly more epochs for real applications.\n",
    "\n",
    "The training takes a little longer here... With the following code you can save and reload the learned weights - just set `train_from_scratch` as needed.\n",
    "Please note that only the weights, not the models themselves, are saved and loaded again. The model definition is made in the code and managed as such.\n",
    "\n",
    "Loading the weights only works if the model definition is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit the model\n",
    "tf.random.set_seed(42) \n",
    "CNNmnist_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb86e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPatience = 50\n",
    "\n",
    "# define paths:\n",
    "encoder_weights_path_10 = './CNNmnist_encoder_10.weights.h5'\n",
    "decoder_weights_path_10 = './CNNmnist_decoder_10.weights.h5'\n",
    "history_path = './CNNmnist_ae.history.h5'\n",
    "\n",
    "train_from_scratch = True\n",
    "if train_from_scratch:\n",
    "    history_ae = CNNmnist_ae.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                 callbacks=[EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                          verbose=False, restore_best_weights=True)])\n",
    "    # Save the weights:\n",
    "    CNNmnist_encoder.save_weights(encoder_weights_path_10)\n",
    "    CNNmnist_decoder.save_weights(decoder_weights_path_10)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history_ae, f)\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    CNNmnist_encoder.load_weights(encoder_weights_path_10)\n",
    "    CNNmnist_decoder.load_weights(decoder_weights_path_10)\n",
    "    # connect\n",
    "    CNNmnist_ae = tf.keras.Sequential([CNNmnist_encoder, CNNmnist_decoder])\n",
    "    CNNmnist_ae.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history_ae = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb73a6",
   "metadata": {},
   "source": [
    "Please note that only the weights, not the models themselves, are saved and loaded again. The model definition is made in the code and managed as such.\n",
    "\n",
    "Loading the weights only works if the model definition is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae31792-e609-450b-96ab-158431044ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plot model training history.\n",
    "    Args:\n",
    "    - history: tensorflow history object.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.plot(history['loss'], label='Training')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Loss history')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77231aa-da85-4cda-8c9b-67131a41ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_ae.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c0d5d-b502-47a0-9e28-ad1c171f68ec",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "We compile the model and can then get a summary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5278961-9fe3-41ec-ae25-e3b6c971c1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNNmnist_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaae9bc-67ba-4d9c-8cfb-6d9ce3210438",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9f575-74b0-4e96-a54b-800c124dbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c18d17-a3b5-4b9e-85ad-8e69e35ec200",
   "metadata": {},
   "source": [
    "This CNN-based model has less than a tenth of the parameters of the network without the convolution layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e935649",
   "metadata": {},
   "source": [
    "### Looking at the reconstructions\n",
    "Let's look at some of the reconstructed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def show_reconstructions(model, images=test_images, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110bc69-107b-4197-a208-b80e2107e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(CNNmnist_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9486c08-5566-43bc-9787-447e25a5f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_ae.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a49703-8f0c-4123-94f8-98d6359aabf6",
   "metadata": {},
   "source": [
    "## Low-dimensional bottleneck autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16fe7e-ec90-4f5e-af63-cb2ba943796b",
   "metadata": {},
   "source": [
    "### Encoding Dimension 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e51f5a-5ea1-4dc8-bfe2-b8d1bacfca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_encoder_30 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='relu')\n",
    "])\n",
    "\n",
    "CNNmnist_decoder_30 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(7*7*16, activation= 'relu'),\n",
    "    tf.keras.layers.Reshape(target_shape = (7, 7, 16)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n",
    "    tf.keras.layers.UpSampling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n",
    "    tf.keras.layers.UpSampling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n",
    "])\n",
    "\n",
    "CNNmnist_ae_30 = tf.keras.Sequential([CNNmnist_encoder_30, CNNmnist_decoder_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b46df-ed2a-4791-9285-251cb815a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "CNNmnist_ae_30.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d76f18-debc-4c0d-aef1-19e7d8f04053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_30_path = './CNNmnist_encoder_30.weights.h5'\n",
    "mnist_decoder_30_path = './CNNmnist_decoder_30.weights.h5'\n",
    "history_path = './CNNmnist_decoder_30.history.h5'\n",
    "\n",
    "if train_from_scratch:\n",
    "    history30 = CNNmnist_ae_30.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                         callbacks=[EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                  verbose=False, restore_best_weights=True)])\n",
    "\n",
    "    # Save the weights:\n",
    "    CNNmnist_encoder_30.save_weights(mnist_encoder_30_path)\n",
    "    CNNmnist_decoder_30.save_weights(mnist_decoder_30_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history30, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    CNNmnist_encoder_30.build(input_shape=(None, 28, 28, 1))\n",
    "    CNNmnist_decoder_30.build(input_shape=(None, 30))\n",
    "    CNNmnist_encoder_30.load_weights(mnist_encoder_30_path)\n",
    "    CNNmnist_decoder_30.load_weights(mnist_decoder_30_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history30 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a5eba-b8f2-4688-99e2-e89e0384801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history30.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cfca7-bfe8-4b0a-9316-f990af6f04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(CNNmnist_ae_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521207f5-dcd4-49b3-a1a7-694ecadfaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_encoder_30.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517e21e-15ef-44b1-b06f-ec78cd2c5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_decoder_30.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33a199-57ed-4ea7-b4df-3cb7e77ee973",
   "metadata": {},
   "source": [
    "### Encoding Dimension 2\n",
    "\n",
    "**TODO**: Implement an autoencoder with an encoding dimension of 2. Follow the above example with an encoding dimension of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# CNNmnist_encoder_2 = ...\n",
    "# \n",
    "# CNNmnist_decoder_2 = ...\n",
    "# \n",
    "# CNNmnist_ae_2 = ...\n",
    "\n",
    "tf.random.set_seed(42) \n",
    "CNNmnist_ae_2.compile(loss=\"mse\", optimizer=\"nadam\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31eaa8a234fd9e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472acbb7-f6cb-405c-aa91-ff71dc7c0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_2_path = './CNNmnist_encoder_2.weights.h5'\n",
    "mnist_decoder_2_path = './CNNmnist_decoder_2.weights.h5'\n",
    "history_path = './CNNmnist_decoder_2.history.h5'\n",
    "\n",
    "if train_from_scratch:\n",
    "    history2 = CNNmnist_ae_2.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                       callbacks=[EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                verbose=False, restore_best_weights=True)])\n",
    "\n",
    "    # Save the weights:\n",
    "    CNNmnist_encoder_2.save_weights(mnist_encoder_2_path)\n",
    "    CNNmnist_decoder_2.save_weights(mnist_decoder_2_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history2, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    CNNmnist_encoder_2.build(input_shape=(None, 28, 28, 1))\n",
    "    CNNmnist_decoder_2.build(input_shape=(None, 2))\n",
    "    CNNmnist_encoder_2.load_weights(mnist_encoder_2_path)\n",
    "    CNNmnist_decoder_2.load_weights(mnist_decoder_2_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aac49c-379e-4e40-a3ab-546c1697e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77266c3-b2ad-41ee-a655-cbaa20fdc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(CNNmnist_ae_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing Decodings\n",
    "In the following, we will see how a few chosen codes will be decoded into an image:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17a4e61c30ddaa9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197d7bd-1edc-4162-bff2-3b06dba12749",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "for ID1 in range(2):\n",
    "    for ID2 in range(2):\n",
    "        code = np.zeros(shape=[1, 2])\n",
    "        code[0, ID1] = 1\n",
    "        code[0, ID2] = 1\n",
    "        coding = np.squeeze(CNNmnist_decoder_2(code))\n",
    "        im = axs[ID1, ID2].imshow(coding, vmin=0, vmax=1)\n",
    "        axs[ID1, ID2].axis('off')\n",
    "        axs[ID1, ID2].set_title(code)\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29153e42-996a-4531-8dea-44a0cd97bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 6, figsize=(12, 12))\n",
    "\n",
    "for val1 in np.linspace(0, 5, 6):\n",
    "    for val2 in np.linspace(0, 5, 6):\n",
    "        code = np.zeros(shape=[1, 2])\n",
    "        if val1>0:\n",
    "            code[0, 0] = 2**(val1-1)\n",
    "        if val2>0:\n",
    "            code[0, 1] = 2**(val2-1)\n",
    "        coding = np.squeeze(CNNmnist_decoder_2(code))\n",
    "        im = axs[int(val1), int(val2)].imshow(coding, vmin=0, vmax=1)\n",
    "        axs[int(val1), int(val2)].axis('off')\n",
    "        axs[int(val1), int(val2)].set_title(code)\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing Encodings\n",
    "Having 2 dimensions is a very good starting points for visualization: We can look at where in the 2D code space the images representing the individual digits are projected to:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3e746dc03e44e18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec63cb-106f-464e-82ca-1355d4cd3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings2 = CNNmnist_encoder_2.predict(val_images, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baa478-e660-4837-ab3a-fd9111d752e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde38fb-f784-44c3-8578-ac5c20cff926",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings2[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3a938-c2dd-495d-b9d4-0db2d9d0c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseAE_embedding = pd.DataFrame(val_encodings2)\n",
    "sparseAE_embedding.columns = ['Dimension 1', 'Dimension 2']\n",
    "sparseAE_embedding['label'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242de161-4482-49b6-a7c0-ea2e4ad45871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseAE_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867fe6e-42c9-42df-95c5-6d6d282ba08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576e5e1-1795-4e5d-9ceb-09132ae07eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(sparseAE_embedding, x='Dimension 1', y='Dimension 2', hue='label', legend='full', palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2b7cf-54aa-4b28-b4c3-565a6bc2d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(sparseAE_embedding, x='Dimension 1', y='Dimension 2', hue='label', legend='full', palette='deep')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO:** Given the above graph, can you find a code that will be decoded into an image representing the digit \"0\"? Use the above two plots as basis for your guess, and adapt the below code cell to verify your finding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "134638618cac9354"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code = np.zeros(shape=[1, 2])\n",
    "# modify the values of the first and second dimension\n",
    "# code[0, 0] = ...\n",
    "# code[0, 1] = ...\n",
    "coding = np.squeeze(CNNmnist_decoder_2(code))\n",
    "plt.imshow(coding, vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.title(code)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51cd947b6d517f39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b63ca8da689e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
