{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrylZy4t6SHl"
   },
   "source": [
    "# Extractive Summarization\n",
    "\n",
    "This notebook demonstrates the process of extractive summarization using AI techniques. Extractive summarization involves selecting important sentences from a text to create a summary. This can be particularly useful in managing large volumes of information efficiently. We will use natural language processing (NLP) libraries and machine learning models to identify and extract key sentences from a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAu_IPBHQor_"
   },
   "source": [
    "We begin by importing the libraries we will be using. `Spacy` is used for sentence tokenization, `SentenceTransformer` for generating sentence embeddings, and `numpy` for mathematical operations.\n",
    "\n",
    "**Run this notebook using the \"Torch\" Kernel.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nopoAbIQlw7"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c74tQ2r8Q0D1"
   },
   "source": [
    "We now initialize Spacy with the English language model and add a sentencizer component. This prepares us to split our text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLT4y8ikQxfA"
   },
   "outputs": [],
   "source": [
    "spacy = English()\n",
    "spacy.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8q3xilMQ8md"
   },
   "source": [
    "### Sentence Splitting\n",
    "We now define a function to split texts into sentences. It is crucial for preparing our text for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_v5d-zQQ5PM"
   },
   "outputs": [],
   "source": [
    "def split(text):\n",
    "  \"\"\"\n",
    "  Splits a text into sentences using the Spacy library.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The text to be split into sentences.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      The list of sentences in the text.\n",
    "  \"\"\"\n",
    "  processed_text = spacy(text)\n",
    "  processed_sentences = processed_text.sents\n",
    "\n",
    "  # TODO: Return a list with all sentences. For a processed_sentence s,\n",
    "  # you get its text with the attribute s.text\n",
    "  return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44tZmV8dRJgC"
   },
   "source": [
    "We now define a sample text to be summarized. This text will be processed through our summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(\"cinderella.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lrtSZI5RQ0E"
   },
   "source": [
    "The next cell applies the previously defined `split` function to our text and prints the individual sentences, allowing us to see how the text has been divided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SdbIPYCeE4b"
   },
   "outputs": [],
   "source": [
    "# TODO: Use the split function you implemented to print all sentences in the text above.\n",
    "sentences = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snubbTP-RX2k"
   },
   "source": [
    "We now load a pre-trained model to generate embeddings for each sentence. These embeddings capture the semantic meaning of sentences in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGixZkLGeOVx"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "print(f'This {sentence_embeddings[0].shape[0]}-dimensional vector is an abstract representation of the sentence:')\n",
    "print(sentences[0])\n",
    "print(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "The sentence embedding model processes each sentence independently - the model has no means of finding out where the sentence is in the overall text, and hence this information is lost in the embeddings. With positional encoding, we have the possibility to add this information. We start without positional encoding (i.e., \"Option 0\" below); you can later experiment wtih two types of positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_length, embedding_dim):\n",
    "    positions = np.arange(seq_length)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "    \n",
    "    pe = np.zeros((seq_length, embedding_dim))\n",
    "    pe[:, 0::2] = np.sin(positions * div_term)  # Apply sin to even indices\n",
    "    pe[:, 1::2] = np.cos(positions * div_term)  # Apply cos to odd indices\n",
    "    \n",
    "    return pe\n",
    "\n",
    "pos_enc = positional_encoding(seq_length=len(sentences), embedding_dim=sentence_embeddings.shape[1])\n",
    "\n",
    "# Option 0: no positional encoding\n",
    "# enhanced_embeddings = sentence_embeddings\n",
    "\n",
    "# Option 1: Concatenation (Doubles dimensionality)\n",
    "# enhanced_embeddings = np.concatenate([sentence_embeddings, pos_enc], axis=1) # Uncomment this if you prefer concatenation\n",
    "\n",
    "# Option 2: Addition (Keeps same dimensionality)\n",
    "enhanced_embeddings = sentence_embeddings + pos_enc  # Uncomment this if you prefer addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Next, we will use PCA (a linear dimensionality reduction technique) to reduce the number of dimensions - this has computational benefits, and also might get us rid of some irrelevant aspects (\"noise\") and thus yield a better clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(enhanced_embeddings)\n",
    "\n",
    "# Apply full PCA (without specifying n_components first - hence a complete decomposition will be computed)\n",
    "pca = PCA()\n",
    "pca.fit(embeddings_scaled)\n",
    "\n",
    "# Cumulative Explained Variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "variance_threshold = 0.95\n",
    "\n",
    "# Find the number of components that retain at least `variance_threshold` variance\n",
    "optimal_components = np.sum(cumulative_explained_variance < variance_threshold) + 1\n",
    "\n",
    "print(f\"{optimal_components} components are needed to capture {variance_threshold*100}% of variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will actually reduce the number of dimensions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with optimal components\n",
    "pca = PCA(n_components=optimal_components)\n",
    "embeddings_pca = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f\"Reduced Embeddings Shape: {embeddings_pca.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egWC317ARuNX"
   },
   "source": [
    "## K-Means Clustering\n",
    "This section clusters the sentence embeddings into groups using KMeans clustering. This helps in identifying sentences with similar meanings, which can be useful for summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7j5WBMzgH1D"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T04:25:40.272415Z",
     "start_time": "2025-09-06T04:25:40.171595200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Apply K-Means to the sentence embeddings.\n",
    "kmenas = None\n",
    "cluster_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-06T04:25:40.171595200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Construct a dictionary of k lists, where each list contains the sentences and the sentence embeddings of one cluster.\n",
    "clustered_sentences = {i: [] for i in range(k)}\n",
    "clustered_embeddings = {i: [] for i in range(k)}\n",
    "\n",
    "for sentence, embedding, cluster_id in zip(sentences, embeddings_pca, cluster_ids):\n",
    "    clustered_sentences[cluster_id].append(sentence)\n",
    "    clustered_embeddings[cluster_id].append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-06T04:25:40.171595200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the sentences in each cluster.\n",
    "for cluster_id, sentence_list in clustered_sentences.items():\n",
    "    print(f\"Cluster {cluster_id} consists of the following sentences:\")\n",
    "    for sentence in sentence_list:\n",
    "        print(\"   \" + sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trZ-GonySEBO"
   },
   "source": [
    "We now compute for each cluster, its *prototypical sentence*. This is the sentence whose embedding is the closest to the cluster's center of mass. In other words, this is the word that is the closest to all other embeddings in the cluster. We begin this by defining two auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEpT-5xfhVBU"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(u, v):\n",
    "    return np.linalg.norm(u - v)\n",
    "\n",
    "def find_prototypical(centroid, vectors):\n",
    "    # TODO: Complete this function so that it returns the **index** of the vector that is closest\n",
    "    # to the centroid.\n",
    "    distances = []\n",
    "    for vector in vectors:\n",
    "        d = euclidean_distance(centroid, vector)\n",
    "        distances.append(d)\n",
    "    \n",
    "    index_closest_vector = 0\n",
    "    closest_dist = distances[0]\n",
    "    for i, di in enumerate(distances):\n",
    "        if di < closest_dist:\n",
    "            index_closest_vector = i\n",
    "            closest_dist = di\n",
    "    \n",
    "    return index_closest_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rUXcpYXjWmJ"
   },
   "outputs": [],
   "source": [
    "# Obtain cluster centroids from k-Means\n",
    "cluster_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Create a dictionary that maps each cluster index to the cluster's prototypical sentence.\n",
    "prototypical_sentence_indices = {}\n",
    "\n",
    "# Loop through clusters\n",
    "for cluster_id in range(k):\n",
    "\n",
    "    # Get cluster centroid\n",
    "    cluster_centroid = cluster_centroids[cluster_id]\n",
    "    \n",
    "    # Get cluster embeddings\n",
    "    cluster_embeddings = clustered_embeddings[cluster_id]\n",
    "    \n",
    "    # Find the prototypical embedding in this cluster\n",
    "    prototypical_index = find_prototypical(cluster_centroid, cluster_embeddings)\n",
    "    \n",
    "    # And store its index\n",
    "    prototypical_sentence_indices[cluster_id] = prototypical_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dJNLrsrphog"
   },
   "outputs": [],
   "source": [
    "# Now we arrange the sentences accordingly\n",
    "prototypical_sentences = []\n",
    "for cluster_id in range(k):\n",
    "\n",
    "  # Get the sentences in this cluster\n",
    "  cluster_sentences = clustered_sentences[cluster_id]\n",
    "\n",
    "  # Get the index of the prototypical sentence\n",
    "  prototypical_sentence_index = prototypical_sentence_indices[cluster_id]\n",
    "\n",
    "  # Store the prototypical sentence\n",
    "  prototypical_sentences.append(\n",
    "    cluster_sentences[prototypical_sentence_index]\n",
    "  )\n",
    "\n",
    "# We now have the k summary sentences that we were looking for...\n",
    "# but they are shuffled as a result of the clustering\n",
    "original_indices = []\n",
    "for sent in prototypical_sentences:\n",
    "  original_indices.append(sentences.index(sent))\n",
    "\n",
    "# We re-organize them based on their original order in the document\n",
    "new_indices = np.argsort(original_indices)\n",
    "prototypical_sentences = [prototypical_sentences[i] for i in new_indices]\n",
    "\n",
    "extractive_summary_sentences = prototypical_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gK9F6o7g_bP"
   },
   "source": [
    "## Visualisation of the Story\n",
    "Finally, we project the sentence embeddings into a 2-dimensional space and color the embeddings according to the clusters they belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cr5247L-gIYJ"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for PCA and plotting\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the sentence embeddings\n",
    "sentence_embeddings_2d = pca.fit_transform(embeddings_pca)\n",
    "\n",
    "# Prepare colors; if you have more than 8 clusters, extend this list of colors\n",
    "colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "# Plot each sentence embedding in the 2D space, colored by its cluster\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "for i, embedding in enumerate(sentence_embeddings_2d):\n",
    "    plt.scatter(embedding[0], embedding[1], color=colors[cluster_ids[i]], label=f'Cluster {cluster_ids[i]}')\n",
    "\n",
    "# Optional: add a legend. This might make the plot crowded if there are many points.\n",
    "# To improve the legend, we're creating custom legend entries to avoid duplicates\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=10, label=f'Cluster {i}') for i, col in enumerate(colors[:k])]\n",
    "plt.legend(handles=legend_elements, loc='best', title=\"Clusters\")\n",
    "\n",
    "plt.title('Sentence Embeddings Reduced to 2D by PCA, Colored by Clustering')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR9C510gjbBy"
   },
   "outputs": [],
   "source": [
    "print('Summary sentences:\\n')\n",
    "for i, sent in enumerate(extractive_summary_sentences, start=1):\n",
    "    print(i, sent, '(Original sentence index: ' + str(original_indices[new_indices[i-1]]) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmbdLCohg1Hd"
   },
   "source": [
    "**TODO:**\n",
    "- Try different numbers of clusters and see the resulting summary. In your opinion, which number of clusters is ideal for your text of choice?\n",
    "- Besides the \"result-oriented\" way of choosing the right number of clusters, what other techniques do you know? Implement them, run the clustering with the obtained number of clusters, and comment on the findings.\n",
    "- Experiment with different options for positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yS7Qa_cjirrk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
