{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZE6RDfi6eR6"
   },
   "source": [
    "# Attribution Maps for FashionMNIST\n",
    "In this notebook, you will investigate why the neural network attributes a given label to a particular piece of fashion.\n",
    "\n",
    "The start is the same as in the `CNN_Fashion` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkjWYuH7KSUF"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vyEJmVc5Fln",
    "outputId": "24836078-1c03-4526-cab5-e49f4ac92a54"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Add a channel dimension\n",
    "train_images = train_images[..., tf.newaxis]\n",
    "test_images = test_images[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k01xJVny6HT7"
   },
   "source": [
    "The code below displays some images in the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "w7NyPqOT6LXg",
    "outputId": "34cf602a-038c-4602-e0b5-99488db06bd6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion_mnist_labels = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, labels, num_images=5):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "        plt.xlabel(fashion_mnist_labels[labels[i]])\n",
    "    plt.show()\n",
    "\n",
    "# Display images from the training set\n",
    "display_images(train_images, train_labels)\n",
    "\n",
    "# Display images from the testing set\n",
    "display_images(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dt-gCJK5LYv"
   },
   "source": [
    "## The network\n",
    "We will work with the network below to predict the label for the fashion items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "PJz4wIAj5QtX",
    "outputId": "4987d325-87ec-4797-f79d-77628189bda7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Assuming you've already filtered train_images, train_labels, test_images, and test_labels\n",
    "# And added a channel dimension to your images\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    # Convolutional base\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "\n",
    "    # Dense layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC1Sy0GUKSUG"
   },
   "source": [
    "## Visualising Internal Parameters\n",
    "Looping through the layers of the notebook, we can visualise the convolution filters the network has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EYX8hK4O-q87",
    "outputId": "32aa4e55-7507-4469-a891-fb9093ba76c5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Iterate thru all the layers of the model\n",
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        weights, bias= layer.get_weights()\n",
    "        print(layer.name, weights.shape)\n",
    "\n",
    "        filters = weights\n",
    "        # optional: normalize filter values between  0 and 1 for visualization\n",
    "        # f_min = weights.min()\n",
    "        # f_max = weights.max()\n",
    "        # filters = (weights - f_min) / (f_max - f_min)\n",
    "        nFiltersInLayer = filters.shape[3]\n",
    "        print(nFiltersInLayer)\n",
    "        filter_cnt=1\n",
    "\n",
    "        figCols = int(np.ceil(np.sqrt(nFiltersInLayer)))\n",
    "        figRows = int(np.ceil(nFiltersInLayer/figCols))\n",
    "\n",
    "        # plotting all the filters\n",
    "        for i in range(nFiltersInLayer):\n",
    "            # get the filters\n",
    "            filt = filters[:, :, :, i]\n",
    "            ax = plt.subplot(figRows, figCols, filter_cnt)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(filt[:, :, 0])\n",
    "            filter_cnt+=1\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n",
    "        cax = plt.axes([0.85, 0.1, 0.025, 0.8])\n",
    "        plt.colorbar(cax=cax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLgoSXaDKSUI"
   },
   "source": [
    "## Feature Maps\n",
    "Also, we can loop through the network layers and look at how an input image looks like after the given layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nh28s7Yu8PaZ",
    "outputId": "7a230a6d-a7ef-4a89-e5f6-3eb86e54c9c3"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)\n",
    "    if layer.name == 'flatten':\n",
    "        # skip the flattening layer\n",
    "        continue\n",
    "\n",
    "    # Build a new model 'model_internal' that consists of all layers of the original model until the current layer\n",
    "    model_internal = models.Model(inputs=model.inputs , outputs=layer.output)\n",
    "    # The predictions of the 'model_internal' model are how a given input 'looks' for the model:\n",
    "    feature_map_layer = model_internal.predict(train_images[1:2])\n",
    "\n",
    "    # visualize the internal representation, depending on its shape:\n",
    "    if len(feature_map_layer.shape)==1:\n",
    "        continue\n",
    "\n",
    "    nFiltersInLayer = feature_map_layer.shape[-1]\n",
    "    print(nFiltersInLayer)\n",
    "    filter_cnt=1\n",
    "\n",
    "    figCols = int(np.ceil(np.sqrt(nFiltersInLayer)))\n",
    "    figRows = int(np.ceil(nFiltersInLayer/figCols))\n",
    "\n",
    "    # plotting all the filters\n",
    "    if len(feature_map_layer.shape)==4:\n",
    "        for i in range(nFiltersInLayer):\n",
    "            # get the filters\n",
    "            filt = feature_map_layer[:, :, :, i]\n",
    "            ax = plt.subplot(figRows, figCols, filter_cnt)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(filt.squeeze())\n",
    "            filter_cnt+=1\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n",
    "        cax = plt.axes([0.85, 0.1, 0.025, 0.8])\n",
    "        plt.colorbar(cax=cax)\n",
    "    elif len(feature_map_layer.shape)==2:\n",
    "        plt.bar(range(feature_map_layer.size), np.squeeze(feature_map_layer))\n",
    "    if filt.size==1:\n",
    "        continue\n",
    "\n",
    "    plt.title(layer.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAOCSFQ0C3TC"
   },
   "source": [
    "## Examples with highest likelihood\n",
    "For better interpretability, it might also be helpful to find out which examples of a given class the model most confidently classifies correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj2BzfP9DpcA",
    "outputId": "c8456795-9a16-46fc-b555-c40954ef059d"
   },
   "outputs": [],
   "source": [
    "model_preds = model.predict(test_images)\n",
    "test_labels_est = np.argmax(model_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIQMvuztECq_"
   },
   "outputs": [],
   "source": [
    "target_class = 1\n",
    "target_idx = np.where(test_labels==target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9y7O4JcQKSUJ",
    "outputId": "cb747cb1-b732-4b38-8e70-a4ccb5e4573f"
   },
   "outputs": [],
   "source": [
    "model_preds_4target = model_preds[ target_idx, target_class ].squeeze()\n",
    "model_preds_4target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adJNP56LEJzh",
    "outputId": "d4cf13db-031d-4758-b139-8cc9b7c392d9"
   },
   "outputs": [],
   "source": [
    "extreme_vals = np.argsort(model_preds_4target)\n",
    "model_preds_4target[extreme_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKmV-rT6MLF3"
   },
   "outputs": [],
   "source": [
    "num_images = 6\n",
    "model_preds_4target[extreme_vals]\n",
    "\n",
    "least_likely_idx = target_idx[0][extreme_vals[:num_images]]\n",
    "most_likely_idx = target_idx[0][extreme_vals[-num_images:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "X8-9jNN6E3VF",
    "outputId": "9cd7bad8-4f17-47cd-e0fe-11fafd06de9a"
   },
   "outputs": [],
   "source": [
    "display_images(test_images[least_likely_idx], test_labels[least_likely_idx],\n",
    "               num_images=num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "Mmkdd1N1FDNw",
    "outputId": "2e2a2793-d1c7-435a-9266-fda9e1deda5c"
   },
   "outputs": [],
   "source": [
    "display_images(test_images[most_likely_idx], test_labels[most_likely_idx],\n",
    "               num_images=num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbXysZOLQS9s"
   },
   "source": [
    "## Saliency Map\n",
    "The calculation of the saliency map is rather technical. We provide the code below with a few explanations. Some of the material here is from https://usmanr149.github.io/urmlblog/cnn/2020/05/01/Salincy-Maps.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE8oPfCkUX0b"
   },
   "source": [
    "We will be calculating the saliency map for a few misclassified images. To do so, we first determine the indices of all images that have been misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlvYRv0WZr4E",
    "outputId": "d645d68a-1ec7-47cf-f3b2-6e0a54a438f4"
   },
   "outputs": [],
   "source": [
    "misclassifications = np.where(test_labels_est != test_labels.squeeze())[0]\n",
    "len(misclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVdyaTbnUkzT"
   },
   "source": [
    "Now we choose the indices of the images we want to get the saliency map for. Here we take the first 12 misclassified images - feel free to change this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzPrbRfPRom6"
   },
   "outputs": [],
   "source": [
    "misclass_idx = misclassifications[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR0eGqcoUuaW"
   },
   "source": [
    "The code below calculates and visualizes the saliency map besides the original image, for the images with index given in the list `misclass_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "ENVkvMWkbrKi",
    "outputId": "c975bf79-73d6-4792-f6b4-09076fade988"
   },
   "outputs": [],
   "source": [
    "nMisClass = len(misclass_idx)\n",
    "figCols = int(2*np.ceil(np.sqrt(2*nMisClass)/2))\n",
    "figRows = int(np.ceil(2*nMisClass/figCols))\n",
    "mc_count = 0\n",
    "\n",
    "fig, axs = plt.subplots(figRows, figCols, figsize=(10, 10))\n",
    "fig.suptitle('Misclassifications')\n",
    "\n",
    "for mc in misclass_idx:\n",
    "    # define image as variable\n",
    "    my_img = tf.Variable(test_images[mc:(mc+1)], dtype=float)\n",
    "\n",
    "    # calculate gradient of the output predictions with respect to the image\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(my_img, training=False)\n",
    "        class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]\n",
    "        loss = pred[0][class_idxs_sorted[0]]\n",
    "\n",
    "    grads = tape.gradient(loss, my_img)\n",
    "    dgrad_abs = tf.math.abs(grads)\n",
    "    dgrad_max_ = np.max(dgrad_abs, axis=3)[0]\n",
    "\n",
    "    # normalize to range between 0 and 1\n",
    "    arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "    grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
    "\n",
    "    # display the input image\n",
    "    axs[mc_count // figCols, mc_count % figCols].set_xticks([])\n",
    "    axs[mc_count // figCols, mc_count % figCols].set_yticks([])\n",
    "    title_str = str(mc) + ': est: ' + fashion_mnist_labels[test_labels_est[mc]] \\\n",
    "                + '\\ntrue:  ' + fashion_mnist_labels[test_labels[mc]]\n",
    "    axs[mc_count // figCols, mc_count % figCols].set_title(title_str)\n",
    "    axs[mc_count // figCols, mc_count % figCols].imshow(np.squeeze(my_img), cmap=plt.cm.binary)\n",
    "\n",
    "    # display the gradient\n",
    "    axs[(mc_count+1) // figCols, (mc_count+1) % figCols].set_xticks([])\n",
    "    axs[(mc_count+1) // figCols, (mc_count+1) % figCols].set_yticks([])\n",
    "    axs[(mc_count+1) // figCols, (mc_count+1) % figCols].imshow(grad_eval, cmap='jet')\n",
    "\n",
    "    mc_count += 2\n",
    "\n",
    "for mc_c in range(mc_count, figRows*figCols):\n",
    "    axs[(mc_c) // figCols, (mc_c) % figCols].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juwnOH8cgLzB"
   },
   "source": [
    "**Exercises:**\n",
    "- Looking at the original images and the saliency maps above, can you understand why the model misclassified these images?\n",
    "- You might also look at the saliency map of some of the *correctly* classified images. Do these classification results look reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_6wOWn4gMzG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
