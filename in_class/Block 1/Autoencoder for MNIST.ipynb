{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecf719b",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a3c2e",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf376ae-7252-4d96-822c-955232ca1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32db67",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_val_images, train_val_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Scale image data:\n",
    "train_val_images = train_val_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Split into training / validation\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_val_images, train_val_labels,\n",
    "                                                                      test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddaf33-3cc3-4ca0-8fc1-74d1b35f15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_label_df = pd.DataFrame(train_val_labels)\n",
    "train_val_label_df.columns = ['label']\n",
    "train_val_label_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11e168-87f4-458b-ac21-81fb7648510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_df = pd.DataFrame(val_labels)\n",
    "val_label_df.columns = ['label']\n",
    "val_label_df['label'].value_counts(sort=False, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fd79a-867c-4f3d-b31d-0af1e708a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c3db4",
   "metadata": {},
   "source": [
    "## The Autoencoder\n",
    "Every autoencoder consists of two parts: an encoder and a decoder.\n",
    "\n",
    "* The **encoder** receives the original data (in our case, the black and white images) as input and generates a lower-dimensional code from it.\n",
    "* The **decoder** receives the code and decodes it into original data (e.g. the images) in the same format as the encoder's inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e6f7b",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd56fb0",
   "metadata": {},
   "source": [
    "The two parts put together form the autoencoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1a773-7283-4330-838a-c8ece79d3de1",
   "metadata": {},
   "source": [
    "**Comments on activation functions:**\n",
    "\n",
    "***Why ReLU?***\n",
    "\n",
    "* Simplicity and Efficiency: ReLU (Rectified Linear Unit) is computationally efficient because it involves simple thresholding at zero. This makes it faster to compute compared to other activation functions.\n",
    "* Sparse Activation: ReLU promotes sparsity in the network by setting negative values to zero, which can help in learning more robust features.\n",
    "* Gradient Propagation: ReLU helps mitigate the vanishing gradient problem, allowing gradients to propagate more effectively during backpropagation.\n",
    "\n",
    "***Why Not SELU?***\n",
    "\n",
    "SELU (Scaled Exponential Linear Unit) is another excellent activation function, especially for self-normalizing neural networks. However, SELU requires careful initialization and specific network architecture (e.g., no Batch Normalization) to maintain its self-normalizing properties. For a straightforward convolutional autoencoder, ReLU is often preferred due to its simplicity and effectiveness.\n",
    "\n",
    "***Sigmoid*** at the end to get an output between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628fd6f-3c94-4d49-8f00-3f9e946db46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu')\n",
    "])\n",
    "\n",
    "mnist_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (100,)),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(28*28, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "mnist_ae = tf.keras.Sequential([mnist_encoder, mnist_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1842b1",
   "metadata": {},
   "source": [
    "### Training\n",
    "The autoencoder uses unsupervised learning, i.e. we do not pass any predefined labels or similar. Instead, the output should be as similar as possible to the input. We use the squared error as a measure of the quality of the reconstruction.\n",
    "\n",
    "For a start, we allow a maximum of 10 epochs to train - as with the other convolutional networks (and with deep neural networks in general), you should allow significantly more epochs for real applications.\n",
    "\n",
    "The training takes a little longer here... With the following code you can save and reload the learned weights - just set `train_from_scratch` as needed.\n",
    "Please note that only the weights, not the models themselves, are saved and loaded again. The model definition is made in the code and managed as such.\n",
    "\n",
    "Loading the weights only works if the model definition is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit the model\n",
    "tf.random.set_seed(42) \n",
    "mnist_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97130f-3295-4041-9368-bb66de745996",
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 50\n",
    "nPatience = 5\n",
    "\n",
    "# define paths:\n",
    "encoder_weights_path = './mnist_encoder.weights.h5'\n",
    "decoder_weights_path = './mnist_decoder.weights.h5'\n",
    "history_path = './mnist_ae.history.h5'\n",
    "\n",
    "\n",
    "train_from_scratch = True\n",
    "if train_from_scratch:\n",
    "    history_ae = mnist_ae.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                       callbacks = [ EarlyStopping(monitor='val_loss', patience=nPatience, \n",
    "                                                   verbose=False, restore_best_weights=True)])\n",
    "    \n",
    "    # Save the weights:\n",
    "    mnist_encoder.save_weights(encoder_weights_path)\n",
    "    mnist_decoder.save_weights(decoder_weights_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history_ae, f)\n",
    "\n",
    "else:\n",
    "    # load previously computed weights\n",
    "    fashion_encoder.load_weights(encoder_weights_path)\n",
    "    fashion_decoder.load_weights(decoder_weights_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history_ae = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715960f-be70-4e41-a39b-eed7f7a159a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plot model training history.\n",
    "    Args:\n",
    "    - history: tensorflow history object.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.plot(history['loss'], label='Training')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Loss history')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce005f6-82c1-4b05-b262-7dc07509daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_ae.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a571f26-baa2-413d-9242-a8431c9e2b33",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "We can now get a summary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1a83f-cb41-4fac-83de-e6b7d0ce9ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70beecc7-d6a3-4b01-ba18-49c153bccc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5a24-a82a-4c94-a3b9-90ede0a66094",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e935649",
   "metadata": {},
   "source": [
    "### Looking at the reconstructions\n",
    "Let's look at some of the reconstructed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def show_reconstructions(model, images=test_images, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110bc69-107b-4197-a208-b80e2107e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(mnist_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a49703-8f0c-4123-94f8-98d6359aabf6",
   "metadata": {},
   "source": [
    "## Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750f07c-4cd4-49f3-bc2d-e0f2680a2327",
   "metadata": {},
   "source": [
    "### Encoding Dimension 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e51f5a-5ea1-4dc8-bfe2-b8d1bacfca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder_sparse30 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='relu')\n",
    "])\n",
    "\n",
    "mnist_decoder_sparse30 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (30,)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(28*28, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "mnist_ae_sparse30 = tf.keras.Sequential([mnist_encoder_sparse30, mnist_decoder_sparse30])\n",
    "mnist_ae_sparse30.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcb776-9857-49df-a319-ee9be153fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_sparse30_path = './mnist_encoder_sparse30.weights.h5'\n",
    "mnist_decoder_sparse30_path = './mnist_decoder_sparse30.weights.h5'\n",
    "history_path = './mnist_decoder_sparse30.history.h5'\n",
    "\n",
    "if train_from_scratch:\n",
    "    history30 = mnist_ae_sparse30.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                    callbacks = [ EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                verbose=False, restore_best_weights=True)])\n",
    "    \n",
    "    # Save the weights:\n",
    "    mnist_encoder_sparse30.save_weights(mnist_encoder_sparse30_path)\n",
    "    mnist_decoder_sparse30.save_weights(mnist_decoder_sparse30_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history30, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    mnist_encoder_sparse30.load_weights(mnist_encoder_sparse30_path)\n",
    "    mnist_decoder_sparse30.load_weights(mnist_decoder_sparse30_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history30 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a5eba-b8f2-4688-99e2-e89e0384801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history30.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cfca7-bfe8-4b0a-9316-f990af6f04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(mnist_ae_sparse30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae34c74-3e60-473e-8157-a31837f34044",
   "metadata": {},
   "source": [
    "### Encoding Dimension 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1eed44-f5bf-4a1b-8b27-9c0629721330",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder_sparse5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='relu')\n",
    "])\n",
    "\n",
    "mnist_decoder_sparse5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (5,)),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(28*28, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "mnist_ae_sparse5 = tf.keras.Sequential([mnist_encoder_sparse5, mnist_decoder_sparse5])\n",
    "mnist_ae_sparse5.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f47ddb-6251-4ce9-b6ab-6a41ff54b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_sparse5_path = './mnist_encoder_sparse5.weights.h5'\n",
    "mnist_decoder_sparse5_path = './mnist_decoder_sparse5.weights.h5'\n",
    "history_path = './mnist_decoder_sparse5.history.h5'\n",
    "\n",
    "if train_from_scratch:\n",
    "    history5 = mnist_ae_sparse5.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                    callbacks = [ EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                verbose=False, restore_best_weights=True)])\n",
    "    \n",
    "    # Save the weights:\n",
    "    mnist_encoder_sparse5.save_weights(mnist_encoder_sparse5_path)\n",
    "    mnist_decoder_sparse5.save_weights(mnist_decoder_sparse5_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history5, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    mnist_encoder_sparse5.load_weights(mnist_encoder_sparse5_path)\n",
    "    mnist_decoder_sparse5.load_weights(mnist_decoder_sparse5_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b079c7-8a19-4c09-aae2-63a76bae61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history5.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa156a6b-f388-4cb8-9521-a777fa85a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(mnist_ae_sparse5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9883ab-f910-4055-af4f-06ece7257c80",
   "metadata": {},
   "source": [
    "### Encoding Dimension 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658973f-618b-4ac4-8f30-ce30dc96fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "mnist_encoder_sparse2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='relu')\n",
    "])\n",
    "\n",
    "mnist_decoder_sparse2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (2,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(28*28, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "mnist_ae_sparse2 = tf.keras.Sequential([mnist_encoder_sparse2, mnist_decoder_sparse2])\n",
    "\n",
    "tf.random.set_seed(42) \n",
    "mnist_ae_sparse2.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f470e-f09f-4b2d-ac14-8238bf146300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_sparse2_path = './mnist_encoder_sparse2.weights.h5'\n",
    "mnist_decoder_sparse2_path = './mnist_decoder_sparse2.weights.h5'\n",
    "history_path = './mnist_decoder_sparse2.history.h5'\n",
    "\n",
    "# Train if needed:\n",
    "if train_from_scratch:\n",
    "    history2 = mnist_ae_sparse2.fit(train_images, train_images, epochs=nEpochs, validation_data=(val_images, val_images),\n",
    "                                    callbacks = [ EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                verbose=False, restore_best_weights=True)])\n",
    "    \n",
    "    # Save the weights:\n",
    "    mnist_encoder_sparse2.save_weights(mnist_encoder_sparse2_path)\n",
    "    mnist_decoder_sparse2.save_weights(mnist_decoder_sparse2_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history2, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    mnist_encoder_sparse2.load_weights(mnist_encoder_sparse2_path)\n",
    "    mnist_decoder_sparse2.load_weights(mnist_decoder_sparse2_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aac49c-379e-4e40-a3ab-546c1697e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77266c3-b2ad-41ee-a655-cbaa20fdc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(mnist_ae_sparse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a201b7-a8d0-452b-990b-f7fb745476b4",
   "metadata": {},
   "source": [
    "### Visualizing the Digits in 2D\n",
    "Since we only have 2 dimensions as \"code\", we can nicely visualize the representation of the individual digits. To do so, we just pass them through the encoder to get the encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec63cb-106f-464e-82ca-1355d4cd3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings_2D = mnist_encoder_sparse2(val_images)\n",
    "val_encodings_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde38fb-f784-44c3-8578-ac5c20cff926",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings_2D[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e3246-6fb1-406b-9335-4ba052d94eac",
   "metadata": {},
   "source": [
    "For nicer visualisation, we create a data frame, consisting of both dimensions and the label. We then use the label (i.e., the digit represented by the image) to color the individual representations in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3a938-c2dd-495d-b9d4-0db2d9d0c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseAE_embedding = pd.DataFrame(val_encodings_2D)\n",
    "sparseAE_embedding.columns = ['Dimension 1', 'Dimension 2']\n",
    "sparseAE_embedding['label'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242de161-4482-49b6-a7c0-ea2e4ad45871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseAE_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576e5e1-1795-4e5d-9ceb-09132ae07eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(sparseAE_embedding, x='Dimension 1', y='Dimension 2', hue='label', legend='full', palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b5bf4-76c0-419b-af55-d5282f216650",
   "metadata": {},
   "source": [
    "We also do a plot on logarithmic scale so we can better see the digits that are more concentraged in the encoding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2b7cf-54aa-4b28-b4c3-565a6bc2d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(sparseAE_embedding, x='Dimension 1', y='Dimension 2', hue='label', legend='full', palette='deep')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da092809-689f-4b06-af2e-2d6ae2324cf1",
   "metadata": {},
   "source": [
    "### Generating outputs\n",
    "So far we have used the encoder to encode a digit image an 2D. Now we will use the decoder to decode a code (i.e., a list of 2 elements) to a full image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cb1d0-c3b3-4c91-84d9-274fc21b3fb7",
   "metadata": {},
   "source": [
    "**Exercise:** Using the visualisations above, can you predict which inputs will be decoded to images representing which digits? Read off the x- and y-coordinates of a point, and use the color code to determine the expected digit it will represent.\n",
    "Use the code below to check your finding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b766539-5183-4960-b713-45ba00a07727",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = np.zeros(shape=[1, 2])\n",
    "# modify the values of the first and second dimension\n",
    "code[0, 0] = 10\n",
    "code[0, 1] = 2\n",
    "coding = np.squeeze(mnist_decoder_sparse2(code))\n",
    "plt.imshow(coding, vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.title(code)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd66582-f5de-47e6-ae1f-1733c1c8823b",
   "metadata": {},
   "source": [
    "## Visualizing Outputs for Several Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197d7bd-1edc-4162-bff2-3b06dba12749",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n",
    "\n",
    "for ID1 in range(2):\n",
    "    for ID2 in range(2):\n",
    "        code = np.zeros(shape=[1, 2])\n",
    "        code[0, ID1] = 1\n",
    "        code[0, ID2] = 1\n",
    "        coding = np.squeeze(mnist_decoder_sparse2(code))\n",
    "        im = axs[ID1, ID2].imshow(coding, vmin=0, vmax=1)\n",
    "        axs[ID1, ID2].axis('off')\n",
    "        axs[ID1, ID2].set_title(code)\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29153e42-996a-4531-8dea-44a0cd97bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 8, figsize=(12, 12))\n",
    "\n",
    "for val1 in np.linspace(0, 7, 8):\n",
    "    for val2 in np.linspace(0, 7, 8):\n",
    "        code = np.zeros(shape=[1, 2])\n",
    "        if val1>0:\n",
    "            code[0, 0] = 2**(val1-1)\n",
    "        if val2>0:\n",
    "            code[0, 1] = 2**(val2-1)\n",
    "        coding = np.squeeze(mnist_decoder_sparse2(code))\n",
    "        im = axs[int(val1), int(val2)].imshow(coding, vmin=0, vmax=1)\n",
    "        axs[int(val1), int(val2)].axis('off')\n",
    "        axs[int(val1), int(val2)].set_title(code)\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27b16f-54e2-4f48-98ee-8e5b8704e71a",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f9d8c-19f7-46e1-9050-2e5e855461fa",
   "metadata": {},
   "source": [
    "### Generate Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39113f80-37a6-42d4-993a-a0653c187e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image_array, noise_factor=0.4):\n",
    "    \"\"\"Adds random noise to each image in the supplied array.\"\"\"\n",
    "    noisy_array = (1-noise_factor) * image_array + noise_factor * np.random.random(size=image_array.shape)\n",
    "    return noisy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1f447-628e-490a-bce7-9f686e9f9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Add noise:\n",
    "# - to training images\n",
    "train_images_noisy = add_noise(train_images)\n",
    "\n",
    "# - to validation images\n",
    "val_images_noisy = add_noise(val_images)\n",
    "\n",
    "# - to test images\n",
    "test_images_noisy = add_noise(test_images)\n",
    "\n",
    "# flatten images:\n",
    "train_images_noisy_flat = train_images_noisy.reshape(train_images_noisy.shape[0], -1)\n",
    "val_images_noisy_flat = val_images_noisy.reshape(val_images_noisy.shape[0], -1)\n",
    "test_images_noisy_flat = test_images_noisy.reshape(test_images_noisy.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bddfa-1bb1-45d1-b829-afdea86fbcca",
   "metadata": {},
   "source": [
    "### Denoising with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f47811-7acb-45ea-aa60-24d82aa8a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 32 components\n",
    "pca = PCA(n_components=32, random_state=42)\n",
    "pca.fit(test_images_noisy_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256e08e-c0d1-43bf-872d-b57e2aac2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_pca = pca.inverse_transform(pca.transform(test_images_noisy_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ea732-c4b2-4b51-932a-a41a65b72354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(X, title):\n",
    "    \"\"\"Small helper function to plot 100 digits.\"\"\"\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=10, figsize=(8, 4))\n",
    "    for img, ax in zip(X, axs.ravel()):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap=\"binary\")\n",
    "        ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04333a8d-8654-4802-be11-afdcff2d10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PCA results\n",
    "plot_digits(test_images, \"Test images\")\n",
    "plot_digits(test_images_noisy, \"Noisy test images\")\n",
    "plot_digits(reconstructed_pca, \"Denoised by PCA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399cc02-c72d-404f-9ebb-b10ff71170f8",
   "metadata": {},
   "source": [
    "### Denoising with AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b3d4d-6817-4757-bc7c-3bac1b9f143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder_denoise = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='relu')\n",
    "])\n",
    "\n",
    "mnist_decoder_denoise = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (30,)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(28*28, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "mnist_ae_denoise = tf.keras.Sequential([mnist_encoder_denoise, mnist_decoder_denoise])\n",
    "mnist_ae_denoise.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa208c18-312d-44aa-b7ec-b7225d3915ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths:\n",
    "mnist_encoder_denoise_path = './mnist_encoder_denoise.weights.h5'\n",
    "mnist_decoder_denoise_path = './mnist_decoder_denoise.weights.h5'\n",
    "history_path = './mnist_decoder_denoise.history.h5'\n",
    "\n",
    "if train_from_scratch:\n",
    "    history_denoise  = mnist_ae_denoise.fit(train_images_noisy, train_images, epochs=nEpochs, validation_data=(val_images_noisy, val_images),\n",
    "                                           callbacks = [ EarlyStopping(monitor='val_loss', patience=nPatience,\n",
    "                                                                       verbose=False, restore_best_weights=True)])\n",
    "                                       \n",
    "    # Save the weights:\n",
    "    mnist_encoder_denoise.save_weights(mnist_encoder_denoise_path)\n",
    "    mnist_decoder_denoise.save_weights(mnist_decoder_denoise_path)\n",
    "\n",
    "    # Save training history:\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history_denoise, f)\n",
    "\n",
    "else:\n",
    "    # load previsously computed weights\n",
    "    mnist_encoder_denoise.load_weights(mnist_encoder_denoise_path)\n",
    "    mnist_decoder_denoise.load_weights(mnist_decoder_denoise_path)\n",
    "\n",
    "    # load history:\n",
    "    with open(history_path, 'rb') as f:\n",
    "        history_denoise = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c8953-b952-4dbb-a101-2f3471d27afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_denoise.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7faaccd-969b-4aa2-a32a-e72024d2bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ae = mnist_ae_denoise(test_images_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419213b-a3f8-4cae-ba17-cde0764ce82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the AE results\n",
    "plot_digits(test_images, \"Test images\")\n",
    "plot_digits(test_images_noisy, \"Noisy test images\")\n",
    "plot_digits(reconstructed_ae.numpy(), \"Denoised by Autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638e406",
   "metadata": {},
   "source": [
    "## Autoencoder based on Convolutions\n",
    "Below is the definition of a very simple autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd6bb9-aaf5-4bac-8a73-67cac359b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmnist_simple_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 Ã— 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "])\n",
    "\n",
    "CNNmnist_simple_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (14, 14, 32)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n",
    "    tf.keras.layers.UpSampling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n",
    "])\n",
    "\n",
    "CNNmnist_simple_ae = tf.keras.Sequential([CNNmnist_simple_encoder, CNNmnist_simple_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65098c6b",
   "metadata": {},
   "source": [
    "**Exercise:** Using inspiration from the commands above, compile and train this autoencoder. Start with 10 epochs, you can always extend the training later. It's probably easiest if you save this notebook under a new name, and then work through it again using CNN layers.\n",
    "\n",
    "Compare the number of parameters and the results with the autoencoder based on the densely connected layers (i.e., without convolutions). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e253c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
