{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61f4f0d-8ad1-4017-b7e4-76d4d2667ba7",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "1. Interrupt all existing kernels (otherwise you will get memory problems). To do so go to the navigation bar and click Kernel > Shut Down All Kernels\n",
    "2. **After** step 1 and contrary to part 2, start this notebook with the **Torch** Kernel. To do so go to the navigation bar and click Kernel > Change Kernel... > Select Torch > Click Select (Alternativly via the kernel button on the top right of the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e444d8-4dee-4d91-a1b0-a2c50bf0b6a4",
   "metadata": {},
   "source": [
    "## Binary Judgement Prediction with Transformer language models\n",
    "\n",
    "The last model class we'll experiment with are Transformer language models. We will *not* train a model from scratch on this dataset because Transformer language models are typically very large networks, with million of parameters, which would likely overfit to the dataset at hand. Instead, we will use a pre-trained language model, an autoregressive Transformer optimised to predict the next word in texts of many different domains.\n",
    "\n",
    "We suggest you use [GPT-neo-125m](https://huggingface.co/EleutherAI/gpt-neo-125m), a model designed to replicate the architecture of OpenAI's GPT-3 in its smallest version (125 million parameters). Feel free to substitute this with another pretrained autoregressive language model from the Hugging Face [model hub](https://huggingface.co/models?sort=trending) but beware of model size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb36a1-ff87-43bf-9fd3-8afd52041df4",
   "metadata": {},
   "source": [
    "First, let's install and load the necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef253af-f851-4b9c-ac1f-254ebe5a314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import accuracy_score, PrecisionRecallDisplay\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15300d5-0590-4cf5-ad79-d72f04708893",
   "metadata": {},
   "source": [
    "Load data as preprocessed in the preparation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9b691-b358-4603-9ee4-3f5ee4224f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2345b6-da60-45c8-baf6-64fe5acbec82",
   "metadata": {},
   "source": [
    "Loading Helper functions that were also used in part 2.\n",
    "\n",
    "A note from the ***Software Engineering point of view:*** Having these functions defined twice in both notebooks is not nice. However, as we run parts on tensorflow and on pytorch separately, it is an easy approach the appropriate framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71bc856-fda1-4fc1-9640-f000181e4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ECHR_dataset_for_binary_judgement_classification(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_binary_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "# Convenience functions: prepare data splits in scikit-friendly format\n",
    "# You don't need to read the code in this cell, but please make sure you execute it.\n",
    "\n",
    "def load_input_from_ECHR_dataset(dataframe):\n",
    "    # Input: text\n",
    "    X_train = data[data.partition == 'train'].text.to_list()\n",
    "    X_val = data[data.partition == 'dev'].text.to_list()\n",
    "    X_test = data[data.partition == 'test'].text.to_list()\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def load_binary_output_from_ECHR_dataset(dataframe):\n",
    "    # Binary output: violation judgement\n",
    "    y_train_binary = data[data.partition == 'train'].binary_judgement.to_numpy()\n",
    "    y_val_binary = data[data.partition == 'dev'].binary_judgement.to_numpy()\n",
    "    y_test_binary = data[data.partition == 'test'].binary_judgement.to_numpy()\n",
    "    return y_train_binary, y_val_binary, y_test_binary\n",
    "\n",
    "def load_regression_output_from_ECHR_dataset(dataframe):\n",
    "    # Regression output: case importance score\n",
    "    y_train_regression = data[data.partition == 'train'].importance.astype(float).to_numpy()\n",
    "    y_val_regression = data[data.partition == 'dev'].importance.astype(float).to_numpy()\n",
    "    y_test_regression = data[data.partition == 'test'].importance.astype(float).to_numpy()\n",
    "    return y_train_regression, y_val_regression, y_test_regression\n",
    "\n",
    "def load_multiclass_output_from_ECHR_dataset(dataframe):\n",
    "    # Multiclass output: case importance label\n",
    "    y_train_multiclass = data[data.partition == 'train'].importance.to_numpy()\n",
    "    y_val_multiclass = data[data.partition == 'dev'].importance.to_numpy()\n",
    "    y_test_multiclass = data[data.partition == 'test'].importance.to_numpy()\n",
    "    return y_train_multiclass, y_val_multiclass, y_test_multiclass\n",
    "\n",
    "def load_ECHR_dataset_for_binary_judgement_classification(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_binary_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def load_ECHR_dataset_for_case_importance_regression(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_regression_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "        return train_ds, val_ds, test_ds\n",
    "\n",
    "def load_ECHR_dataset_for_case_importance_classification(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_multiclass_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def load_classification_model_and_tokenizer(model_name_or_path):\n",
    "    lm = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Load the tokenizer suitable for this model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    if not lm.config.pad_token_id:\n",
    "        lm.config.pad_token_id = lm.config.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return lm, tokenizer\n",
    "\n",
    "class EHRCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx],\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length',\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_attention_mask=True,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87931fe-0c5a-4a41-a768-955e730322ad",
   "metadata": {},
   "source": [
    "Load the data in model-friendly format using the convenience functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de085d4-ecd0-4c99-bafb-fa931726228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using our convenience functions\n",
    "train_ds, val_ds, test_ds = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
    "\n",
    "# Load the tokenizer suitable for the model model\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-125m\"\n",
    "lm, tokenizer = load_classification_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Create dataset and data loaders for training and validation\n",
    "train_dataset = EHRCDataset(train_ds['texts'], train_ds['labels'], tokenizer, max_length=2048)\n",
    "val_dataset = EHRCDataset(val_ds['texts'], val_ds['labels'], tokenizer, max_length=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13803c-7d4f-40c7-9fe1-6e2574cac9fe",
   "metadata": {},
   "source": [
    "### Zero-shot classification and prompting\n",
    "\n",
    "Note that this model is pre-trained on the general language modelling task (predicting the next word in a text) and not on the legal judgement prediction task. This is different from the setup you have seen in the tutorial on pre-trained Transformers. The type of classification we will perform with this model is typically referred to as *zero-shot classification*, meaning that the model is asked to classify by seeing *no* examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353ee8-bab0-4b4a-a7e8-49dd657b6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_NAME,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d70d1-ae6c-4e2d-8673-7c40c618ca76",
   "metadata": {},
   "source": [
    "Instead of using a `text-classification` pipeline, we are using a `zero-shot-classification` pipeline. These two are almost equivalent except that `zero-shot-classification` doesn't require a hardcoded number of potential classes. They can be chosen at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748f985-2b55-4853-b1a1-bb240ae938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"innocent\", \"guilty\"]\n",
    "label2id = {label: i for i, label in enumerate(candidate_labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231e104-238b-4c4c-b805-a2cbe4c30220",
   "metadata": {},
   "source": [
    "Why should this work? The language model is essentially asked if \"innocent\" is more or less likely to follow the court case text then \"guilty\".\n",
    "\n",
    "But does it work in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d067b65-098b-458a-840e-03ca9f2cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_binary_classifier_7 = []\n",
    "\n",
    "for text in tqdm(val_ds[\"texts\"]):\n",
    "\n",
    "    # Forward pass of zero-shot classification\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels\n",
    "    )\n",
    "\n",
    "    # Get the model prediction (labels ordered according to their probability)\n",
    "    prediction = label2id[result[\"labels\"][0]]\n",
    "    predictions_binary_classifier_7.append(prediction)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc_classifier7 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_7)\n",
    "print(\"\\nAccuracy:\", acc_classifier7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742a824-80a4-4d1c-bd18-b73d907aad51",
   "metadata": {},
   "source": [
    "To further steer the model towards giving sensible answers, it is good practice to prepend or append a templated string to the input example. In this case, we could for instance use the template \"The party being sued in this court case is\", which makes the model much less surprised to see \"innocent\" or \"guilty\" as continuations and gives the model a context to interpret those continuations as we would like it to. This technique is referred to as *prompting*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482533e9-3003-4dfa-96e2-7eb898a677df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The party being sued in this court case is {}\"\n",
    "candidate_labels = [\"innocent\", \"guilty\"]\n",
    "label2id = {label: i for i, label in enumerate(candidate_labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aacdf9-edce-48db-b3d2-884ac60fc439",
   "metadata": {},
   "source": [
    "Does this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb3e42-5c63-4667-930a-e84b70a2ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_binary_classifier_8 = []\n",
    "\n",
    "for text in tqdm(val_ds[\"texts\"]):\n",
    "\n",
    "    # Forward pass of zero-shot classification\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels,\n",
    "        hypothesis_template=prompt  # here we prompt the model with our template\n",
    "    )\n",
    "\n",
    "    # Get the model prediction (labels ordered according to their probability)\n",
    "    prediction = label2id[result[\"labels\"][0]]\n",
    "    predictions_binary_classifier_8.append(prediction)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc_classifier8 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_8)\n",
    "print(\"\\nAccuracy:\", acc_classifier8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb34db-4085-4086-b5e3-e5ed38f57904",
   "metadata": {},
   "source": [
    "**Exercise:** Try at least one more combination of prompt and labels and test the corresponding zero-shot classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a4e35-4694-4b8c-894d-a652847f750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"...\"  # fill in a prompt\n",
    "# candidate_labels = [\"...\", \"...\"]  # fill in potential labels\n",
    "label2id = {label: i for i, label in enumerate(candidate_labels)}\n",
    "\n",
    "predictions_binary_classifier_9 = []\n",
    "\n",
    "for text in tqdm(val_ds[\"texts\"]):\n",
    "  # ... # forward pass\n",
    "  )\n",
    "\n",
    "  # ... # get the model prediction\n",
    "  \n",
    "# ... # calculate the accuracy\n",
    "\n",
    "print(\"\\nAccuracy:\", acc_classifier9)                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24362ae8-f5a3-412b-8c0b-28a3c4ddd68c",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Finally, we fine-tune the pre-trained language model on the binary prediction task. By showing it examples of court cases and supervised labels, we obtain a Transformer model specialized for the judgement prediction task. Note that this might result in the model forgetting previous knowledge and becoming less performant in other tasks, including next-word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1b570-2d19-4d7f-b5a0-2ce8708247c2",
   "metadata": {},
   "source": [
    "Let's launch the fine-tuning and save the fine-tuned model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47b65f-fb66-4d86-92b2-19442095fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define convenience functions\n",
    "def finetune_lm(model, train_dataset, val_dataset, n_epochs, batch_size, learning_rate, output_dir):\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=n_epochs,\n",
    "        logging_dir=\"./logs\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=learning_rate,\n",
    "        gradient_accumulation_steps=4 # added due to out-of-memory CUDA error\n",
    "    )\n",
    "\n",
    "    # Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.predictions.argmax(-1), p.label_ids)},\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bdbbc-6911-4d78-a2d1-1b546615e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using our convenience functions\n",
    "train_ds, val_ds, test_ds = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
    "\n",
    "# Load the tokenizer suitable for the model model\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-125m\"\n",
    "lm, tokenizer = load_classification_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Create dataset and data loaders for training and validation\n",
    "train_dataset = EHRCDataset(train_ds['texts'], train_ds['labels'], tokenizer, max_length=2048)\n",
    "val_dataset = EHRCDataset(val_ds['texts'], val_ds['labels'], tokenizer, max_length=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f659ce-6bf7-49e7-83ed-573a3b71a366",
   "metadata": {},
   "source": [
    "Now we can either train the model from scratch or load a pre-trained model. Note that training the model from scratch takes significant time (>3h). So we do not expect that you train the model from scratch necessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcd126-ed06-4bde-9ee7-58b254629a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_from_scratch = False\n",
    "\n",
    "if train_from_scratch:\n",
    "    N_EPOCHS = 5\n",
    "    BATCH_SIZE = 3\n",
    "    LEARNING_RATE = 1e-5\n",
    "    modelFile_finetuned = \"models_trained/lm_for_classification_5ep\"\n",
    "\n",
    "    lm_finetuned, lm_trainer = finetune_lm(lm, train_dataset, val_dataset, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, modelFile_finetuned)\n",
    "    \n",
    "    # Save or use the trained model as needed\n",
    "    lm_finetuned.save_pretrained(modelFile_finetuned)\n",
    "    \n",
    "else:\n",
    "    # load fine-tuned model    \n",
    "    folder_path = \"./models_trained/\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(\"models_trained\")\n",
    "    modelFile_finetuned = \"models_trained/lm_for_classification_5ep\"\n",
    "    if not os.path.exists(modelFile_finetuned):\n",
    "        # Download model\n",
    "        !wget https://polybox.ethz.ch/index.php/s/uBs2WuEvu4TVfCo/download\n",
    "        # unzip model\n",
    "        !tar -xvzf download -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "    lm_finetuned = AutoModelForSequenceClassification.from_pretrained(modelFile_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b505f-9ea8-45c8-b55d-62df82b56ed4",
   "metadata": {},
   "source": [
    "Now we obtain predictions from the model and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e088f-7a78-4d39-9687-17648e2b92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "lm_finetuned.to(device)\n",
    "\n",
    "# List to store predicted labels\n",
    "predictions_binary_classifier_10 = []\n",
    "\n",
    "# Tokenize and predict labels for each example in the dataset\n",
    "for text in tqdm(val_ds['texts']):\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokenized_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = lm_finetuned(**tokenized_input)\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_label = torch.argmax(output.logits, dim=1).item()\n",
    "\n",
    "    # Store predicted label in the list\n",
    "    predictions_binary_classifier_10.append(predicted_label)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc_classifier10 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_10)\n",
    "print(acc_classifier10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97c847-0e6b-4f4c-91d6-f71fd27196f6",
   "metadata": {},
   "source": [
    "Now we export the accuracies of the individual models for the last notebook, in which we compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e4900-573e-45d8-bb61-4d0d2b15bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "acc = {\n",
    "    \"acc_classifier7\": acc_classifier7,\n",
    "    \"acc_classifier8\": acc_classifier8,\n",
    "    \"acc_classifier9\": acc_classifier9,\n",
    "    \"acc_classifier10\": acc_classifier10\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"accuracy.json\", \"w\") as f:\n",
    "    json.dump(acc, f)\n",
    "data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21d8d3-6926-4851-9a04-67e98810acbe",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65735098-b99c-441c-b169-6ed3679d56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "_, _, test_set = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
    "\n",
    "test_documents = test_set['texts']\n",
    "test_labels = test_set['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a413b-a92f-4773-a680-27c58f33ffa8",
   "metadata": {},
   "source": [
    "Example evaluation with Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb955b-038d-47c8-9540-666669baa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prompt = \"Is this a case of 'violation' of human rights or a case of 'absolution'? It is a case of {}\"\n",
    "candidate_labels = [\"violation\", \"absolution\"]  # fill in potential labels\n",
    "label2id = {label: i for i, label in enumerate(candidate_labels)}\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"EleutherAI/gpt-neo-125m\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Make predictions with Transformers\n",
    "binary_predictions = []\n",
    "\n",
    "for text in tqdm(test_documents):\n",
    "    # Forward pass of zero-shot classification\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels,\n",
    "        hypothesis_template=prompt\n",
    "    )\n",
    "\n",
    "    # Get the model prediction (labels ordered according to their probability)\n",
    "    prediction = label2id[result[\"labels\"][0]]\n",
    "    binary_predictions.append(prediction)\n",
    "\n",
    "# Evaluation report\n",
    "report = classification_report(\n",
    "    y_true=test_labels,\n",
    "    y_pred=binary_predictions\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39df7bf-579b-47fe-b70f-fe5a34845886",
   "metadata": {},
   "source": [
    "**Exercise:** Compare the results to the evaluation on the test set in part 2 (logistic regression/LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf385e-6f98-405a-9243-fb217ef735f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
