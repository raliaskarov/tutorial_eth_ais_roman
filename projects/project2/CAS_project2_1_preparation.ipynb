{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775bf8ab-29db-4b88-a2c7-708629550015",
   "metadata": {},
   "source": [
    "# Legal judgement prediction\n",
    "\n",
    "For this project, we will use the **ECHR dataset**, a collection of 11.5K court cases extracted from the public database of the European Court of Human Rights and further annotated by human experts (more info [here](https://www.aclweb.org/anthology/P19-1424/)). You will develop NLP models that, given the facts of a case, predict whether a human rights article or protocol has been violated. We call such problems *binary classification*.\n",
    "\n",
    "We will start from simple logistic regression classifiers that use bag-of-words representations of a court case as features, then move to bidirectional LSTM classifiers with frozen and adaptive embeddings, and conclude with pre-trained and fine-tuned Transformer language models.\n",
    "\n",
    "For those who want to go above and beyond, or simply exercise their NLP classification skills further, it is possible to work on a non-mandatory project extension. Here, you will build models that predict a court case's \"importance score\". This is a value from 1 to 4 that allows legal practitioners to identify pivotal cases. You will address this as a *multi-class classification* problem. But more on this later!\n",
    "\n",
    "All of the binary classification tasks, which are mandatory, are based on notions and code that you have been exposed to through lectures and/or tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8605d-2a47-4b65-a79d-6d775858905a",
   "metadata": {},
   "source": [
    "## Preliminary data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9d255-4053-497e-8704-be374035dd02",
   "metadata": {},
   "source": [
    "Let's begin by loading the dataset. The ECHR dataset is open-source and can be downloaded from [this web page](https://archive.org/details/ECHR-ACL2019), but we are going to load a cleaner version of it, which has been pre-processed for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8186bd-06ee-40bb-8eb7-c23857b3bcd2",
   "metadata": {},
   "source": [
    "Now we can import the `load_dataset` from `datasets`, as well as the `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085226f-34c0-449b-805c-4ad4bbc504a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7afc8-29e4-4281-b313-cf45100aacf4",
   "metadata": {},
   "source": [
    "We load the data from the Hugging Face dataset hub and we store it in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee554c-678c-475c-b78b-3112447c54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glnmario/ECHR\")\n",
    "full_data = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Here, 'train' is just the default name for single-partition datasets.\n",
    "# The actual training, development, and test set are defined in the\n",
    "# first column of the dataframe ('partition')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627699-514f-4a19-8cd1-d7ac65524484",
   "metadata": {},
   "source": [
    "**Exercise:** Display and inspect the first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8416c-fabe-4202-9941-af394648a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # fill in this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c44b2-f750-4260-9241-30b1e6e77176",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see the solution</summary>\n",
    "  \n",
    "  ```python\n",
    "full_data.head()\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e02fec-7cd8-4aa3-a1cd-c86162b97eea",
   "metadata": {},
   "source": [
    "As it is common for datasets used in Machine Learning projects, the dataset is split into 3 partitions: training, development, and test set. The training and development sets contain cases from 1959 through 2013, and the test set from 2014 through 2018.\n",
    "> Note: *It's good practice to never look at the test set during development, as the test set represents the data your Machine Learning system will have to deal with once deployed, which you can't observe at development time. Here, we will keep the test set at hand but you should avoid making any modelling decision based on its content or features. Furthermore, for data which covers a significant period of time (as we have it here), it's best to use the most recent portion of the data as test data, as this will be most similar to the real-world data for which we will use the system.*\n",
    "\n",
    "The sizes of the partitions, in terms of number of court cases, are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36a821-0919-45c7-ac21-e2aab843f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = len(full_data[full_data.partition == \"train\"])\n",
    "length_dev = len(full_data[full_data.partition == \"dev\"])\n",
    "length_test = len(full_data[full_data.partition == \"test\"])\n",
    "length_total = len(full_data)\n",
    "\n",
    "print(\"Training set     \", length_train, \"(\", round(length_train/length_total*100,2), \"% )\")\n",
    "print(\"Development set  \", length_dev, \"(\", round(length_dev/length_total*100,2), \"% )\")\n",
    "print(\"Test set         \", length_test, \"(\", round(length_test/length_total*100,2), \"% )\")\n",
    "print(\"Total           \", length_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e8044-357f-410c-87a1-246c2e43a8e0",
   "metadata": {},
   "source": [
    "Each instance in this dataset is a court case. Each court case is annotated with the following properties (the columns of the dataframe):\n",
    "\n",
    "*   `partition`: a label indicating dataset partition this court case belongs to (\"train\", \"dev\", or \"test\")\n",
    "*   `itemid`: a code which uniquely identifies this court case\n",
    "*   `languageisocode`: an [ISO code](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes) describing the language in which the case is reported\n",
    "*   `respondent`: the ISO code of the party being sued or tried (respondents are nation states)\n",
    "*   `branch`: the branch of the Court dealing with the case, indicating at which stage of the trial a judgement was made (it can be one out of \"ADMISSIBILITY\", \"CHAMBER\", \"GRANDCHAMBER\", \"COMMITTEE\")\n",
    "*   `date`: the date of the judgement\n",
    "*   `docname`: the title of the court case (for example, \"ERIKSON v. ITALY\")\n",
    "*   `importance`: an \"importance score\" from 1 (key case) to 4 (unimportant), denoting a case's contribution in the development of case-law\n",
    "*   `conclusion`: a short summary of the case conclusion (for example, \"Inadmissible\" or \"Violation of Art. 6-1; No violation of Art. 10\"\n",
    "*   `judges`: the name of the judges\n",
    "*   `text`: the facts brought to the attention of the Court\n",
    "*   `binary_judgement`: a binary label indicating whether an article or protocol was (1) or wasn't (0) violated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a24daf-4d84-4261-9ab8-32d41eccdef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598953e-29e5-4d31-acd4-37901e96a654",
   "metadata": {},
   "source": [
    " ### Filter court cases based on length\n",
    " We are now going to filter out from the dataset the court cases with the longest texts. We will do this for two reasons. First, this will speed up the experiments. Second, the Transformer model that we will use at the end of the project has, like most Transormers, a limited *window size*, which cannot fit more than 2048 tokens. This is the maximum sequence length that a\n",
    " Transformer can process at a time.\n",
    "\n",
    " Therefore, we will set a threshold. We keep all texts with a length smaller or equal than this threshold and filter out the others.\n",
    "\n",
    "***Set a threshold by inspecting how many data points it tosses out and how balanced the sizes of the different partitions are (see the next four code cells). The threshold should be smaller than 2048, but greater than or equal to 300.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0d8b3-5302-462d-af33-a275519d7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = ... # fill this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e1668-c7ce-4de6-b757-99334f492485",
   "metadata": {},
   "source": [
    "We chose threshold = 1024 that we have texts that are long enough but also not too long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cdc14-8d88-408b-80f7-720575452112",
   "metadata": {},
   "source": [
    "Let's look at basic text length statistics and how many court cases are left out when using a certain threshold.\n",
    "\n",
    "First, we measure the length of every text in the dataset. We do this by splitting each text into words as indicated by whitespace characters, and then counting the number of resulting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a49a7-9632-40cf-bd1e-9457371b6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text lengths using whitespaces as a simple criterion to separate words\n",
    "text_lengths = []\n",
    "for text in full_data.text:\n",
    "  word_list = text.split()\n",
    "  num_words = len(word_list)\n",
    "  text_lengths.append(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc530c-0d91-4639-b254-1c1697649627",
   "metadata": {},
   "source": [
    "Now can plot the distribution of text lengths, marking the threshold with a vertical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70335a6-148f-4a23-b39f-73f96e07ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot text lengths\n",
    "plt.hist(text_lengths, bins=100, alpha=0.5)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Text length (number of words)')\n",
    "\n",
    "# Add a vertical bar corresponding to the threshold\n",
    "plt.axvline(THRESHOLD, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0c98f-0e0e-4f27-a263-dd817efb8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_included = np.sum(np.array(text_lengths) < THRESHOLD) / len(text_lengths)\n",
    "print(f\"Percentage of texts included: {round(percentage_included,4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7ae5f-f358-462d-9a62-9e2ead974f74",
   "metadata": {},
   "source": [
    "With THRESHOLD = 1024, we can inlcude around 35.67 % of all texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889dd44-507e-473a-b136-bf3c0cd70aa7",
   "metadata": {},
   "source": [
    "As you can see this leaves out quite a few court cases, but it is okay for the purposes of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476b2e6-7091-4914-9641-ab3f9b2640d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length as an extra column to the dataset\n",
    "full_data['text_length'] = text_lengths\n",
    "\n",
    "# Calculate how many cases are discarded\n",
    "n_left_out = sum(full_data.text_length > THRESHOLD)\n",
    "print(f\"Omitting {n_left_out} long cases.\")\n",
    "\n",
    "# Filter out court cases with a text length larger than the threshold\n",
    "data = full_data[full_data.text_length <= THRESHOLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77495ef2-31aa-42db-a7ea-c319672b253f",
   "metadata": {},
   "source": [
    "Let's also make sure the dataset is still reasonably balanced with respect to the training, validation, and test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38301c-5712-489a-9377-f892be2d0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train_partitioned = len(data[data.partition == \"train\"])\n",
    "length_dev_partitioned = len(data[data.partition == \"dev\"])\n",
    "length_test_partitioned = len(data[data.partition == \"test\"])\n",
    "length_total_partitioned = len(data)\n",
    "\n",
    "print(\"Training set     \", length_train_partitioned, \"(\", round(length_train_partitioned/length_total_partitioned*100,2), \"% )\")\n",
    "print(\"Development set   \", length_dev_partitioned, \"(\", round(length_dev_partitioned/length_total_partitioned*100,2), \"% )\")\n",
    "print(\"Test set         \", length_test_partitioned, \"(\", round(length_test_partitioned/length_total_partitioned*100,2), \"% )\")\n",
    "print(\"Total            \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01150eaf-8a0b-4779-8df0-494842c91cc3",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6829ff-7182-4c77-85d6-c7e4557fa7d4",
   "metadata": {},
   "source": [
    "Now that we have our final version of the dataset, let's visualise the distribution of some of the dataset properties (date, branch, respondent, etc.) to get a sense of the data. What time span does the dataset cover? How many cases make it to the Grand Chamber? Which countries have been sued most often?\n",
    "\n",
    "**Exercise:** Fill in the code for the second plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0de2c2-1754-4f12-b82b-f3751610309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot number of instances per date\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.countplot(x='date', data=data, palette='viridis')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels\n",
    "plt.title('Number of Instances by Date')\n",
    "\n",
    "# Plot number of instances per branch\n",
    "plt.subplot(3, 1, 2)\n",
    "... # fill in this line\n",
    "plt.title('Number of Instances by Branch')\n",
    "\n",
    "# Plot number of instances per top 10 respondents\n",
    "plt.subplot(3, 1, 3)\n",
    "top_respondents = data['respondent'].value_counts().nlargest(10)\n",
    "sns.barplot(x=top_respondents.index, y=top_respondents.values, palette='colorblind')\n",
    "plt.title('Number of Instances by Top 10 Respondents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b114a9-f967-40f0-8c3a-b217c780ed6c",
   "metadata": {},
   "source": [
    "Let's now look at how many cases in this dataset actually resulted in violations of human rights articles or protocols. This is typically called the *class label distribution*. It will give us an idea of the dataset *class balance* (or *class imbalance*), an important property to look out for when making modelling and evaluation decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810f4cb-1597-44d0-b0be-d5a6b9655740",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot binary class label distribution per partition\n",
    "sns.countplot(\n",
    "    x='partition',\n",
    "    hue='binary_judgement',\n",
    "    data=data,\n",
    "    palette='colorblind',\n",
    "    order=['train', 'dev', 'test']\n",
    ")\n",
    "\n",
    "# Annotate plot\n",
    "plt.legend(title='Judgement', labels=['0: no violation', '1: violation'])\n",
    "plt.title('Distribution of Binary Judgement Labels for Each Dataset Partition')\n",
    "plt.xlabel('Partition')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40f845-d131-46e5-8038-3158a97c5125",
   "metadata": {},
   "source": [
    "Finally, let's look at the class distribution of importance scores. Remember: importance scores range from 1 (key case) to 4 (unimportant).\n",
    "\n",
    "**Exercise:** Write code that plots the class distribution per data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbef0cd-c6d3-42c7-8c4a-d2c7ca3c0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance score distribution per partition\n",
    "\n",
    "# ... # write the code snippet that plots class distribution by partition\n",
    "\n",
    "# Annotate plot\n",
    "plt.legend(title='Importance')\n",
    "plt.title('Distribution of Case Importance Score for Each Partition')\n",
    "plt.xlabel('Partition')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd7482-b591-4020-8278-babd99367674",
   "metadata": {},
   "source": [
    "Let's save the data for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05394bb-4138-4ad0-8cbb-8521c79de13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
