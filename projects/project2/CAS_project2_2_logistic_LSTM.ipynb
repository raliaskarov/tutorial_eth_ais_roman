{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0a508a-53f8-4e3c-b382-a063a1cec363",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "1. Interrupt all existing kernels (otherwise you will get memory problems). To do so go to the navigation bar and click Kernel > Shut Down All Kernels\n",
    "2. **Start this notebook with the Tensorflow Kernel.** To do so go to the navigation bar and click Kernel > Change Kernel... > Select Tensorflow > Click Select (Alternativly via the kernel button on the top right of the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e48647-5b6c-4eb1-a303-45d703360160",
   "metadata": {},
   "source": [
    "## Binary Judgement Prediction with Bag of Words\n",
    "\n",
    "Let's finally start with the task of predicting the outcome of a case given the text describing the main facts brought to the attention of the court. As we have just seen, each court case is annotated with a binary judgement label: whether the offendant has (label 1) or has not (label 0) violated any human rights article or protocol. This is a similar scenario to the sentiment classification task you have worked on previously in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1858b14-1fe6-4b23-833e-fe9a8a1ef9de",
   "metadata": {},
   "source": [
    "### Set-up\n",
    "First, we load the necessary python libraries. Similarly to the sentiment classification example, we will use `keras` and `tensorflow`.\n",
    "\n",
    "**Exercise:** Fix the random seed of `tensorflow` and `numpy` to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990a93c-9b57-4888-93c1-329000f07ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Initialize random number generators to ensure reproducibility\n",
    "\n",
    "# set random seed for tensorflow\n",
    "... # fill in this line\n",
    "\n",
    "# set random seed for numpy\n",
    "... # fill in this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111449ad-9fbc-4091-8a69-d6eccd544c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions: prepare data splits in scikit-friendly format\n",
    "# You don't need to read the code in this cell, but please make sure you execute it.\n",
    "\n",
    "def load_input_from_ECHR_dataset(dataframe):\n",
    "    # Input: text\n",
    "    X_train = data[data.partition == 'train'].text.to_list()\n",
    "    X_val = data[data.partition == 'dev'].text.to_list()\n",
    "    X_test = data[data.partition == 'test'].text.to_list()\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def load_binary_output_from_ECHR_dataset(dataframe):\n",
    "    # Binary output: violation judgement\n",
    "    y_train_binary = data[data.partition == 'train'].binary_judgement.to_numpy()\n",
    "    y_val_binary = data[data.partition == 'dev'].binary_judgement.to_numpy()\n",
    "    y_test_binary = data[data.partition == 'test'].binary_judgement.to_numpy()\n",
    "    return y_train_binary, y_val_binary, y_test_binary\n",
    "\n",
    "def load_regression_output_from_ECHR_dataset(dataframe):\n",
    "    # Regression output: case importance score\n",
    "    y_train_regression = data[data.partition == 'train'].importance.astype(float).to_numpy()\n",
    "    y_val_regression = data[data.partition == 'dev'].importance.astype(float).to_numpy()\n",
    "    y_test_regression = data[data.partition == 'test'].importance.astype(float).to_numpy()\n",
    "    return y_train_regression, y_val_regression, y_test_regression\n",
    "\n",
    "def load_multiclass_output_from_ECHR_dataset(dataframe):\n",
    "    # Multiclass output: case importance label\n",
    "    y_train_multiclass = data[data.partition == 'train'].importance.to_numpy()\n",
    "    y_val_multiclass = data[data.partition == 'dev'].importance.to_numpy()\n",
    "    y_test_multiclass = data[data.partition == 'test'].importance.to_numpy()\n",
    "    return y_train_multiclass, y_val_multiclass, y_test_multiclass\n",
    "\n",
    "def load_ECHR_dataset_for_binary_judgement_classification(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_binary_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def load_ECHR_dataset_for_case_importance_regression(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_regression_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "        return train_ds, val_ds, test_ds\n",
    "\n",
    "def load_ECHR_dataset_for_case_importance_classification(dataframe, for_tensorflow=False):\n",
    "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
    "    y_train, y_val, y_test = load_multiclass_output_from_ECHR_dataset(dataframe)\n",
    "    if for_tensorflow:\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    else:\n",
    "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
    "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
    "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d48a5-2a94-421b-b9f5-6fc9d768f171",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now load the data in needed for the binary classification task in a model-friendly format, using some convenience functions defined in the cell above. As we have seen, the ECHR dataset comes with a predefined train-validation-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623403f-d4c5-4bc6-94a2-d54618d6038f",
   "metadata": {},
   "source": [
    "First, we load the data from the preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf9f6c-aca7-46ae-b9e2-06c5bd1cd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c256b4-ae91-47ea-9efb-7b5c08e83c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = load_ECHR_dataset_for_binary_judgement_classification(data, for_tensorflow=True)\n",
    "\n",
    "# Print 3 examples from the dataset\n",
    "for example, label in train_ds.take(3):\n",
    "  print(\"Input: \", example)\n",
    "  print(10*\".\")\n",
    "  print('Target labels: ', label)\n",
    "  print(50*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6184f9-dda8-4686-8105-cf1b4deb02a1",
   "metadata": {},
   "source": [
    "### Fit and evaluate\n",
    "\n",
    "The following piece of code defines a function that trains (fits) a model on the training data and evaluates it on the development set. It then returns a dictionary with the training and validation history.\n",
    "\n",
    "Please take some time to read this code and to understand all of its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c76747-e40a-455e-832c-853b2fcbc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_binary_classifier(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    model, # model to train\n",
    "    model_name, # name of the model\n",
    "    learning_rate, # too small, and it learns slowly; too large, and it might overshoot the optimal solution\n",
    "    buffer_size, # number of elements randomly sampled from the dataset to form a batch during training\n",
    "    batch_size, # number of training examples utilized in one iteration\n",
    "    n_epochs, # number of iterations to train the model\n",
    "    patience_n_epochs=5\n",
    "    ):\n",
    "\n",
    "    # preliminaries\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    # start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train\n",
    "    history = model.fit(\n",
    "        train_ds.shuffle(buffer_size=buffer_size).batch(batch_size),\n",
    "        validation_data=val_ds.batch(batch_size),\n",
    "        epochs=n_epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[EarlyStopping(\n",
    "            monitor='val_accuracy', patience=patience_n_epochs, verbose=False, restore_best_weights=True\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    # end timing\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Evaluate Training Progress\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "\n",
    "    resDict = {}\n",
    "    resDict['train_loss'] = history_dict['loss']\n",
    "    resDict['val_loss'] = history_dict['val_loss']\n",
    "    resDict['train_accuracy'] = history_dict['accuracy']\n",
    "    resDict['val_accuracy'] = history_dict['val_accuracy']\n",
    "    resDict['epochs'] = len(resDict['train_accuracy'])\n",
    "    resDict['model_name'] = model_name\n",
    "    resDict['training_time'] = training_time\n",
    "\n",
    "    return resDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774e6bf-2b70-4789-803a-07f025bfb2ff",
   "metadata": {},
   "source": [
    "Now that the data is loaded and the training and evaluation procedure is in place, we can move to modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125a73e-7055-40ec-aac9-5b3726228aec",
   "metadata": {},
   "source": [
    "### Creating Bag-of-Words text representations\n",
    "\n",
    "We will use [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to obtain bag-of-words representations of texts.\n",
    "\n",
    "These representations will depend on two main parameters:\n",
    "* the vocabulary size `VOCAB_SIZE`, which limits the number of word considered to the `VOCAB_SIZE` most frequent ones\n",
    "* the type of bag-of-words representation: based on raw word counts (`count`) or on word counts weighed by inverse document frequency (`tf-idf`)\n",
    "\n",
    "**Exercise:** Write code to create count-based and tf-idf text representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616c1b5-da84-4dfc-a00f-048464d1edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c6565-a83a-4df7-b756-43dfb3bc8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Create count-based features\n",
    "# ----------------------------\n",
    "# encoder_bow_count = ... # fill in this line\n",
    "# ... # fill in this line\n",
    "\n",
    "# Create tf-idf features\n",
    "# ----------------------------\n",
    "# encoder_bow_tfidf = ... # fill in this line\n",
    "# ... # fill in this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72e4d7-ca4c-4245-a0fd-1aff20e02e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a peak at the vocabulary\n",
    "vocab = np.array(encoder_bow_count.get_vocabulary())\n",
    "vocab[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff83be6-b3fc-4ba5-a3f0-b2c5cef9142f",
   "metadata": {},
   "source": [
    "### Create folder for saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81270538-c19d-4738-a083-8dc7b8d9f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"models_trained\" # create one folder for models trained from scratch\n",
    "if not os.path.exists(folder_path):\n",
    "  os.mkdir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d74b3-78b2-40ca-9dfe-5712b9f1531b",
   "metadata": {},
   "source": [
    "### Binary classifier 1\n",
    "As a first model, we will implement a logistic regression classifier with count-based BOW representations.\n",
    "\n",
    "**Exercise:** Write code to define the model architecture, the training obectives, and the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e68871-c8a8-44f6-97b0-362169e6eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main hyperparameters\n",
    "# --------------------------------------------------------------\n",
    "LEARNING_RATE = 0.005\n",
    "N_EPOCHS = 20\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# Define model architecture\n",
    "# --------------------------------------------------------------\n",
    "binary_classifier_1 = Sequential(\n",
    "    name = f'Logistic regression, count-based BOW, |V| = {VOCAB_SIZE}'\n",
    ")\n",
    "# binary_classifier_1.add(...)  # fill in this line\n",
    "# binary_classifier_1.add(...)  # fill in this line\n",
    "# binary_classifier_1.add(...)  # fill in this line\n",
    "\n",
    "# Define training objective, evaluation metric, and optimizer\n",
    "# --------------------------------------------------------------\n",
    "binary_classifier_1.compile(\n",
    "    # loss='...', # fill in this line\n",
    "    # metrics=['...'], # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
    ")\n",
    "print(binary_classifier_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7de01a-2113-4850-8648-892e9b79a6ef",
   "metadata": {},
   "source": [
    "Let's evaluate the first classifier. You have two options: train it from scratch or load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f6ae1-7cc3-428c-90c6-a3b39b25d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_1 = os.path.join(folder_path, 'models_BoW/logistic_regression_count_based_BOW_V_1000.keras')\n",
    "historyFile_classifier_1 = os.path.join(folder_path, 'models_BoW/logistic_regression_count_based_BOW_V_1000.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_1 =  fit_and_eval_binary_classifier(\n",
    "      train_ds=train_ds,\n",
    "      val_ds=val_ds,\n",
    "      model=binary_classifier_1,\n",
    "      model_name=binary_classifier_1.name,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      buffer_size=BUFFER_SIZE,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      n_epochs=N_EPOCHS,\n",
    "      patience_n_epochs=N_EPOCHS\n",
    "  )\n",
    "  # save trained model\n",
    "  folder_name_bow = \"models_BoW\"\n",
    "  folder_path_bow = os.path.join(folder_path, folder_name_bow) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_bow):\n",
    "    os.mkdir(folder_path_bow)\n",
    "\n",
    "  binary_classifier_1.save(modelFile_classifier_1) # save classifier\n",
    "  with open(historyFile_classifier_1, 'w') as json_file: # save training history\n",
    "    json.dump(res_dict_classifier_1, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_1)) | (not os.path.exists(historyFile_classifier_1)):\n",
    "    !tar -xvzf ./pretrained/models_BoW.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_1 = load_model(modelFile_classifier_1)\n",
    "  with open(historyFile_classifier_1, 'r') as json_file:\n",
    "    res_dict_classifier_1 = json.load(json_file)\n",
    "\n",
    "train_acc_model_1 = res_dict_classifier_1['train_accuracy']\n",
    "val_acc_model_1 = res_dict_classifier_1['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c181d92-a5e0-4746-87a4-b0845ed87d0c",
   "metadata": {},
   "source": [
    "These are its training and validation accuracy over epochs. (Note that the model stops training after `patience_n_epochs` where its loss doesn't improve. We set this value equal to the number of epochs, so the model completes them all. However, you can set this parameter to a lower value for more efficient training. This is what you'd likely do in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4e82b-c609-46eb-8455-c81189e962a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    range(1, len(train_acc_model_1) + 1),  # the epochs for the x-axis\n",
    "    train_acc_model_1,  # the training accuracy\n",
    "    'b:',  # for dotted blue line\n",
    "    label='Logreg count-based BOW, Training acc'\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(val_acc_model_1) + 1),  # the epochs for the x-axis\n",
    "    val_acc_model_1,  # the validation accuracy\n",
    "    'b',  # for dense blue line\n",
    "    label='Logreg count-based BOW, Validation acc'\n",
    ")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a372a-64c0-4f9c-8f20-2de7fbbf7336",
   "metadata": {},
   "source": [
    "### Binary classifier 2\n",
    "Next is a logistic regression classifier with tfidf-based BOW representations.\n",
    "\n",
    "\n",
    "**Exercise:** Write code to define the model architecture, the training obectives, and the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68693292-06da-4571-8dc2-7fdf2a0da70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main hyperparameters\n",
    "# --------------------------------------------------------------\n",
    "LEARNING_RATE = 0.005\n",
    "N_EPOCHS = 20\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# Define model architecture\n",
    "# --------------------------------------------------------------\n",
    "binary_classifier_2 = Sequential(\n",
    "    name = f'Logistic regression, tfidf-based BOW, |V| = {VOCAB_SIZE}'\n",
    ")\n",
    "#binary_classifier_2.add(...)  # fill in this line\n",
    "#binary_classifier_2.add(...)  # fill in this line\n",
    "#binary_classifier_2.add(...)  # fill in this line\n",
    "\n",
    "# Define training objective, evaluation metric, and optimizer\n",
    "# --------------------------------------------------------------\n",
    "binary_classifier_2.compile(\n",
    "    #loss='...',  # fill in this line\n",
    "    #metrics=['...'],  # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
    ")\n",
    "print(binary_classifier_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae1c94-df1a-4f07-b120-ee187d03b436",
   "metadata": {},
   "source": [
    "You have two options: either train the model from scratch or load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c751b36-1883-44df-856d-7d3c4d3a3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_2 = os.path.join(folder_path, 'models_BoW/logistic_regression_tfidf_based_BOW_V_1000.keras')\n",
    "historyFile_classifier_2 = os.path.join(folder_path, 'models_BoW/logistic_regression_tfidf_based_BOW_V_1000.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_2 =  fit_and_eval_binary_classifier(\n",
    "      train_ds=train_ds,\n",
    "      val_ds=val_ds,\n",
    "      model=binary_classifier_2,\n",
    "      model_name=binary_classifier_2.name,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      buffer_size=BUFFER_SIZE,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      n_epochs=N_EPOCHS,\n",
    "      patience_n_epochs=N_EPOCHS\n",
    "  )\n",
    "  # save trained model\n",
    "  folder_name_bow = \"models_BoW\"\n",
    "  folder_path_bow = os.path.join(folder_path, folder_name_bow) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_bow):\n",
    "    os.mkdir(folder_path_bow)\n",
    "\n",
    "  binary_classifier_2.save(modelFile_classifier_2) # save classifier\n",
    "  with open(historyFile_classifier_2, 'w') as json_file:\n",
    "    json.dump(res_dict_classifier_2, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_2)) | (not os.path.exists(historyFile_classifier_2)):\n",
    "    !tar -xvzf ./pretrained/models_BoW.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_2 = load_model(modelFile_classifier_2)\n",
    "  with open(historyFile_classifier_2, 'r') as json_file:\n",
    "    res_dict_classifier_2 = json.load(json_file)\n",
    "\n",
    "train_acc_model_2 = res_dict_classifier_2['train_accuracy']\n",
    "val_acc_model_2 = res_dict_classifier_2['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db75d1d-7c16-4b1d-b62f-399883089e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    range(1, len(train_acc_model_2) + 1),\n",
    "    train_acc_model_2,\n",
    "    'g:',\n",
    "    label='Logreg tfidf-based BOW, Training acc'\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(val_acc_model_2) + 1),\n",
    "    val_acc_model_2,\n",
    "    'g',\n",
    "    label='Logreg tfidf-based BOW, Validation acc'\n",
    ")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de9881-f26c-4aac-ae05-a2e9a6e99d1e",
   "metadata": {},
   "source": [
    "### Model comparison\n",
    "\n",
    "**Exercise:** To compare the two models visually, ***plot the training and validation accuracy of the two bag-of-words models.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117fe7b-f2f3-4544-8f53-4ec62b6510fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "...  # fill in this line\n",
    "...  # fill in this line\n",
    "...  # fill in this line\n",
    "...  # fill in this line\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.grid(True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d7899-fd30-4852-af7b-cc84db48a2b6",
   "metadata": {},
   "source": [
    "**Exercise:** Briefly describe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6ccb6-1b84-4375-a6d4-80c9d265df48",
   "metadata": {},
   "source": [
    "*Enter your response here (two or three sentences should suffice).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cb08a-9fe2-4a3e-a5d1-c0a742e9d55c",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "To understand the models' performance beyond the evaluation scores, it is useful to carry out what could be called an *intepretability analysis*.\n",
    "\n",
    "We interpret what the model has learned by analysing its weights.\n",
    "\n",
    "**Exercise:** Write code to extract the weights from the two classifiers above and to obtain the vocabulary entries with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e04dc-f148-4ae3-b7df-a7b0cc617161",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = np.array(encoder_bow_count.get_vocabulary())\n",
    "vocab2 = np.array(encoder_bow_tfidf.get_vocabulary())\n",
    "\n",
    "# Extract the classifier weights\n",
    "# classifier_1_vocab_weights = ...  # fill in this line\n",
    "# classifier_2_vocab_weights = ...  # fill in this line\n",
    "\n",
    "# Sort the weights and get the correspondingly sorted vocabulary indices\n",
    "# classifier_1_vocab_weights_sorted = ...  # fill in this line\n",
    "# classifier_2_vocab_weights_sorted = ...  # fill in this line\n",
    "\n",
    "# The indices with the largest values indicate which words are most indicative of violations\n",
    "print(\"Words predictive of violations\")\n",
    "print(\"Model 1:\\n\", vocab1[classifier_1_vocab_weights_sorted[-10:]])\n",
    "print()\n",
    "print(\"Model 2:\\n\", vocab2[classifier_2_vocab_weights_sorted[-10:]])\n",
    "\n",
    "# ... and vice versa\n",
    "print(\"\\n\\nWords predictive of absolution\")\n",
    "print(\"Model 1:\\n\", vocab1[classifier_1_vocab_weights_sorted[:10]])\n",
    "print(\"Model 2:\\n\", vocab2[classifier_2_vocab_weights_sorted[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0340b-dcc2-4d5f-bf52-187c0d8d6715",
   "metadata": {},
   "source": [
    "Do the words with the highest weights correspond to sensible violation or absolution cues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d0dc4-5f77-4279-8a0a-07f305430456",
   "metadata": {},
   "source": [
    "## Binary Judgement Prediction with LSTMs\n",
    "\n",
    "As a next model class, we will consider recurrent neural models — in particular, LSTMs. As you have learned, these models are able to take into account the order of words in sentences, which is in principle a big advantage over bag-of-words models. \"The woman sued Switzerland\" is not the same as \"Switzerland sued the woman\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63f6a8-7a1b-425d-a388-6853abfe0b58",
   "metadata": {},
   "source": [
    "### BiLSTM with embeddings trained from scratch\n",
    "\n",
    "First, we'll design a simple one-layer bidirectional LSTM with word embeddings learned from scratch.\n",
    "\n",
    "**Exercise:** Write code to create word embeddings for the vocabulary of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c20ab-3c65-4e30-830c-2220833e0509",
   "metadata": {},
   "source": [
    "First, define the right encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f60c03-c33a-47af-b1f7-ea26825deb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# encoder_embed = ... # fill in this line\n",
    "# ... # fill in this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a42255-52da-4114-a76f-792b378c7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the vocabulary id of the word \"human\"\n",
    "encoder_embed(\"human\").numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028253d4-5763-43c3-a664-b81441be3b62",
   "metadata": {},
   "source": [
    "Then, create the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e2784-d006-4ae7-be60-a5b0f1a25326",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    # input_dim=...,  # fill in this line\n",
    "    # output_dim=...,  # fill in this line\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a66e-5800-4bdf-a308-c1031ca3c4be",
   "metadata": {},
   "source": [
    "**Exercise: *Write code to define the model architecture***. Remember, this should include an input layer, encoder and embedding layers, a bidirectional LSTM layer and an output layer. Keep the dimensionality of the LSTM layer low (for example, 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb088e3-98a6-4532-b80e-6e8a16a52438",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 10000\n",
    "N_EPOCHS = 5 # can be increased to get better results if computational resources available, e.g. to 20\n",
    "\n",
    "binary_classifier_3 = Sequential(\n",
    "    name=f\"1-layer BiLSTM, embeddings from scratch\"\n",
    ")\n",
    "# binary_classifier_3.add(...)  # fill in this line\n",
    "# binary_classifier_3.add(...)  # fill in this line\n",
    "# binary_classifier_3.add(...)  # fill in this line\n",
    "# binary_classifier_3.add(...)  # fill in this line\n",
    "# binary_classifier_3.add(...)  # fill in this line\n",
    "\n",
    "binary_classifier_3.compile(\n",
    "    #loss='...',  # fill in this line\n",
    "    #metrics=['...'],  # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
    ")\n",
    "print(binary_classifier_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9faab88-3cee-426d-a8e8-13c2f4ea9ddd",
   "metadata": {},
   "source": [
    "You have two options: either train the model from scratch or load a pretrained model. Note: the LSTM takes longer to train than the logistic regression. We set the patience parameter to 5 to avoid redundant epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff25f4-4f8c-49d8-8860-fb8cb84045a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_3 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_from_scratch.keras')\n",
    "historyFile_classifier_3 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_from_scratch.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_3 =  fit_and_eval_binary_classifier(\n",
    "      train_ds=train_ds,\n",
    "      val_ds=val_ds,\n",
    "      model=binary_classifier_3,\n",
    "      model_name=binary_classifier_3.name,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      buffer_size=BUFFER_SIZE,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      n_epochs=N_EPOCHS,\n",
    "      patience_n_epochs=5\n",
    "  )\n",
    "  # save trained model\n",
    "  folder_name_lstm = \"models_LSTM\"\n",
    "  folder_path_lstm = os.path.join(folder_path, folder_name_lstm) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_lstm):\n",
    "    os.mkdir(folder_path_lstm)\n",
    "\n",
    "  binary_classifier_3.save(modelFile_classifier_3) # save classifier\n",
    "  with open(historyFile_classifier_3, 'w') as json_file: # save history\n",
    "    json.dump(res_dict_classifier_3, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_3)) | (not os.path.exists(historyFile_classifier_3)):\n",
    "    !tar -xvzf ./pretrained/models_LSTM.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_3 = load_model(modelFile_classifier_3)\n",
    "  with open(historyFile_classifier_3, 'r') as json_file:\n",
    "    res_dict_classifier_3 = json.load(json_file)\n",
    "\n",
    "train_acc_model_3 = res_dict_classifier_3['train_accuracy']\n",
    "val_acc_model_3 = res_dict_classifier_3['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6afa71-c574-4cdc-a32f-1645cb402b4e",
   "metadata": {},
   "source": [
    "### Deeper network\n",
    "Next, let's try with a deeper two-layer LSTM network. Word embeddings will be still learned from scratch.\n",
    "\n",
    "\n",
    "**Exercise: *Define the full two-layer Bidirectional LSTM in the cell below.***  This is identical to the one-layer model, but with an extra Bidirectional LSTM layer. Again, keep the dimensionality of the LSTM layers low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91ebd5-0e72-419a-8bbb-c66deed26d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_embed = ... # fill in this line\n",
    "# ... # fill in this line\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    # input_dim=...,  # fill in this line\n",
    "    # output_dim=...,  # fill in this line\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    trainable=True,\n",
    ")\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 10000\n",
    "N_EPOCHS = 5 # can be increased to get better results if computational resources available, e.g. to 20\n",
    "\n",
    "binary_classifier_4 = Sequential(\n",
    "    name=f\"2-layer BiLSTM, embeddings from scratch\"\n",
    ")\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "# binary_classifier_4.add(...)  # fill in this line\n",
    "\n",
    "#compile the model before training\n",
    "binary_classifier_4.compile(\n",
    "    #loss='...',  # fill in this line\n",
    "    #metrics=['...'],  # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10e618-8494-4347-8858-7481575c12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_classifier_4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933253d-d829-4f44-9850-1fc103cf4257",
   "metadata": {},
   "source": [
    "You have two options: either train the model from scratch or load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c21b11-13d3-4981-91c5-d2cd0fe85fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_4 = os.path.join(folder_path, 'models_LSTM/2_layer_BiLSTM_embeds_from_scratch.keras')\n",
    "historyFile_classifier_4 = os.path.join(folder_path, 'models_LSTM/2_layer_BiLSTM_embeds_from_scratch.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_4 =  fit_and_eval_binary_classifier(\n",
    "      train_ds=train_ds,\n",
    "      val_ds=val_ds,\n",
    "      model=binary_classifier_4,\n",
    "      model_name=binary_classifier_4.name,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      buffer_size=BUFFER_SIZE,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      n_epochs=N_EPOCHS,\n",
    "      patience_n_epochs=5\n",
    "  )\n",
    "  # save trained model\n",
    "  folder_name_lstm = \"models_LSTM\"\n",
    "  folder_path_lstm = os.path.join(folder_path, folder_name_lstm) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_lstm):\n",
    "    os.mkdir(folder_path_lstm)\n",
    "\n",
    "  binary_classifier_4.save(modelFile_classifier_4) # save classifier\n",
    "  with open(historyFile_classifier_4, 'w') as json_file: # save history\n",
    "    json.dump(res_dict_classifier_4, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_4)) | (not os.path.exists(historyFile_classifier_4)):\n",
    "    !tar -xvzf ./pretrained/models_LSTM.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_4 = load_model(modelFile_classifier_4)\n",
    "  with open(historyFile_classifier_4, 'r') as json_file:\n",
    "    res_dict_classifier_4 = json.load(json_file)\n",
    "\n",
    "train_acc_model_4 = res_dict_classifier_4['train_accuracy']\n",
    "val_acc_model_4 = res_dict_classifier_4['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b90b1-946d-4847-8c7e-1d9998e8da16",
   "metadata": {},
   "source": [
    "**Exercise:** To compare the two bidirectional LSTMs, plot the training and validation accuracy of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3ed9c-d868-4043-8109-a0bd5b60a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...  # fill in this line\n",
    "# ...  # fill in this line\n",
    "# ...  # fill in this line\n",
    "# ...  # fill in this line\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47b1d6-c2d0-4f8a-8d6b-5ceb72b83f40",
   "metadata": {},
   "source": [
    "### Pre-trained word embeddings\n",
    "\n",
    "The dataset at hand is very domain-specific and not particularly large so it is unlikely that the model will be able learn the general meaning of words. Luckily the network can be initialised with pre-trained word embeddings, which were trained on generalist corpora to capture the meaning of all words in the vocabulary. We will download pre-trained GloVe embeddings of dimensionality 50, trained on a corpus of 6 billion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15fac6-573f-4379-8962-f9186ab086c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_glove = os.path.join(folder_path, 'models_glove') # concatenate the directory path and folder name\n",
    "if not os.path.isfile('glove.6B.zip'):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip -q glove.6B.zip -d $folder_path_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744eb2b-fac9-4050-9957-1353fe19096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "# ----------------------------------\n",
    "file_path_glove_50d = os.path.join(folder_path_glove, 'glove.6B.50d.txt')\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(file_path_glove_50d) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for i, word in enumerate(encoder_embed.get_vocabulary()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        print(word)\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bf4b3-7de8-4168-9b1a-b7fea7276adc",
   "metadata": {},
   "source": [
    "#### Frozen embeddings\n",
    "\n",
    "Here, we are going to leave the word embeddings \"frozen\". That is, they will not be updated throughout the training of the LSTM. In this way, the embeddings will remain general representations of word meaning while the rest of the network will specialise for the legal judgement prediction task.\n",
    "\n",
    "**Exercise: *Define a Bidirectional LSTM with frozen, pre-trained word embeddings.*** You can make the LSTM one-layer for faster training. If computational power is available, use can use more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29042178-7d73-4355-86be-a6137f380214",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 10000\n",
    "N_EPOCHS = 5 # can be increased to get better results if computational resources available, e.g. to 20\n",
    "\n",
    "pretrained_embedding_layer_frozen = Embedding(\n",
    "    #input_dim=...,  # fill in this line\n",
    "    #output_dim=...,  # fill in this line\n",
    "    #embeddings_initializer=Constant(embedding_matrix),\n",
    "    #trainable=...,  # fill in this line\n",
    ")\n",
    "\n",
    "binary_classifier_5 = Sequential(\n",
    "    name=f\"1-layer BiLSTM classifier (frozen pre-trained embeddings)\"\n",
    ")\n",
    "# Fill in the following lines to build the LSTM\n",
    "#binary_classifier_5.add(...)\n",
    "#binary_classifier_5.add(...)\n",
    "#binary_classifier_5.add(...)\n",
    "#binary_classifier_5.add(...)\n",
    "#binary_classifier_5.add(...)\n",
    "\n",
    "#compile the model before training\n",
    "binary_classifier_5.compile(\n",
    "    #loss='...',  # fill in this line\n",
    "    #metrics=['...'],  # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55e699-d63a-4b88-88a1-4d968008fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier_5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a44494-2dd6-4bf0-85b7-90fbc810e97f",
   "metadata": {},
   "source": [
    "Again, you have two options: either train the model from scratch or load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163b7d8-b6a3-42b9-a5b0-0b19d5b53a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_5 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.keras')\n",
    "historyFile_classifier_5 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_5 =  fit_and_eval_binary_classifier(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    model=binary_classifier_5,\n",
    "    model_name=binary_classifier_5.name,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    patience_n_epochs=5\n",
    "  )\n",
    "\n",
    "  # save trained model\n",
    "  folder_name_lstm = \"models_LSTM\"\n",
    "  folder_path_lstm = os.path.join(folder_path, folder_name_lstm) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_lstm):\n",
    "    os.mkdir(folder_path_lstm)\n",
    "\n",
    "  binary_classifier_5.save(modelFile_classifier_5) # save classifier\n",
    "  with open(historyFile_classifier_5, 'w') as json_file: # save dictionary with history\n",
    "    json.dump(res_dict_classifier_5, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_5)) | (not os.path.exists(historyFile_classifier_5)):\n",
    "    !tar -xvzf ./pretrained/models_LSTM.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_5 = load_model(modelFile_classifier_5)\n",
    "  with open(historyFile_classifier_5, 'r') as json_file:\n",
    "    res_dict_classifier_5 = json.load(json_file)\n",
    "\n",
    "train_acc_model_5 = res_dict_classifier_5['train_accuracy']\n",
    "val_acc_model_5 = res_dict_classifier_5['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e720f-1cc3-49c8-8265-9e223dc7ef1d",
   "metadata": {},
   "source": [
    "#### Adaptive embeddings\n",
    "\n",
    "Now let's unfreeze the embeddings and allow them to be updated throughout training.  To do so, we need to set `trainable=True` when defining the embedding layer.\n",
    "\n",
    "**Exercise: *Define a Bidirectional LSTM with adaptive embeddings.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22d77-baae-41f3-a7fa-3fb413377cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 10000\n",
    "N_EPOCHS = 5 # can be increased to get better results if computational resources available, e.g. to 20\n",
    "\n",
    "pretrained_embedding_layer_adaptive = Embedding(\n",
    "    #input_dim=...,  # fill in this line\n",
    "    #output_dim=...,  # fill in this line\n",
    "    #embeddings_initializer=Constant(embedding_matrix),\n",
    "    #trainable=...,  # fill in this line\n",
    ")\n",
    "\n",
    "binary_classifier_6 = Sequential(\n",
    "    name=f\"1-layer BiLSTM, adaptive pre-trained embeddings\"\n",
    ")\n",
    "# Fill in the following lines to build the LSTM\n",
    "#binary_classifier_6.add(...)\n",
    "#binary_classifier_6.add(...)\n",
    "#binary_classifier_6.add(...)\n",
    "#binary_classifier_6.add(...)\n",
    "#binary_classifier_6.add(...)\n",
    "\n",
    "#compile the model before training\n",
    "binary_classifier_6.compile(\n",
    "    #loss='...',  # fill in this line\n",
    "    #metrics=['...'],  # fill in this line\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44069bfa-e8fa-453a-b3d9-3d4fab3b758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier_6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022585e9-dd13-41c4-aa92-b8e88996ca72",
   "metadata": {},
   "source": [
    "You have two options: either train the model from scratch or load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfab31-12c8-4b3d-92f3-cfcc590e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = True # if set to True, we train clf from scratch, otherwise load pretrained model\n",
    "\n",
    "modelFile_classifier_6 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.keras')\n",
    "historyFile_classifier_6 = os.path.join(folder_path, 'models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.json')\n",
    "\n",
    "# train from scratch\n",
    "if train_from_scratch:\n",
    "  # fit_and_eval_binary_classifier returns the training history\n",
    "  res_dict_classifier_6 =  fit_and_eval_binary_classifier(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    model=binary_classifier_6,\n",
    "    model_name=binary_classifier_6.name,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    patience_n_epochs=5\n",
    "  )\n",
    "\n",
    "  # save trained model\n",
    "  folder_name_lstm = \"models_LSTM\"\n",
    "  folder_path_lstm = os.path.join(folder_path, folder_name_lstm) # concatenate the directory path and folder name\n",
    "  if not os.path.exists(folder_path_lstm):\n",
    "    os.mkdir(folder_path_lstm)\n",
    "\n",
    "  binary_classifier_6.save(modelFile_classifier_6) # save classifier\n",
    "  with open(historyFile_classifier_6, 'w') as json_file: # save dictionary with history\n",
    "    json.dump(res_dict_classifier_6, json_file)\n",
    "\n",
    "# load pretrained model either from zipfile or from folder\n",
    "else:\n",
    "  if (not os.path.exists(modelFile_classifier_6)) | (not os.path.exists(historyFile_classifier_6)):\n",
    "    !tar -xvzf ./pretrained/models_LSTM.tar.gz -C $folder_path # the path to the zip file and the destination directory\n",
    "\n",
    "  binary_classifier_6 = load_model(modelFile_classifier_6)\n",
    "  with open(historyFile_classifier_6, 'r') as json_file:\n",
    "    res_dict_classifier_6 = json.load(json_file)\n",
    "\n",
    "train_acc_model_6 = res_dict_classifier_6['train_accuracy']\n",
    "val_acc_model_6 = res_dict_classifier_6['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102896d5-bfa0-42ef-b217-b68316053b6a",
   "metadata": {},
   "source": [
    "**Exercise: *Plot the training and validation accuracy of the four LSTM models.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360148b8-a830-4351-9173-470ef1743beb",
   "metadata": {},
   "source": [
    "  First, we plot the two models without pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf2dac-748b-474a-a566-ae752c87724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # fill in this code block\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "\n",
    "plt.title('Training and validation accuracy without pre-trained embeddings')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcf6da-f298-40ca-8567-6399e0889449",
   "metadata": {},
   "source": [
    "Next we plot the performance development of two models with pre-trained embeddings (frozen and adapted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc897f-c73c-4b2d-bc1f-8ad0a8891efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # fill in this code block\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "#plt.plot(...)\n",
    "\n",
    "plt.title('Training and validation accuracy with pre-trained embeddings')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320aa9b-7fb5-4da2-bb9e-3b6c4e7b242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_acc_model_1) + 1), train_acc_model_1, 'k:', label='Logreg count-based BOW, Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_1) + 1), val_acc_model_1, 'k',  label='Logreg count-based BOW, Validation acc')\n",
    "plt.plot(range(1, len(train_acc_model_2) + 1), train_acc_model_2, 'y:', label='Logreg tfidf-based BOW, Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_2) + 1), val_acc_model_2, 'y',  label='Logreg tfidf-based BOW, Validation acc')\n",
    "\n",
    "plt.plot(range(1, len(train_acc_model_3) + 1), train_acc_model_3, 'b:', label='1-layer BiLSTM, Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_3) + 1), val_acc_model_3, 'b',  label='1-layer BiLSTM, Validation acc')\n",
    "plt.plot(range(1, len(train_acc_model_4) + 1), train_acc_model_4, 'g:', label='2-layer BiLSTM, Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_4) + 1), val_acc_model_4, 'g',  label='2-layer BiLSTM, Validation acc')\n",
    "\n",
    "plt.plot(range(1, len(train_acc_model_5) + 1), train_acc_model_5, 'r:', label='1-layer BiLSTM (frozen pre-trained embeddings), Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_5) + 1), val_acc_model_5, 'r',  label='1-layer BiLSTM (frozen pre-trained embeddings), Validation acc')\n",
    "plt.plot(range(1, len(train_acc_model_6) + 1), train_acc_model_6, 'c:', label='1-layer BiLSTM (adaptive pre-trained embeddings), Training acc')\n",
    "plt.plot(range(1, len(val_acc_model_6) + 1), val_acc_model_6, 'c',  label='1-layer BiLSTM (adaptive pre-trained embeddings), Validation acc')\n",
    "\n",
    "plt.title('Development of Training and Validation Accuracy: All Considered Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.legend(loc='right', bbox_to_anchor=(2.2, 0.5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef46cc5-9b4c-4d7b-b850-4c2f07a46b46",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0ce45-5ceb-4f24-a18c-fb84ddd8e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "_, _, test_set = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
    "\n",
    "test_documents = test_set['texts']\n",
    "test_labels = test_set['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9bb7f-1fc2-4d71-8a44-1b43195f44e3",
   "metadata": {},
   "source": [
    "Example evaluation with logistic regression classifiers and LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45fb54-c02a-4b2c-be59-05b340cb12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List to store predicted probabilities\n",
    "predictions = []\n",
    "\n",
    "# Make prediction for the test set sentences\n",
    "for text in tqdm(test_documents):\n",
    "    # Tokenize input text\n",
    "    tokenized_input = tf.constant([text])  # Ensure tensors are in the correct format for TensorFlow\n",
    "\n",
    "    # Forward pass using the predict method\n",
    "    predicted_prob = binary_classifier_1.predict(tokenized_input, verbose = 0)\n",
    "\n",
    "    # Apply sigmoid to get probabilities and take the first element (assuming single logit)\n",
    "    predicted_prob = tf.sigmoid(predicted_prob).numpy().squeeze().item()\n",
    "\n",
    "    # Store the predicted probability\n",
    "    predictions.append(predicted_prob)\n",
    "\n",
    "# Turn predicted probabilities into binary classification scores\n",
    "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Evaluate model by comparing its prediction to the gold labels\n",
    "report = classification_report(y_true=test_labels, y_pred=binary_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db5c77-2f35-45e1-a81e-e7df9a74f16f",
   "metadata": {},
   "source": [
    "By examining the report for the test data, we observe that the classifier performs significantly better on positive cases (absolution). Specifically, the positive cases have a recall of 1.0, whereas the negative cases have a recall of only 0.03. Recall, also known as the True Positive Rate (TPR), indicates the proportion of actual positives correctly identified by the classifier. This means that while the classifier successfully recognizes all positive cases, it detects only 3% of the negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefb4de-384c-47f3-9409-66d9a9c2b6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
