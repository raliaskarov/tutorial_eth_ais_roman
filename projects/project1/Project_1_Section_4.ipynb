{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCbtisSPs2mZ"
   },
   "source": [
    "# Project 1 Section 4: Object Detection and Image Segmentation\n",
    "\n",
    "We learn here how to detect objects in images and how to segment an image according to the objects in it.\n",
    "\n",
    "## Tutorial 1: Object detection\n",
    "\n",
    "Object Detection helps us understand the spatial context and location of different objects in an image. Below, you are going to perform Object detection with the use of the pretrained DETR model.\n",
    "\n",
    "The DETR model is a complex CNN that takes as input an image and outputs the same image annotated with boxes identifying the objects in it. It was trained on COCO 2017, a dataset of 118k annotated images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reZbcOJYu9Hs"
   },
   "source": [
    "For more information about the DETR model, please see [End-to-End Object Detection with Transformers](https://https://arxiv.org/abs/2005.12872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHecQXVivIXh"
   },
   "source": [
    "From the transformers module import the classes `DetrImageProcessor` and `DetrForObjectDetection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QFmSzvkvJfJ"
   },
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcgRNeO7vMr8"
   },
   "source": [
    "* The `DetrImageProcessor` class is used for the pre-processing of the images which then will be used as input to the DETR model.\n",
    "* The `DetrForObjectDetection` class provides access to the pre-trained DETR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJpnze2nvRYR"
   },
   "source": [
    "Load the model `facebook/detr-resnet-50` for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ye6awS0vRDO"
   },
   "outputs": [],
   "source": [
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPU1fNmevb2d"
   },
   "source": [
    "Load the object detector model `facebook/detr-resnet-50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q8nFdCfvejN",
    "outputId": "c53169e9-bd97-4363-c0ac-ca90826b8e6b"
   },
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18N7Yt1Lvitd"
   },
   "source": [
    "See the 89 different objects that the model has been trained to recognize, with the `.config.id2label` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KocuiIrkXOns",
    "outputId": "74322472-f73e-4eec-81ab-369209728b20"
   },
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK1Z7Gz-voxd"
   },
   "source": [
    "For example, it can recognise objects like: a bird, a cat, a hat, a car, a tie, a bicycle etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T26t2AsTvtPo"
   },
   "source": [
    "We now load the following image from the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "A5-RGDdPXPgT",
    "outputId": "f71bdcfb-f239-4d13-9459-600bfb21ce6a"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hkywaULv-t7"
   },
   "source": [
    "Preprocess the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4ORGm9QZjKA"
   },
   "outputs": [],
   "source": [
    "inputs = image_processor(images = image,\n",
    "                         return_tensors = \"pt\")\n",
    "outputs = model(**inputs)\n",
    "target_sizes = torch.tensor([image.size[::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGgARFLzwYMQ"
   },
   "source": [
    "With the `post_process_object_detection()` method, return a dictionary which will contain the objects detected in the image. The three key-value pairs in the dictionary are:\n",
    "\n",
    "* scores — The confidence of each detected object\n",
    "* labels — the index of the detected object in model.config.id2label\n",
    "* boxes — the bounding boxes of each detected object\n",
    "\n",
    "The `post_process_object_detection()` method takes in the following arguments:\n",
    "\n",
    "* the output of the model (outputs)\n",
    "* the target size of the image (target_sizes)\n",
    "* the threshold value (0.9) for filtering out predictions, which means that predictions with confidence greater than 90% will be returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M__X-C2LZ0Hs",
    "outputId": "8243920c-be49-4c0e-b3d7-eafef54da239"
   },
   "outputs": [],
   "source": [
    "results = image_processor.post_process_object_detection(outputs,\n",
    "                                                        target_sizes = target_sizes,\n",
    "                                                        threshold = 0.9)[0]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YwUh1tGwiZ0"
   },
   "source": [
    "Visualise the detected objects with their confidence scores, labels and drawing boxes around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m60_owMjiQaw",
    "outputId": "dbeb27f3-ad1c-427b-fd69-5367717a61c4"
   },
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPCgMiVI05dT"
   },
   "source": [
    "Draw bounding boxes around objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g51NXkNs08sk"
   },
   "outputs": [],
   "source": [
    "draw.rectangle(box, outline=\"yellow\", width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kesYqDy01Ds9"
   },
   "source": [
    "Display the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "J7SDdQ9t1CS6",
    "outputId": "a2b6607c-cd13-45d8-8e5f-e1881b0e3813"
   },
   "outputs": [],
   "source": [
    "draw.text((box[0], box[1]-10),\n",
    "          model.config.id2label[label.item()],\n",
    "          fill=\"white\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_f77wqhRNE3"
   },
   "source": [
    "## Tutorial 2 (semantic image segmentation)\n",
    "\n",
    "We start by importing necessary tools.\n",
    " - torch: A powerful library for working with artificial intelligence models.\n",
    " - torchvision: Offers tools and models specifically for image tasks.\n",
    " - PIL (Python Imaging Library): Helps us work with images (loading, displaying).\n",
    " - requests: Enables us to fetch data from the internet.\n",
    " - matplotlib: A plotting library, for showing images and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJZwwRjGRtKG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import requests\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBGCHR5eR4Jx"
   },
   "source": [
    "### Loading the model\n",
    "\n",
    "We now load a pre-trained CNN, DeepLabV3, which has been trained to recognize various objects in images. The model is set to \"evaluation mode,\" indicating it's ready to analyze images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa9yXXpmSF_y"
   },
   "outputs": [],
   "source": [
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()  # We tell the model it's time to work (evaluation mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmMaY5vgSMLO"
   },
   "source": [
    "### Loading the image\n",
    "\n",
    "We retrieve an image from the internet to analyze. The image is opened and ready to be processed, similar to selecting a photograph to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "image_url = \"https://cdn.pixabay.com/photo/2018/10/01/09/21/pets-3715733_960_720.jpg\"\n",
    "r = requests.get(url, timeout=20)\n",
    "r.raise_for_status()\n",
    "\n",
    "image = Image.open(BytesIO(r.content))\n",
    "image.load()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydJ-Nvl9SS8o"
   },
   "source": [
    "### Preparing the image\n",
    "\n",
    "We now transform the image into a format suitable for the model. It's akin to translating a document into a language the expert (our CNN) understands. This step ensures the image is correctly interpreted by the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdbkmqdKSfQc"
   },
   "outputs": [],
   "source": [
    "# Before analyzing, we need to adjust the image to the format the model expects.\n",
    "# This involves converting the image to a tensor (a multi-dimensional array used in AI models) and normalizing it.\n",
    "preprocess = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # The model expects a batch of images, even if there's just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaDaWiuXTP1M"
   },
   "source": [
    "#### Analyzing the image\n",
    "\n",
    "Here, the model examines the image and determines which parts of the image belong to different objects. This process is similar to asking an expert to identify and categorize different elements in a photograph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV_br7sETlVA"
   },
   "outputs": [],
   "source": [
    "# Now, we ask our pre-trained model to analyze the image and give us the segmentation result.\n",
    "with torch.no_grad():  # This tells PyTorch we don't need to do any training.\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9392YfDTnD9"
   },
   "source": [
    "#### Visualizing the results\n",
    "\n",
    "This final step displays the original image alongside the segmentation result produced by our model. The segmentation map uses different colors to represent various parts of the image identified by the model, offering a visual representation of the model's \"understanding\" of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ2irsOXTspu"
   },
   "outputs": [],
   "source": [
    "# Finally, we visualize the results. We'll show the original image and the model's understanding of it side by side.\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')  # Hide the axis for a cleaner look\n",
    "plt.subplot(1, 2, 2)\n",
    "# We use a color map to differentiate the segments identified by the model.\n",
    "plt.imshow(output_predictions.byte().cpu().numpy(), cmap='nipy_spectral', interpolation='nearest')\n",
    "plt.title('Segmentation Result')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eggXLZceTy_8"
   },
   "source": [
    "Observe how the CNN recognizes and distinguishes between cats and dogs. The cats are colored in green and the dogs are colored in gray. Note however, that it incorrectly believes that there is a cat in front of the two dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ovvwv5r3rCR"
   },
   "source": [
    "## Assessment\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Perform object detection on this image [cat and dog](https://www.companionanimalclinicvirginia.com/wp-content/uploads/2018/12/white_cat_and_dog.jpg) by using the DETR pre-trained model. Here is the URL `https://www.companionanimalclinicvirginia.com/wp-content/uploads/2018/12/white_cat_and_dog.jpg`.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Perform semantic image segmentation on this [image](https://cdn.pixabay.com/photo/2017/12/27/14/02/friends-3042751_960_720.jpg). Here is the URL `https://cdn.pixabay.com/photo/2017/12/27/14/02/friends-3042751_960_720.jpg`. Use the following models:\n",
    "\n",
    "* FCN: You can load this model with the command `model = fcn_resnet101(pretrained=True)`. Do not forget to import it.\n",
    "* LRASPP MobileNetV3: You can load this model with the command `model = lraspp_mobilenet_v3_large(pretrained=True)`.\n",
    "\n",
    "Which one works best for this image? Do the same for the image of the cats and dogs. Which one works best for that image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
